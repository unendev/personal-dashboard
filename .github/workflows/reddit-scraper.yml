name: Redditå¤šæ¿å—çˆ¬è™«

on:
  schedule:
    # æ¯å¤©åŒ—äº¬æ—¶é—´19:00 (UTC 11:00) æ‰§è¡Œ
    - cron: '0 11 * * *'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ“¥ Checkoutä»£ç 
        uses: actions/checkout@v4
      
      - name: ğŸ è®¾ç½®Pythonç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: ğŸ“¦ å®‰è£…ä¾èµ–
        run: |
          cd linuxdo-scraper/reddit_scraper
          pip install --upgrade pip
          pip install requests python-dotenv asyncpg
      
      - name: ğŸš€ æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GITHUB_ACTIONS: true
        run: |
          cd linuxdo-scraper/reddit_scraper
          python reddit_scraper_multi.py
      
      - name: ğŸ“Š ä¸Šä¼ æŠ¥å‘Šä¸ºArtifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: reddit-reports-${{ github.run_number }}
          path: |
            linuxdo-scraper/reddit_scraper/data/*.json
            linuxdo-scraper/reddit_scraper/reports/*.md
            linuxdo-scraper/reddit_scraper/logs/*.log
          retention-days: 30
      
      - name: âœ… ä»»åŠ¡å®Œæˆé€šçŸ¥
        if: success()
        run: |
          echo "ğŸ‰ Redditçˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼"
          echo "ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¸Šä¼ åˆ°Artifacts"
      
      - name: âŒ ä»»åŠ¡å¤±è´¥é€šçŸ¥
        if: failure()
        run: |
          echo "ğŸ’¥ Redditçˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼"
          echo "è¯·æ£€æŸ¥æ—¥å¿—æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"

