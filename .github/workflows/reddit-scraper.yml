name: Reddit çˆ¬è™«å®šæ—¶ä»»åŠ¡

on:
  schedule:
    # æ¯å¤© UTC 00:00 è¿è¡Œï¼ˆåŒ—äº¬æ—¶é—´ 08:00ï¼‰
    - cron: '0 0 * * *'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  scrape-reddit:
    runs-on: ubuntu-latest
    
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python ç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          cd linuxdo-scraper
          pip install -r requirements.txt

      - name: è¿è¡Œ Reddit çˆ¬è™«
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          GITHUB_ACTIONS: true
        run: |
          cd linuxdo-scraper/reddit_scraper
          python reddit_scraper_multi.py

      - name: æäº¤ç”Ÿæˆçš„æ•°æ®
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add linuxdo-scraper/reddit_scraper/data/
          git add linuxdo-scraper/reddit_scraper/reports/
          git diff --quiet && git diff --staged --quiet || git commit -m "ğŸ¤– è‡ªåŠ¨æ›´æ–° Reddit æ•°æ® $(date +'%Y-%m-%d %H:%M:%S')"
          git push