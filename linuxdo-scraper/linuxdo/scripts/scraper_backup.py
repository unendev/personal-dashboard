# scraper.py
import asyncio
import os
import re
import json
from datetime import datetime
from playwright.async_api import async_playwright
from playwright_stealth import Stealth
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg

# --- ä»£ç†é…ç½® ---
proxy_for_all = "http://127.0.0.1:10809"
os.environ['HTTP_PROXY'] = proxy_for_all
os.environ['HTTPS_PROXY'] = proxy_for_all
print(f"--- è„šæœ¬å·²é…ç½®ä½¿ç”¨ HTTP ä»£ç†: {proxy_for_all} ---")
# -------------------------

# --- é…ç½® ---
load_dotenv()
WARM_UP_URL = "https://linux.do/" 
RSS_URL = "https://linux.do/latest.rss" 
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
NEON_DB_URL = os.getenv("DATABASE_URL") # ä»ç¯å¢ƒå˜é‡åŠ è½½ Neon æ•°æ®åº“ URL
POST_COUNT_LIMIT = 30 # ç¨å¾®å‡å°‘æ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªå¸–å­çš„å¤„ç†æ›´å¤æ‚äº†

# --- æ‘˜è¦å•ä¸ªå¸–å­ï¼Œå¹¶è¦æ±‚è¿”å›JSON ---
def analyze_single_post_with_deepseek(post):
    """ä½¿ç”¨DeepSeekå¯¹å•ä¸ªå¸–å­è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œå¹¶è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ã€‚"""
    clean_content = re.sub(r'<.*?>', ' ', post.get('content', ''))
    clean_content = re.sub(r'\s+', ' ', clean_content).strip()
    excerpt = (clean_content[:800] + '...') if len(clean_content) > 800 else clean_content

    if not excerpt or len(excerpt.strip()) < 10:
        return {
            "error": "å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒAIæ‘˜è¦ã€‚",
            "core_issue": "N/A", "key_info": [], "post_type": "æœªçŸ¥", "value_assessment": "ä½"
        }

    # ä½¿ç”¨DeepSeek APIè¿›è¡Œåˆ†æ
    prompt = f"""
    ä½ æ˜¯ä¸€åä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹è®ºå›å¸–å­å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœã€‚
    ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

    **å¸–å­æ ‡é¢˜**: {post['title']}
    **å†…å®¹èŠ‚é€‰**: {excerpt}

    **è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
    {{
      "core_issue": "è¿™é‡Œç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­çš„æ ¸å¿ƒè®®é¢˜",
      "key_info": [
        "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹1",
        "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹2"
      ],
      "post_type": "ä»[æŠ€æœ¯é—®ç­”, èµ„æºåˆ†äº«, æ–°é—»èµ„è®¯, ä¼˜æƒ æ´»åŠ¨, æ—¥å¸¸é—²èŠ, æ±‚åŠ©, è®¨è®º, äº§å“è¯„æµ‹]ä¸­é€‰æ‹©ä¸€ä¸ª",
      "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª"
    }}
    """
    
    try:
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 400,
            "temperature": 0.3
        }
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies={"http": proxy_for_all, "https": proxy_for_all}
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # å°è¯•è§£æAIè¿”å›çš„æ–‡æœ¬ä¸ºJSON
            cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
            analysis_data = json.loads(cleaned_text)
            return analysis_data
        else:
            print(f"    - DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {
                "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                "core_issue": "APIè°ƒç”¨å¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
            }
        
    except json.JSONDecodeError:
        print(f"    - JSONè§£æå¤±è´¥! AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹: '{ai_response[:100]}...'")
        return {
            "error": "AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹", "raw_response": ai_response,
            "core_issue": "AIåˆ†æå¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
        }
    except Exception as e:
        print(f"    - å¯¹å¸–å­ '{post['title'][:20]}...' çš„æ‘˜è¦å¤±è´¥: {e}")
        return {
            "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}",
            "core_issue": "AIåˆ†æå¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
        }

# --- ç”ŸæˆAIæŠ¥å‘Šï¼Œå¤„ç†ç»“æ„åŒ–æ•°æ® ---
def generate_ai_summary_report(posts_data):
    if not posts_data:
        return {"summary_analysis": {"error": "æ²¡æœ‰å¸–å­æ•°æ®"}, "processed_posts": []}

    print("\n--- å¼€å§‹ç”Ÿæˆé€å¸–ç»“æ„åŒ–åˆ†æ ---")
    processed_posts = [] 
    for i, post in enumerate(posts_data):
        print(f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(posts_data)} ç¯‡: {post['title']}")
        # å°†åˆ†æç»“æœï¼ˆä¸€ä¸ªå­—å…¸ï¼‰å­˜å…¥ 'analysis' é”®
        post['analysis'] = analyze_single_post_with_deepseek(post)
        processed_posts.append(post)
        
        if i < len(posts_data) - 1:
            print("    ...ç­‰å¾…3ç§’ä»¥éµå®ˆAPIé€Ÿç‡é™åˆ¶...")
            time.sleep(3) 
            
    print("--- é€å¸–åˆ†æå…¨éƒ¨å®Œæˆ ---\n")

    print("--- å¼€å§‹ç”Ÿæˆä»Šæ—¥æ•´ä½“æ´å¯ŸæŠ¥å‘Š (JSONæ ¼å¼) ---")
    # ä¸ºäº†ç”Ÿæˆæ•´ä½“æŠ¥å‘Šï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå¸–å­çš„æ ‡é¢˜å’Œåˆ†æç»“æœæ‰“åŒ…å‘ç»™AI
    summaries_for_prompt = []
    for post in processed_posts:
        if 'error' not in post['analysis']:
            summaries_for_prompt.append({
                "title": post['title'],
                "analysis": post['analysis']
            })

    # ä½¿ç”¨json.dumpsæ¥åˆ›å»ºä¸€ä¸ªç´§å‡‘çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
    all_summaries_text = json.dumps(summaries_for_prompt, ensure_ascii=False, indent=2)

    overall_prompt = f"""
    ä½ æ˜¯ä¸€åèµ„æ·±çš„è®ºå›å†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©è®ºå›çƒ­é—¨å¸–å­çš„JSONæ ¼å¼æ‘˜è¦åˆ—è¡¨ã€‚
    è¯·æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½é«˜åº¦æµ“ç¼©çš„ä¸­æ–‡"ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ"æŠ¥å‘Šï¼Œå¹¶ä¸¥æ ¼ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ã€‚
    ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

    **ä»Šæ—¥å¸–å­æ‘˜è¦åˆé›† (JSONæ ¼å¼):**
    {all_summaries_text}
    ---
    **è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
    {{
      "overview": "ç”¨ä¸€ä¸¤å¥è¯æ€»ç»“ä»Šå¤©ç¤¾åŒºçš„æ•´ä½“æ°›å›´å’Œè®¨è®ºç„¦ç‚¹ã€‚",
      "highlights": {{
        "tech_savvy": ["æç‚¼1-3æ¡æœ€ç¡¬æ ¸çš„æŠ€æœ¯å¹²è´§"],
        "resources_deals": ["æç‚¼1-3æ¡æœ€å€¼å¾—å…³æ³¨çš„ä¼˜æƒ æˆ–èµ„æºåˆ†äº«"],
        "hot_topics": ["æç‚¼1-3ä¸ªå¼•å‘æœ€å¹¿æ³›è®¨è®ºçš„è¯é¢˜"]
      }},
      "conclusion": "ç”¨ä¸€å¥è¯å¯¹ä»Šå¤©çš„å†…å®¹åšä¸ªé£è¶£æˆ–æ·±åˆ»çš„æ€»ç»“ã€‚"
    }}
    """
    try:
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": overall_prompt}
            ],
            "max_tokens": 800,
            "temperature": 0.3
        }
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies={"http": proxy_for_all, "https": proxy_for_all}
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
            summary_analysis = json.loads(cleaned_text)
            print("--- æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæ¯• ---")
        else:
            print(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šå¤±è´¥: {response.status_code} - {response.text}")
            summary_analysis = {
                "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                "overview": "ä»Šæ—¥AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚",
                "highlights": {},
                "conclusion": ""
            }
    except Exception as e:
        print(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šå¤±è´¥: {e}")
        summary_analysis = {
            "error": f"AIç”Ÿæˆæ•´ä½“æŠ¥å‘Šå¤±è´¥: {e}",
            "overview": "ä»Šæ—¥AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚",
            "highlights": {},
            "conclusion": ""
        }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- ç”ŸæˆJSONæŠ¥å‘Šæ–‡ä»¶ ---
def generate_json_report(report_data, posts_count):
    today_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"linux.do_report_{today_str}.json"

    # å‡†å¤‡æœ€ç»ˆçš„JSONç»“æ„
    final_json = {
        "meta": {
            "report_date": today_str,
            "title": f"Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})",
            "source": "Linux.do",
            "post_count": posts_count
        },
        "summary": report_data.get('summary_analysis', {}),
        "posts": []
    }

    for post in report_data.get('processed_posts', []):
        final_json["posts"].append({
            "id": post.get('id', 'N/A'),
            "title": post.get('title', 'æ— æ ‡é¢˜'),
            "url": post.get('link', '#'),
            # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨AIåˆ†æå‡ºçš„ç»“æ„åŒ–æ•°æ®
            "analysis": post.get('analysis', {})
        })

    with open(filename, 'w', encoding='utf-8') as f:
        # ä½¿ç”¨ ensure_ascii=False æ¥æ­£ç¡®å¤„ç†ä¸­æ–‡å­—ç¬¦
        # ä½¿ç”¨ indent=2 æ¥ç¾åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿é˜…è¯»
        json.dump(final_json, f, ensure_ascii=False, indent=2)

    print(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

async def create_posts_table():
    """è¿æ¥åˆ°Neonæ•°æ®åº“å¹¶åˆ›å»ºpostsè¡¨ï¼Œå¦‚æœå®ƒä¸å­˜åœ¨çš„è¯ã€‚"""
    if not NEON_DB_URL:
        print("é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        print("æ•°æ®åº“è¡¨ 'posts' æ£€æŸ¥æˆ–åˆ›å»ºæˆåŠŸã€‚")
    except Exception as e:
        print(f"åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¤„ç†åçš„å¸–å­æ•°æ®æ’å…¥åˆ°Neonæ•°æ®åº“ä¸­ã€‚"""
    if not NEON_DB_URL:
        print("é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        print("å¼€å§‹å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“...")
        for post in posts_data:
            post_id = post.get('id')
            title = post.get('title')
            url = post.get('link')
            analysis = post.get('analysis', {})
            core_issue = analysis.get('core_issue')
            key_info = json.dumps(analysis.get('key_info', []))
            post_type = analysis.get('post_type')
            value_assessment = analysis.get('value_assessment')

            await conn.execute("""
                INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (id) DO UPDATE SET
                    title = EXCLUDED.title,
                    url = EXCLUDED.url,
                    core_issue = EXCLUDED.core_issue,
                    key_info = EXCLUDED.key_info,
                    post_type = EXCLUDED.post_type,
                    value_assessment = EXCLUDED.value_assessment,
                    timestamp = CURRENT_TIMESTAMP;
            """, post_id, title, url, core_issue, key_info, post_type, value_assessment)
            print(f"  - å¸–å­ '{title[:30]}...' (ID: {post_id}) å·²æ’å…¥/æ›´æ–°ã€‚")
        print("æ‰€æœ‰å¸–å­æ•°æ®å·²æˆåŠŸæ’å…¥/æ›´æ–°åˆ°æ•°æ®åº“ã€‚")
    except Exception as e:
        print(f"æ’å…¥å¸–å­æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    finally:
        if conn:
            await conn.close()

# --- ç”ŸæˆMarkdownæŠ¥å‘Šï¼Œä½¿å…¶é€‚é…æ–°çš„æ•°æ®ç»“æ„ ---
def generate_markdown_report(report_data, posts_count):
    today_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"Linux.do_Daily_Report_{today_str}.md"

    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})\n\n")
        
        # æ¸²æŸ“ç²¾åæç‚¼éƒ¨åˆ†
        f.write("## ğŸš€ ä»Šæ—¥ç²¾åæç‚¼\n\n")
        summary = report_data.get('summary_analysis', {})
        f.write(f"**ä»Šæ—¥æ¦‚è§ˆ:** {summary.get('overview', 'N/A')}\n\n")
        f.write(f"**é«˜ä»·å€¼ä¿¡æ¯é€Ÿé€’:**\n")
        highlights = summary.get('highlights', {})
        if highlights.get('tech_savvy'):
            f.write(f"*   **æŠ€æœ¯å¹²è´§:**\n")
            for item in highlights['tech_savvy']:
                f.write(f"    *   {item}\n")
        if highlights.get('resources_deals'):
            f.write(f"*   **ä¼˜æƒ /èµ„æº**:\n")
            for item in highlights['resources_deals']:
                f.write(f"    *   {item}\n")
        if highlights.get('hot_topics'):
            f.write(f"*   **çƒ­è®®è¯é¢˜**:\n")
            for item in highlights['hot_topics']:
                f.write(f"    *   {item}\n")
        f.write(f"\n**ä»Šæ—¥ç»“è¯­:** {summary.get('conclusion', 'N/A')}\n\n")
        f.write("---\n\n")

        # æ¸²æŸ“é€å¸–æ‘˜è¦éƒ¨åˆ†
        f.write("## ğŸ“° é€å¸–æ‘˜è¦ä¸åˆ†æ\n\n")
        posts = report_data.get('processed_posts', [])
        if posts:
            for post in posts:
                title = post.get('title', 'æ— æ ‡é¢˜')
                link = post.get('link', '#')
                analysis = post.get('analysis', {})
                
                f.write(f"### [{title}]({link})\n\n")
                # ä½¿ç”¨Markdownå¼•ç”¨æ ¼å¼å±•ç¤ºç»“æ„åŒ–åˆ†æç»“æœ
                f.write(f"> 1.  **æ ¸å¿ƒè®®é¢˜**: {analysis.get('core_issue', 'N/A')}\n")
                f.write(f"> 2.  **å…³é”®ä¿¡æ¯/è§£å†³æ–¹æ¡ˆ**:\n")
                for info in analysis.get('key_info', []):
                    f.write(f">     *   {info}\n")
                if not analysis.get('key_info'):
                     f.write(f">     *   æ— \n")
                f.write(f"> 3.  **å¸–å­ç±»å‹**: {analysis.get('post_type', 'N/A')}\n")
                f.write(f"> 4.  **ä»·å€¼è¯„ä¼°**: {analysis.get('value_assessment', 'N/A')}\n\n")
        else:
            f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°æ–°å¸–å­ã€‚\n")
            
        # æ¸²æŸ“åŸå§‹å¸–å­åˆ—è¡¨
        f.write("---\n\n")
        f.write(f"## ğŸ“‹ åŸå§‹å¸–å­åˆ—è¡¨ (å…± {posts_count} ç¯‡)\n\n")
        if posts:
            for post in posts:
                f.write(f"- [{post.get('title', 'æ— æ ‡é¢˜')}]({post.get('link', '#')})\n")
        else:
            f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•åŸå§‹å¸–å­ã€‚\n")

    print(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

# --- æ ¸å¿ƒçˆ¬è™«é€»è¾‘ ---
async def fetch_linuxdo_posts_optimized():
    print("å¼€å§‹æ‰§è¡Œä¼˜åŒ–ç‰ˆçˆ¬å–ä»»åŠ¡...")
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy={"server": proxy_for_all}
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            }
        )
        stealth = Stealth()
        await stealth.apply_stealth_async(context)
        page = await context.new_page()

        try:
            print(f"æ­£åœ¨è®¿é—®é¦–é¡µè¿›è¡Œé¢„çƒ­: {WARM_UP_URL}")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    await page.goto(WARM_UP_URL, wait_until="domcontentloaded", timeout=180000)
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    print(f"é¢„çƒ­å°è¯• {attempt + 1} å¤±è´¥ï¼Œé‡æ–°å°è¯•...")
                    await asyncio.sleep(2)

            print("é¢„çƒ­å®Œæˆï¼Œä¼šè¯å·²å»ºç«‹ã€‚")
            await asyncio.sleep(3)

            print(f"æ­£åœ¨é€šè¿‡æµè§ˆå™¨ç›´æ¥è®¿é—® RSS æº: {RSS_URL}")
            for attempt in range(max_retries):
                try:
                    await page.goto(RSS_URL, wait_until="networkidle", timeout=120000)
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    print(f"RSSè®¿é—®å°è¯• {attempt + 1} å¤±è´¥ï¼Œé‡æ–°å°è¯•...")
                    await asyncio.sleep(2)

            await asyncio.sleep(1)

            rss_text = await page.evaluate("document.documentElement.outerHTML")

            with open("debug_rss_content.html", 'w', encoding='utf-8') as f:
                f.write(rss_text)
            print(f"å·²ä¿å­˜è°ƒè¯•å†…å®¹åˆ° debug_rss_content.html (é•¿åº¦: {len(rss_text)} å­—ç¬¦)")

            all_posts = []
            print("å¼€å§‹è§£æRSSå†…å®¹...")

            print("å°è¯•æ–¹å¼1ï¼šç›´æ¥è§£æXML")
            try:
                root = ET.fromstring(rss_text)
                items = root.findall('.//channel/item')
                print(f"æ‰¾åˆ° {len(items)} ä¸ªitemæ ‡ç­¾")
                for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                    title = item.find('title')
                    link = item.find('link')
                    description = item.find('description')
                    if title is not None and link is not None:
                        title_text = title.text
                        link_text = link.text
                        description_text = description.text if description is not None else ""
                        print(f"  è§£æå¸–å­ {i+1}: {title_text[:50]}... -> {link_text}")
                        match = re.search(r'/t/[^/]+/(\d+)', link_text)
                        if match:
                            topic_id = match.group(1)
                            all_posts.append({
                                "title": title_text,
                                "link": link_text,
                                "id": topic_id,
                                "description": description_text
                            })
                            print(f"    æˆåŠŸè§£æï¼ŒID: {topic_id}, æè¿°é•¿åº¦: {len(description_text)}")
                        else:
                            print(f"    æ— æ³•åŒ¹é…ID: {link_text}")
                    else:
                        print(f"  å¸–å­ {i+1} ç¼ºå°‘titleæˆ–linkæ ‡ç­¾")
            except ET.ParseError as e:
                print(f"XMLè§£æå¤±è´¥: {e}")

            # æ–¹å¼2ï¼šä»HTMLä¸­æå–XMLå†…å®¹
            if not all_posts:
                print("å°è¯•æ–¹å¼2ï¼šä»HTMLä¸­æå–XMLå†…å®¹")
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(rss_text, 'html.parser')
                    pre_tag = soup.find('pre')
                    if pre_tag:
                        xml_content = pre_tag.get_text()
                        print(f"ä»preæ ‡ç­¾æå–åˆ° {len(xml_content)} å­—ç¬¦çš„XMLå†…å®¹")
                        import html
                        xml_content = html.unescape(xml_content)
                        root = ET.fromstring(xml_content)
                        items = root.findall('.//channel/item')
                        print(f"æ‰¾åˆ° {len(items)} ä¸ªitemæ ‡ç­¾")
                        for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                            title = item.find('title')
                            link = item.find('link')
                            description = item.find('description')
                            if title is not None and link is not None:
                                title_text = title.text
                                link_text = link.text
                                description_text = description.text if description is not None else ""
                                print(f"  è§£æå¸–å­ {i+1}: {title_text[:50]}... -> {link_text}")
                                match = re.search(r'/t/[^/]+/(\d+)', link_text)
                                if match:
                                    topic_id = match.group(1)
                                    all_posts.append({"title": title_text, "link": link_text, "id": topic_id, "description": description_text})
                                    print(f"    æˆåŠŸè§£æï¼ŒID: {topic_id}")
                                else:
                                    print(f"    æ— æ³•åŒ¹é…ID: {link_text}")
                            else:
                                print(f"  å¸–å­ {i+1} ç¼ºå°‘titleæˆ–linkæ ‡ç­¾")
                    else:
                        print("æœªæ‰¾åˆ°preæ ‡ç­¾")
                except Exception as e2:
                    print(f"HTMLæå–è§£æä¹Ÿå¤±è´¥: {e2}")

            # æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–åŸºæœ¬ä¿¡æ¯
            if not all_posts:
                print("å°è¯•æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–åŸºæœ¬ä¿¡æ¯ (å…œåº•æ–¹æ¡ˆ)")
                title_pattern = r'<title>([^<]+)</title>'
                link_pattern = r'<link>([^<]+)</link>'
                titles = re.findall(title_pattern, rss_text)
                links = re.findall(link_pattern, rss_text)

                for i, (title, link) in enumerate(zip(titles, links)):
                    if i >= POST_COUNT_LIMIT:
                        break
                    match = re.search(r'/t/[^/]+/(\d+)', link)
                    if match:
                        all_posts.append({"title": title, "link": link, "id": match.group(1), "description": ""})
            
            print(f"ä» RSS æºæˆåŠŸè§£æåˆ° {len(all_posts)} ä¸ªå¸–å­ã€‚")
            
            posts_with_content = []
            for i, post in enumerate(all_posts):
                print(f"  > æ­£åœ¨å¤„ç†å¸–å­: {post['title']}")

                # ä¼˜å…ˆä½¿ç”¨RSSæè¿°ä¸­çš„å†…å®¹ä½œä¸ºå¸–å­çš„'content'
                rss_content = post.get('description', '')

                # æ¸…ç†RSSæè¿°å†…å®¹ï¼ˆç§»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½ï¼‰
                if rss_content:
                    clean_content = re.sub(r'<[^>]+>', ' ', rss_content)
                    clean_content = ' '.join(clean_content.split())
                    clean_content = clean_content.replace('\n', ' ').replace('\r', ' ')
                    clean_content = ' '.join(clean_content.split())

                    if len(clean_content.strip()) > 10:
                        post['content'] = clean_content
                        posts_with_content.append(post)
                        print(f"    + ä½¿ç”¨RSSæè¿°å†…å®¹ (é•¿åº¦: {len(clean_content)} å­—ç¬¦)")
                    else:
                        post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                        posts_with_content.append(post)
                        print(f"    + RSSå†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹")
                else:
                    post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                    posts_with_content.append(post)
                    print(f"    + ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹")

                if i < len(all_posts) - 1:
                    await asyncio.sleep(1)

            print(f"æˆåŠŸè·å–äº† {len(posts_with_content)} ä¸ªå¸–å­çš„è¯¦ç»†å†…å®¹ã€‚")
            return posts_with_content

        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            try:
                await page.screenshot(path="error_screenshot_optimized.png")
            except Exception as screenshot_error:
                print(f"æ— æ³•ä¿å­˜é”™è¯¯æˆªå›¾: {screenshot_error}")
            return []
        finally:
            await browser.close()
            print("æµè§ˆå™¨å·²å…³é—­ã€‚")

# --- ä¸»å‡½æ•° ---
async def main():
    await create_posts_table()
    posts_data = await fetch_linuxdo_posts_optimized()
    if posts_data:
        report_data = generate_ai_summary_report(posts_data)
        
        # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“
        if report_data.get('processed_posts'):
            await insert_posts_into_db(report_data['processed_posts'])

        # ç°åœ¨åŒæ—¶ç”Ÿæˆä¸¤ç§æ ¼å¼çš„æŠ¥å‘Š
        generate_json_report(report_data, len(posts_data))
        generate_markdown_report(report_data, len(posts_data))
    else:
        print("æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œä»»åŠ¡ç»“æŸã€‚")
        # å³ä½¿å¤±è´¥ï¼Œä¹Ÿç”Ÿæˆä¸€ä¸ªç©ºçš„æŠ¥å‘Šæ–‡ä»¶ï¼Œè¡¨ç¤ºä»»åŠ¡å·²è¿è¡Œ
        empty_data = {"summary_analysis": {"overview": "æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ã€‚"}, "processed_posts": []}
        generate_json_report(empty_data, 0)
        generate_markdown_report(empty_data, 0)

if __name__ == "__main__":
    asyncio.run(main())
