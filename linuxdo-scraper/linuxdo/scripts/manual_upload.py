# manual_upload.py - é€šç”¨æ‰‹åŠ¨ä¸Šä¼ è„šæœ¬
import asyncio
import os
import json
import argparse
from datetime import datetime, date
from dotenv import load_dotenv
import asyncpg

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
NEON_DB_URL = os.getenv("DATABASE_URL")

# --- ä»£ç†é…ç½® ---
proxy_for_all = "http://127.0.0.1:10809"
os.environ['HTTP_PROXY'] = proxy_for_all
os.environ['HTTPS_PROXY'] = proxy_for_all
print(f"--- è„šæœ¬å·²é…ç½®ä½¿ç”¨ HTTP ä»£ç†: {proxy_for_all} ---")

# Windows ä¸‹å°†äº‹ä»¶å¾ªç¯ç­–ç•¥åˆ‡æ¢ä¸º Selectorï¼Œé¿å…æŸäº›åº“åœ¨ Proactor ä¸‹çš„å…¼å®¹æ€§é—®é¢˜
if os.name == 'nt':
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except Exception:
        pass

async def connect_to_database_with_retry(max_retries=3, delay=5):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“è¿æ¥"""
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•è¿æ¥æ•°æ®åº“ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
            conn = await asyncpg.connect(NEON_DB_URL)
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼")
            return conn
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt + 1} æ¬¡è¿æ¥å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                print(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                await asyncio.sleep(delay)
            else:
                raise e

async def upload_data_to_db(json_file_path, target_date=None):
    """ä¸Šä¼ æŒ‡å®šJSONæ–‡ä»¶çš„æ•°æ®åˆ°æ•°æ®åº“"""
    if not NEON_DB_URL:
        print("é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚")
        return False

    # æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_file_path):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        return False
    
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    json_posts = json_data.get('posts', [])
    if not json_posts:
        print("âŒ JSONæ–‡ä»¶ä¸­æ²¡æœ‰å¸–å­æ•°æ®")
        return False
    
    # ä»metaä¸­è·å–æŠ¥å‘Šç”Ÿæˆæ—¶é—´
    meta = json_data.get('meta', {})
    generation_time_str = meta.get('generation_time', '')
    
    if generation_time_str:
        try:
            generation_time = datetime.fromisoformat(generation_time_str.replace('Z', '+00:00'))
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            generation_time = datetime.now()
    else:
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        generation_time = datetime.now()
    
    # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸï¼Œä½¿ç”¨è¯¥æ—¥æœŸ
    if target_date:
        try:
            if isinstance(target_date, str):
                target_date = datetime.strptime(target_date, '%Y-%m-%d').date()
            generation_time = datetime.combine(target_date, generation_time.time())
        except Exception as e:
            print(f"âš ï¸ ç›®æ ‡æ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæ—¶é—´: {e}")
    
    print(f"ğŸ“„ ä»JSONæ–‡ä»¶ä¸­è¯»å–åˆ° {len(json_posts)} æ¡å¸–å­è®°å½•")
    print(f"ğŸ“… ä½¿ç”¨æ—¶é—´æˆ³: {generation_time}")
    
    conn = None
    try:
        print(f"æ•°æ®åº“URLé•¿åº¦: {len(NEON_DB_URL)}")
        print(f"æ•°æ®åº“URLå‰ç¼€: {NEON_DB_URL[:20]}...")
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶è¿æ¥æ•°æ®åº“
        conn = await connect_to_database_with_retry(max_retries=3, delay=10)
        
        # ç¡®ä¿è¡¨å­˜åœ¨
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        print("âœ… æ•°æ®åº“è¡¨æ£€æŸ¥å®Œæˆ")
        
        success_count = 0
        error_count = 0
        
        print("å¼€å§‹ä¸Šä¼ æ•°æ®åˆ°æ•°æ®åº“...")
        
        for i, post in enumerate(json_posts):
            try:
                post_id = post.get('id')
                title = post.get('title')
                url = post.get('url')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []))
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                
                # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“ï¼Œä½¿ç”¨æŒ‡å®šçš„æ—¶é—´æˆ³
                await conn.execute("""
                    INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, timestamp)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                    ON CONFLICT (id) DO UPDATE SET
                        title = EXCLUDED.title,
                        url = EXCLUDED.url,
                        core_issue = EXCLUDED.core_issue,
                        key_info = EXCLUDED.key_info,
                        post_type = EXCLUDED.post_type,
                        value_assessment = EXCLUDED.value_assessment,
                        timestamp = EXCLUDED.timestamp;
                """, post_id, title, url, core_issue, key_info, post_type, value_assessment, generation_time)
                
                success_count += 1
                print(f"  âœ… {i+1}/{len(json_posts)}: {title[:50]}... (ID: {post_id})")
                
            except Exception as e:
                error_count += 1
                print(f"  âŒ {i+1}/{len(json_posts)}: ä¸Šä¼ å¤±è´¥ - {e}")
                continue
        
        print(f"\nğŸ“Š ä¸Šä¼ ç»“æœ:")
        print(f"  âœ… æˆåŠŸ: {success_count} æ¡")
        print(f"  âŒ å¤±è´¥: {error_count} æ¡")
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {success_count/(success_count+error_count)*100:.1f}%")
        
        # éªŒè¯ä¸Šä¼ ç»“æœ
        if target_date:
            if isinstance(target_date, str):
                target_date = datetime.strptime(target_date, '%Y-%m-%d').date()
            db_count = await conn.fetchval("""
                SELECT COUNT(*) FROM posts WHERE DATE(timestamp) = $1
            """, target_date)
            
            print(f"\nğŸ” éªŒè¯ç»“æœ:")
            print(f"  æ•°æ®åº“ä¸­{target_date}çš„è®°å½•æ•°: {db_count}")
            print(f"  JSONæ–‡ä»¶ä¸­çš„è®°å½•æ•°: {len(json_posts)}")
            
            if db_count >= len(json_posts):
                print("âœ… æ•°æ®ä¸Šä¼ æˆåŠŸï¼æ‰€æœ‰è®°å½•éƒ½å·²ä¿å­˜åˆ°æ•°æ®åº“ã€‚")
                return True
            elif db_count > 0:
                print(f"âš ï¸  éƒ¨åˆ†æ•°æ®ä¸Šä¼ æˆåŠŸï¼æ•°æ®åº“ä¸­å·²æœ‰ {db_count} æ¡è®°å½•ï¼ŒJSONæ–‡ä»¶ä¸­æœ‰ {len(json_posts)} æ¡è®°å½•ã€‚")
                return True
            else:
                print("âŒ æ•°æ®ä¸Šä¼ å¤±è´¥ï¼æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„è®°å½•ã€‚")
                return False
        else:
            print("âœ… æ•°æ®ä¸Šä¼ å®Œæˆï¼")
            return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False
    finally:
        if conn:
            await conn.close()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­ã€‚")

def find_json_file_by_date(date_str):
    """æ ¹æ®æ—¥æœŸæŸ¥æ‰¾å¯¹åº”çš„JSONæ–‡ä»¶"""
    possible_files = [
        f"linux.do_report_{date_str}.json",
        f"linux.do_report_{date_str.replace('-', '_')}.json",
        f"linux.do_report_{date_str.replace('-', '')}.json"
    ]
    
    for file_path in possible_files:
        if os.path.exists(file_path):
            return file_path
    
    return None

async def main():
    parser = argparse.ArgumentParser(description='æ‰‹åŠ¨ä¸Šä¼ Linux.doæ•°æ®åˆ°æ•°æ®åº“')
    parser.add_argument('--date', '-d', type=str, help='æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--file', '-f', type=str, help='æŒ‡å®šJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--list', '-l', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ—å‡ºå¯ç”¨çš„JSONæ–‡ä»¶
    if args.list:
        print("=== å¯ç”¨çš„JSONæ–‡ä»¶ ===")
        json_files = [f for f in os.listdir('.') if f.startswith('linux.do_report_') and f.endswith('.json')]
        json_files.sort()
        for i, file in enumerate(json_files, 1):
            print(f"{i:2d}. {file}")
        return
    
    # ç¡®å®šè¦ä¸Šä¼ çš„æ–‡ä»¶
    json_file_path = None
    target_date = None
    
    if args.file:
        json_file_path = args.file
        if args.date:
            target_date = args.date
    elif args.date:
        target_date = args.date
        json_file_path = find_json_file_by_date(args.date)
        if not json_file_path:
            print(f"âŒ æœªæ‰¾åˆ°æ—¥æœŸ {args.date} å¯¹åº”çš„JSONæ–‡ä»¶")
            print("å¯ç”¨çš„æ–‡ä»¶:")
            json_files = [f for f in os.listdir('.') if f.startswith('linux.do_report_') and f.endswith('.json')]
            for file in json_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {file}")
            return
    else:
        print("âŒ è¯·æŒ‡å®šæ—¥æœŸ (--date) æˆ–æ–‡ä»¶è·¯å¾„ (--file)")
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return
    
    print(f"=== æ‰‹åŠ¨ä¸Šä¼ æ•°æ®åˆ°æ•°æ®åº“ ===")
    print(f"ğŸ“ JSONæ–‡ä»¶: {json_file_path}")
    if target_date:
        print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
    
    success = await upload_data_to_db(json_file_path, target_date)
    
    if success:
        print(f"\nğŸ‰ æ•°æ®ä¸Šä¼ å®Œæˆï¼")
    else:
        print(f"\nğŸ’¥ æ•°æ®ä¸Šä¼ å¤±è´¥ï¼")

if __name__ == "__main__":
    asyncio.run(main())
