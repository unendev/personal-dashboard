# scraper_optimized.py - ä¼˜åŒ–ç‰ˆLinux.doçˆ¬è™«ï¼ˆæœ¬åœ°PCè¿è¡Œç‰ˆï¼‰
# åŠŸèƒ½ï¼šçˆ¬å–Linux.doè®ºå›RSSï¼Œä½¿ç”¨DeepSeek AIåˆ†æï¼Œå­˜å…¥Neonæ•°æ®åº“
# ä½¿ç”¨æ–¹æ³•ï¼š
#   1. ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install drissionpage asyncpg requests python-dotenv
#   2. ç¡®ä¿å·²å®‰è£… Chromeï¼ˆç³»ç»Ÿæµè§ˆå™¨ï¼‰
#   3. é…ç½® .env æ–‡ä»¶ï¼ˆè§ä¸‹æ–¹é…ç½®è¯´æ˜ï¼‰
#   4. è¿è¡Œ: python scraper_optimized.py

import asyncio
import os
import re
import json
import logging
from datetime import datetime
from DrissionPage import ChromiumPage, ChromiumOptions
from bs4 import BeautifulSoup
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg

# =============================================================================
# é…ç½®åŒºåŸŸ - æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹
# =============================================================================

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
import pathlib
SCRIPT_DIR = pathlib.Path(__file__).resolve().parent
SCRAPER_ROOT = SCRIPT_DIR.parents[2]
env_path = SCRAPER_ROOT / '.env'
load_dotenv(dotenv_path=env_path)

# ä»£ç†é…ç½®ï¼ˆå¦‚æœä¸éœ€è¦ä»£ç†ï¼Œè®¾ç½®ä¸º Noneï¼‰
PROXY_URL = os.getenv("PROXY_URL", "http://127.0.0.1:10809")  # é»˜è®¤ä»£ç†åœ°å€
USE_PROXY = PROXY_URL and PROXY_URL.lower() != "none"  # æ˜¯å¦ä½¿ç”¨ä»£ç†

# APIé…ç½®
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# æ•°æ®åº“é…ç½®
NEON_DB_URL = os.getenv("DATABASE_URL")

# çˆ¬å–é…ç½®
WARM_UP_URL = "https://linux.do/"
RSS_URL = "https://linux.do/latest.rss"
POST_COUNT_LIMIT = int(os.getenv("POST_COUNT_LIMIT", "30"))  # çˆ¬å–å¸–å­æ•°é‡

# è¯„è®ºæŠ“å–é…ç½®
REQUEST_INTERVAL = 2  # å¸–å­é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
PAGE_LOAD_WAIT = 2  # é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
COMMENT_SUMMARY_LENGTH = 150  # è¯„è®ºæ‘˜è¦é•¿åº¦
TOP_COMMENTS_LIMIT = 10  # é«˜èµè¯„è®ºæ•°é‡é™åˆ¶

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 5

# AIè¯·æ±‚é—´éš”ï¼ˆé¿å…è§¦å‘é€Ÿç‡é™åˆ¶ï¼‰
AI_REQUEST_DELAY = 3

# æµè§ˆå™¨é…ç½®
HEADLESS = os.getenv("HEADLESS", "true").lower() == "true"  # æ˜¯å¦æ— å¤´æ¨¡å¼
CF_CHALLENGE_TIMEOUT = int(os.getenv("CF_CHALLENGE_TIMEOUT", "90"))
USER_DATA_DIR = os.getenv("USER_DATA_DIR")
PROFILE_DIRECTORY = os.getenv("PROFILE_DIRECTORY")
CHROME_PATH = os.getenv("CHROME_PATH")

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# å¯åŠ¨æ£€æŸ¥
# =============================================================================

def user_data_enabled():
    return USER_DATA_DIR and USER_DATA_DIR.lower() != "none"

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®"""
    logger.info("=" * 80)
    logger.info("Linux.do çˆ¬è™« - æœ¬åœ°PCè¿è¡Œç‰ˆ")
    logger.info("=" * 80)
    
    issues = []
    
    # æ£€æŸ¥ä»£ç†é…ç½®
    if USE_PROXY:
        logger.info(f"âœ“ ä»£ç†æ¨¡å¼: å¯ç”¨ ({PROXY_URL})")
        os.environ['HTTP_PROXY'] = PROXY_URL
        os.environ['HTTPS_PROXY'] = PROXY_URL
    else:
        logger.info("âœ“ ä»£ç†æ¨¡å¼: ç¦ç”¨ï¼ˆç›´è¿ï¼‰")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not DEEPSEEK_API_KEY:
        issues.append("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
    else:
        logger.info(f"âœ“ DeepSeek APIå¯†é’¥: {DEEPSEEK_API_KEY[:8]}...")
    
    # æ£€æŸ¥æ•°æ®åº“URL
    if not NEON_DB_URL:
        issues.append("âŒ æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
    else:
        logger.info(f"âœ“ æ•°æ®åº“è¿æ¥: å·²é…ç½®")
    
    # æ£€æŸ¥é…ç½®
    logger.info(f"âœ“ çˆ¬å–æ•°é‡: {POST_COUNT_LIMIT} ç¯‡å¸–å­")
    logger.info(f"âœ“ æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´' if HEADLESS else 'æœ‰ç•Œé¢'}")
    logger.info(f"âœ“ AIè¯·æ±‚é—´éš”: {AI_REQUEST_DELAY} ç§’")
    logger.info(f"âœ“ CFæŒ‘æˆ˜ç­‰å¾…: {CF_CHALLENGE_TIMEOUT} ç§’")
    if CHROME_PATH:
        logger.info(f"âœ“ Chromeè·¯å¾„: {CHROME_PATH}")
    if user_data_enabled():
        logger.info(f"âœ“ ä½¿ç”¨Chromeé…ç½®: {USER_DATA_DIR}")
        if PROFILE_DIRECTORY:
            logger.info(f"âœ“ Profileç›®å½•: {PROFILE_DIRECTORY}")
    logger.info("=" * 80)
    
    if issues:
        logger.error("\né…ç½®é—®é¢˜ï¼š")
        for issue in issues:
            logger.error(issue)
        logger.error("\nè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å˜é‡ï¼š")
        logger.error("  DEEPSEEK_API_KEY=your_api_key")
        logger.error("  DATABASE_URL=your_postgres_url")
        logger.error("  PROXY_URL=http://127.0.0.1:10809  # å¯é€‰ï¼Œä¸ç”¨ä»£ç†åˆ™è®¾ä¸º none")
        logger.error("  POST_COUNT_LIMIT=30  # å¯é€‰ï¼Œé»˜è®¤30")
        return False
    
    return True

# =============================================================================
# Cloudflare æŒ‘æˆ˜å¤„ç†
# =============================================================================

def get_page_html(page):
    try:
        return page.html
    except Exception:
        try:
            return page.html()
        except Exception:
            return ""

def wait_for_cloudflare_challenge(page, timeout=30):
    """
    ç­‰å¾… Cloudflare æŒ‘æˆ˜å®Œæˆ
    
    Args:
        page: DrissionPage é¡µé¢å¯¹è±¡
        timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Cloudflare æŒ‘æˆ˜é¡µé¢çš„ç‰¹å¾
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å« Cloudflare ç›¸å…³å†…å®¹
            try:
                title = page.title
            except Exception:
                title = ""
            
            # Cloudflare æŒ‘æˆ˜é¡µé¢çš„å¸¸è§æ ‡é¢˜
            cf_indicators = [
                "Just a moment",
                "Checking your browser",
                "Please wait",
                "Attention Required",
                "DDoS protection",
                "è¯·ç¨å€™",
            ]

            is_cf_title = any(indicator.lower() in title.lower() for indicator in cf_indicators)
            html = get_page_html(page)
            is_cf_html = "challenge-platform" in html or "turnstile" in html
            is_cf_challenge = is_cf_title or is_cf_html
            
            if not is_cf_challenge:
                # æ²¡æœ‰æ£€æµ‹åˆ° Cloudflare æŒ‘æˆ˜ï¼Œå¯ä»¥ç»§ç»­
                return True
            
            logger.info(f"â³ æ£€æµ‹åˆ° Cloudflare æŒ‘æˆ˜ï¼Œç­‰å¾…ä¸­... ({int(time.time() - start_time)}s)")
            time.sleep(2)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥ Cloudflare çŠ¶æ€æ—¶å‡ºé”™: {e}")
            time.sleep(1)
    
    logger.warning(f"âš ï¸ Cloudflare æŒ‘æˆ˜ç­‰å¾…è¶…æ—¶ ({timeout}s)ï¼Œç»§ç»­æ‰§è¡Œ...")
    return False

def _try_call(obj, names, *args, **kwargs):
    for name in names:
        if hasattr(obj, name):
            try:
                getattr(obj, name)(*args, **kwargs)
                return True
            except Exception:
                continue
    return False

def build_browser_page():
    options = ChromiumOptions()

    _try_call(options, ["auto_port"])

    if CHROME_PATH:
        set_ok = _try_call(options, ["set_browser_path"], CHROME_PATH)
        if not set_ok:
            set_ok = _try_call(options, ["set_paths"], browser_path=CHROME_PATH)
        if not set_ok:
            logger.warning("âš ï¸ æ— æ³•è®¾ç½®CHROME_PATHï¼Œä½¿ç”¨é»˜è®¤Chromeè·¯å¾„")

    if user_data_enabled():
        _try_call(options, ["set_user_data_path", "set_user_data_dir"], USER_DATA_DIR)
        if PROFILE_DIRECTORY:
            _try_call(options, ["set_argument"], f"--profile-directory={PROFILE_DIRECTORY}")

    if HEADLESS:
        if not _try_call(options, ["set_headless"], True):
            _try_call(options, ["set_argument"], "--headless=new")

    if USE_PROXY:
        if not _try_call(options, ["set_proxy"], PROXY_URL):
            _try_call(options, ["set_argument"], f"--proxy-server={PROXY_URL}")

    # åŸºç¡€åæ£€æµ‹å‚æ•°
    for arg in [
        "--no-sandbox",
        "--disable-dev-shm-usage",
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-site-isolation-trials",
        "--disable-web-security",
        "--window-size=1920,1080",
    ]:
        _try_call(options, ["set_argument"], arg)

    return ChromiumPage(options)

# =============================================================================
# é‡è¯•è£…é¥°å™¨
# =============================================================================

def retry_on_failure(max_retries=3, delay=5):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥")
                        raise e
                    logger.info(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
            return None
        return wrapper
    return decorator

# =============================================================================
# AIåˆ†æå‡½æ•°
# =============================================================================

def analyze_single_post_with_deepseek(post):
    """ä½¿ç”¨DeepSeekå¯¹å•ä¸ªå¸–å­è¿›è¡Œæ·±åº¦åˆ†æï¼ˆåŒ…å«çœŸå®è¯„è®ºï¼‰"""
    try:
        # æ¸…ç†æ¥¼ä¸»å†…å®¹
        main_content = post.get('content', '')
        clean_content = re.sub(r'<.*?>', ' ', main_content)
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        excerpt = (clean_content[:800] + '...') if len(clean_content) > 800 else clean_content

        if not excerpt or len(excerpt.strip()) < 10:
            logger.warning(f"âš ï¸ å¸–å­ '{post.get('title', 'N/A')}' å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡AIåˆ†æ")
            return {
                "error": "å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º",
                "core_issue": "N/A", 
                "key_info": [], 
                "post_type": "æœªçŸ¥", 
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }

        # æ„å»ºè¯„è®ºæ‘˜è¦ï¼ˆçœŸå®è¯„è®ºï¼‰
        comments_section = ""
        comments = post.get('comments', [])
        comment_count = len(comments)
        
        if comment_count > 0:
            # æŒ‰ç‚¹èµæ•°æ’åºï¼Œå–å‰Næ¡
            top_comments = sorted(comments, key=lambda x: x.get('likes', 0), reverse=True)[:TOP_COMMENTS_LIMIT]
            
            comments_summary = []
            for i, comment in enumerate(top_comments, 1):
                author = comment.get('author', 'åŒ¿å')
                content = comment.get('content', '')[:COMMENT_SUMMARY_LENGTH]  # é™åˆ¶é•¿åº¦
                likes = comment.get('likes', 0)
                comments_summary.append(f"{i}. [{author}] (ğŸ‘{likes}): {content}...")
            
            comments_section = f"""

**è¯„è®ºåŒºè®¨è®º** ({comment_count}æ¡çœŸå®è¯„è®º)ï¼š
{chr(10).join(comments_summary)}
"""
        else:
            comments_section = "\nï¼ˆæš‚æ— è¯„è®ºï¼‰"

        # æ„å»ºAIåˆ†ææç¤ºè¯ï¼ˆç¤¾åŒºé£å‘è§‚å¯Ÿç‰ˆ + çœŸå®è¯„è®ºæ•´åˆï¼‰
        prompt = f"""
ä½ æ˜¯ä¸€åLinux.doç¤¾åŒºè§‚å¯Ÿå‘˜ï¼Œæ“…é•¿æ•æ‰ç¤¾åŒºçƒ­ç‚¹ã€èµ„æºåˆ†äº«å’Œå®ç”¨æŠ€å·§ã€‚è¯·åŸºäº**æ¥¼ä¸»å†…å®¹å’Œè¯„è®ºåŒºçœŸå®è®¨è®º**ï¼Œç”Ÿæˆä¸€ä»½**è½»å¿«å®ç”¨çš„åˆ†ææŠ¥å‘Š**ã€‚

**åˆ†æè§’è‰²å®šä½**ï¼š
- ç¤¾åŒºé£å‘è§‚å¯Ÿï¼šåŸºäºçœŸå®è¯„è®ºåˆ†æè®¨è®ºè¶‹åŠ¿ã€ç¾¤ä½“æƒ…ç»ª
- å®ç”¨èµ„æºæŒ–æ˜ï¼šä»å¸–å­å’Œè¯„è®ºä¸­å‘ç°æœ‰ä»·å€¼çš„å·¥å…·ã€æ•™ç¨‹ã€ä¼˜æƒ ã€æŠ€å·§
- æ¥åœ°æ°”è¡¨è¾¾ï¼šé€šä¿—æ˜“æ‡‚ï¼Œè´´è¿‘ç”¨æˆ·å®é™…éœ€æ±‚
- å¿«é€Ÿè·å–è¦ç‚¹ï¼šè®©è¯»è€…3åˆ†é’Ÿäº†è§£æ ¸å¿ƒå†…å®¹å’Œç¤¾åŒºè®¨è®º

**é‡è¦è¦æ±‚**ï¼š
1. è¯­è¨€è½»å¿«ã€å£è¯­åŒ–ï¼Œé¿å…è¿‡äºå­¦æœ¯
2. é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§
3. **åŸºäºçœŸå®è¯„è®º**åˆ†æç¤¾åŒºè®¨è®ºçƒ­åº¦å’Œæƒ…ç»ªï¼ˆä¸æ˜¯æ¨æµ‹ï¼ï¼‰
4. æç‚¼è¯„è®ºä¸­çš„æœ‰ä»·å€¼è§‚ç‚¹å’Œè§£å†³æ–¹æ¡ˆ
5. è¿”å›æ ¼å¼å¿…é¡»æ˜¯çº¯JSONï¼Œä¸è¦åŒ…å«```json```æ ‡è®°

**å¸–å­å†…å®¹**ï¼š
- æ ‡é¢˜: {post['title']}
- æ¥¼ä¸»å†…å®¹: {excerpt}{comments_section}

**è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡º**ï¼š
{{
  "core_issue": "ç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­çš„æ ¸å¿ƒå†…å®¹ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰",
  "key_info": [
    "å…³é”®ä¿¡æ¯ç‚¹1ï¼ˆçªå‡ºå®ç”¨æ€§ï¼‰",
    "å…³é”®ä¿¡æ¯ç‚¹2ï¼ˆçªå‡ºå®ç”¨æ€§ï¼‰",
    "å…³é”®ä¿¡æ¯ç‚¹3ï¼ˆçªå‡ºå®ç”¨æ€§ï¼‰"
  ],
  "post_type": "ä»[æŠ€æœ¯é—®ç­”, èµ„æºåˆ†äº«, æ–°é—»èµ„è®¯, ä¼˜æƒ æ´»åŠ¨, æ—¥å¸¸é—²èŠ, æ±‚åŠ©, è®¨è®º, äº§å“è¯„æµ‹]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "detailed_analysis": "ç”Ÿæˆ500-800å­—çš„è½»å¿«åˆ†æï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼ˆç”¨markdownæ ¼å¼ï¼‰ï¼š\\n\\n## ğŸ“‹ è¯é¢˜èƒŒæ™¯\\nç®€è¦è¯´æ˜è¿™ä¸ªè¯é¢˜ä¸ºä»€ä¹ˆç«ã€ä¸ºä»€ä¹ˆé‡è¦ï¼ˆ2-3å¥è¯ï¼‰\\n\\n## ğŸ¯ æ ¸å¿ƒå†…å®¹\\nç”¨é€šä¿—è¯­è¨€å±•å¼€æ¥¼ä¸»å¸–å­çš„ä¸»è¦å†…å®¹ï¼Œçªå‡ºå…³é”®ç‚¹å’Œæœ‰ç”¨ä¿¡æ¯\\n\\n## ğŸ’¡ å®ç”¨æŠ€å·§/èµ„æºï¼ˆå¦‚é€‚ç”¨ï¼‰\\n**å·¥å…·/èµ„æº**ï¼šä»å¸–å­å’Œè¯„è®ºä¸­æå–çš„å…·ä½“å·¥å…·ã€ç½‘ç«™ã€è½¯ä»¶æ¨è\\n**æ“ä½œæ–¹æ³•**ï¼šç®€å•çš„ä½¿ç”¨æ­¥éª¤æˆ–é…ç½®æ–¹æ³•\\n**æ³¨æ„äº‹é¡¹**ï¼šè¯„è®ºä¸­æåˆ°çš„å¸¸è§å‘ç‚¹å’Œé¿å…æ–¹æ³•\\n\\n## ğŸ’¬ ç¤¾åŒºé£å‘ï¼ˆåŸºäº{comment_count}æ¡çœŸå®è¯„è®ºï¼‰\\n**è®¨è®ºçƒ­åº¦**ï¼šè¯„è®ºæ•°é‡å’Œæ´»è·ƒåº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰ï¼Œä¸»è¦è®¨è®ºæ–¹å‘\\n**æƒ…ç»ªå€¾å‘**ï¼šç»Ÿè®¡è¯„è®ºä¸­æ”¯æŒ/åå¯¹/ä¸­ç«‹çš„æ¯”ä¾‹ï¼Œå¼•ç”¨å…¸å‹è§‚ç‚¹\\n**çƒ­é—¨è§‚ç‚¹**ï¼šæ€»ç»“é«˜èµè¯„è®ºçš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆ2-3æ¡ï¼Œæ³¨æ˜ç‚¹èµæ•°ï¼‰\\n**äº‰è®®ç„¦ç‚¹**ï¼šè¯„è®ºåŒºçš„ä¸»è¦åˆ†æ­§å’Œä¸åŒçœ‹æ³•ï¼ˆå¦‚æœ‰ï¼‰\\n**å®ç”¨å»ºè®®**ï¼šè¯„è®ºä¸­æä¾›çš„è§£å†³æ–¹æ¡ˆæˆ–ç»éªŒåˆ†äº«\\n\\n## ğŸ”§ å®ç”¨ä»·å€¼\\n**é€‚ç”¨äººç¾¤**ï¼šè°æœ€éœ€è¦è¿™ä¸ªä¿¡æ¯\\n**ä½¿ç”¨åœºæ™¯**ï¼šä»€ä¹ˆæ—¶å€™èƒ½ç”¨ä¸Š\\n**æ¨èç†ç”±**ï¼šä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼ˆç»“åˆè¯„è®ºåé¦ˆï¼‰\\n\\n## ğŸš€ ä¸€å¥è¯æ€»ç»“\\nç”¨æœ€ç®€æ´çš„è¯æ¦‚æ‹¬è¿™ä¸ªå¸–å­çš„ä»·å€¼å’Œç¤¾åŒºå…±è¯†"
}}
"""
        
        # è°ƒç”¨DeepSeek API
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 2000,
            "temperature": 0.5
        }
        
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if USE_PROXY else None
        
        response = requests.post(
            DEEPSEEK_API_URL,
            headers=headers,
            json=data,
            proxies=proxies,
            timeout=90  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºéœ€è¦ç”Ÿæˆæ›´é•¿çš„æ·±åº¦åˆ†æ
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # è§£æJSON
            cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
            analysis_data = json.loads(cleaned_text)
            logger.info(f"âœ“ AIåˆ†ææˆåŠŸ: {post['title'][:40]}...")
            return analysis_data
        else:
            logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {
                "error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                "core_issue": "APIè°ƒç”¨å¤±è´¥", 
                "key_info": [], 
                "post_type": "é”™è¯¯", 
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }
        
    except json.JSONDecodeError as e:
        ai_resp = locals().get('ai_response', 'N/A')
        logger.error(f"âŒ JSONè§£æå¤±è´¥! AIè¿”å›: '{ai_resp[:100] if ai_resp else 'N/A'}...'")
        return {
            "error": "AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹", 
            "raw_response": ai_resp[:200] if ai_resp else "N/A",
            "core_issue": "AIåˆ†æå¤±è´¥", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }
    except requests.exceptions.Timeout:
        logger.error(f"âŒ DeepSeek APIè¯·æ±‚è¶…æ—¶")
        return {
            "error": "APIè¯·æ±‚è¶…æ—¶",
            "core_issue": "APIè¶…æ—¶", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {post.get('title', 'N/A')[:30]}... - {e}")
        return {
            "error": f"åˆ†æå¤±è´¥: {e}",
            "core_issue": "AIåˆ†æå¤±è´¥", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }

# =============================================================================
# çˆ¬è™«æ ¸å¿ƒå‡½æ•°
# =============================================================================

async def fetch_post_replies(page, post_url, post_title):
    """
    è®¿é—®å¸–å­è¯¦æƒ…é¡µå¹¶æå–çœŸå®è¯„è®º

    Args:
        page: DrissionPage é¡µé¢å¯¹è±¡
        post_url: å¸–å­URL
        post_title: å¸–å­æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        dict: åŒ…å«æ¥¼ä¸»å†…å®¹å’Œè¯„è®ºåˆ—è¡¨çš„å­—å…¸
    """
    try:
        logger.info(f"  â³ è®¿é—®å¸–å­: {post_title[:40]}...")
        page.get(post_url)

        wait_for_cloudflare_challenge(page, timeout=CF_CHALLENGE_TIMEOUT)
        await asyncio.sleep(3)

        html = get_page_html(page)
        soup = BeautifulSoup(html, "lxml")
        posts_elements = soup.select(".topic-post")

        if not posts_elements:
            logger.warning("    âš ï¸ æœªæ‰¾åˆ°.topic-postï¼Œé¢å¤–ç­‰å¾…5ç§’åé‡è¯•...")
            await asyncio.sleep(5)
            html = get_page_html(page)
            soup = BeautifulSoup(html, "lxml")
            posts_elements = soup.select(".topic-post")

        if not posts_elements:
            posts_elements = soup.select("article")
            if not posts_elements:
                logger.error("    âŒ é¡µé¢ç»“æ„å¼‚å¸¸ï¼Œæ— æ³•æ‰¾åˆ°å¸–å­å†…å®¹")
                debug_file = f"../debug_page_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info(f"    ğŸ“„ å·²ä¿å­˜é¡µé¢HTMLåˆ°: {debug_file}")
                return None
            logger.info("    âœ“ ä½¿ç”¨articleæ ‡ç­¾ä½œä¸ºå¤‡ç”¨")

        logger.info(f"    âœ… æ‰¾åˆ° {len(posts_elements)} ä¸ªå¸–å­ï¼ˆ1æ¥¼ä¸» + {len(posts_elements)-1}è¯„è®ºï¼‰")

        main_content = ""
        main_post_elem = posts_elements[0]
        content_elem = main_post_elem.select_one(".cooked")
        if content_elem:
            main_content = content_elem.get_text(strip=True)

        comments = []
        for i, reply_elem in enumerate(posts_elements[1:], start=1):
            try:
                author_elem = reply_elem.select_one(".username")
                author = author_elem.get_text(strip=True) if author_elem else ""

                content_elem = reply_elem.select_one(".cooked")
                content = content_elem.get_text(strip=True) if content_elem else ""

                likes = 0
                likes_elem = reply_elem.select_one(".likes")
                if likes_elem:
                    likes_text = likes_elem.get_text(strip=True)
                    likes = int("".join(filter(str.isdigit, likes_text)) or "0")

                time_str = ""
                time_elem = reply_elem.select_one(".post-date")
                if time_elem:
                    time_str = time_elem.get("title") or ""

                if content:
                    comments.append({
                        "author": author,
                        "content": content,
                        "likes": likes,
                        "time": time_str,
                    })
            except Exception as e:
                logger.warning(f"      âš ï¸ æå–è¯„è®º{i}å¤±è´¥: {e}")
                continue

        logger.info(f"    âœ… æˆåŠŸæå– {len(comments)} æ¡è¯„è®º")

        return {
            "main_content": main_content,
            "comments": comments,
            "total_replies": len(comments),
        }

    except Exception as e:
        logger.error(f"    âŒ è®¿é—®å¸–å­è¯¦æƒ…é¡µå¤±è´¥: {e}")
        import traceback
        logger.error(f"    è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        try:
            html = get_page_html(page)
            debug_file = f"linuxdo/debug_page_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(html)
            logger.info(f"    ğŸ“„ å·²ä¿å­˜é¡µé¢HTMLåˆ°: {debug_file}")
        except Exception:
            pass
        return None

async def fetch_posts_with_replies(page, posts):
    """
    ä¸ºæ¯ä¸ªå¸–å­æŠ“å–çœŸå®è¯„è®º
    
    Args:
        page: DrissionPage é¡µé¢å¯¹è±¡
        posts: å¸–å­åˆ—è¡¨
    
    Returns:
        list: åŒ…å«è¯„è®ºçš„å¸–å­åˆ—è¡¨
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ” å¼€å§‹æŠ“å–å¸–å­è¯¦æƒ…é¡µï¼ˆçœŸå®è¯„è®ºï¼‰")
    logger.info(f"{'='*60}\n")
    
    enhanced_posts = []
    
    for i, post in enumerate(posts):
        logger.info(f"[{i+1}/{len(posts)}] å¤„ç†: {post['title'][:50]}...")
        
        # è®¿é—®å¸–å­è¯¦æƒ…é¡µè·å–è¯„è®º
        replies_data = await fetch_post_replies(page, post['link'], post['title'])
        
        if replies_data:
            # åˆå¹¶æ•°æ®
            post['main_content'] = replies_data['main_content']
            post['comments'] = replies_data['comments']
            post['total_replies'] = replies_data['total_replies']
            
            # å¦‚æœåŸæ¥çš„contentæ˜¯ä»RSSæ¥çš„ï¼Œç°åœ¨æ›¿æ¢ä¸ºçœŸå®å†…å®¹
            if replies_data['main_content']:
                post['content'] = replies_data['main_content']
        else:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä¿æŒåŸæœ‰çš„RSS description
            post['main_content'] = post.get('content', '')
            post['comments'] = []
            post['total_replies'] = 0
            logger.warning(f"    âš ï¸ ä½¿ç”¨RSSæè¿°ä½œä¸ºé™çº§æ–¹æ¡ˆ")
        
        enhanced_posts.append(post)
        
        # è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«å°
        if i < len(posts) - 1:
            await asyncio.sleep(REQUEST_INTERVAL)
    
    logger.info(f"\nâœ… å®Œæˆå¸–å­è¯¦æƒ…æŠ“å–")
    logger.info(f"   æ€»å¸–å­æ•°: {len(enhanced_posts)}")
    logger.info(f"   æ€»è¯„è®ºæ•°: {sum(p.get('total_replies', 0) for p in enhanced_posts)}\n")
    
    return enhanced_posts

@retry_on_failure(max_retries=MAX_RETRIES, delay=RETRY_DELAY)
async def fetch_linuxdo_posts():
    """çˆ¬å–Linux.doå¸–å­"""
    logger.info("ğŸš€ å¼€å§‹çˆ¬å–Linux.doå¸–å­...")
    
    page = None
    try:
        page = build_browser_page()

        # é¢„çƒ­ï¼šè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯
        logger.info(f"â³ è®¿é—®é¦–é¡µé¢„çƒ­: {WARM_UP_URL}")
        page.get(WARM_UP_URL)

        # æ£€æµ‹å¹¶ç­‰å¾… Cloudflare æŒ‘æˆ˜
        wait_for_cloudflare_challenge(page, timeout=CF_CHALLENGE_TIMEOUT)

        logger.info("âœ“ é¢„çƒ­å®Œæˆ")
        await asyncio.sleep(3)

        # è®¿é—®RSSæº
        logger.info(f"â³ è®¿é—®RSSæº: {RSS_URL}")
        page.get(RSS_URL)
        await asyncio.sleep(2)

        # è·å–RSSå†…å®¹
        rss_text = get_page_html(page)

        # ä¿å­˜è°ƒè¯•æ–‡ä»¶
        debug_filename = f"debug_rss_content_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(rss_text)
        logger.info(f"âœ“ å·²ä¿å­˜è°ƒè¯•æ–‡ä»¶: {debug_filename} ({len(rss_text)} å­—ç¬¦)")

        # è§£æRSSå†…å®¹
        all_posts = []
        logger.info("â³ è§£æRSSå†…å®¹...")

        # æ–¹å¼1ï¼šç›´æ¥è§£æXML
        try:
            root = ET.fromstring(rss_text)
            items = root.findall('.//channel/item')
            logger.info(f"âœ“ æ‰¾åˆ° {len(items)} ä¸ªå¸–å­")

            for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')

                if title is not None and link is not None:
                    title_text = title.text
                    link_text = link.text
                    description_text = description.text if description is not None else ""

                    match = re.search(r'/t/[^/]+/(\d+)', link_text)
                    if match:
                        topic_id = match.group(1)
                        all_posts.append({
                            "title": title_text,
                            "link": link_text,
                            "id": topic_id,
                            "description": description_text,
                        })

        except ET.ParseError:
            # æ–¹å¼2ï¼šä»HTMLä¸­æå–XML
            logger.info("â³ å°è¯•ä»HTMLä¸­æå–XML...")
            try:
                from bs4 import BeautifulSoup
                import html

                soup = BeautifulSoup(rss_text, 'html.parser')
                pre_tag = soup.find('pre')

                if pre_tag:
                    xml_content = html.unescape(pre_tag.get_text())
                    root = ET.fromstring(xml_content)
                    items = root.findall('.//channel/item')

                    for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                        title = item.find('title')
                        link = item.find('link')
                        description = item.find('description')

                        if title is not None and link is not None:
                            match = re.search(r'/t/[^/]+/(\d+)', link.text)
                            if match:
                                all_posts.append({
                                    "title": title.text,
                                    "link": link.text,
                                    "id": match.group(1),
                                    "description": description.text if description is not None else "",
                                })

            except Exception as e2:
                logger.error(f"âŒ HTMLæå–å¤±è´¥: {e2}")

        # æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        if not all_posts:
            logger.info("â³ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰...")
            title_pattern = r'<title>([^<]+)</title>'
            link_pattern = r'<link>([^<]+)</link>'
            titles = re.findall(title_pattern, rss_text)
            links = re.findall(link_pattern, rss_text)

            for i, (title, link) in enumerate(zip(titles, links)):
                if i >= POST_COUNT_LIMIT:
                    break
                match = re.search(r'/t/[^/]+/(\d+)', link)
                if match:
                    all_posts.append({
                        "title": title,
                        "link": link,
                        "id": match.group(1),
                        "description": "",
                    })

        logger.info(f"âœ“ æˆåŠŸè§£æ {len(all_posts)} ç¯‡å¸–å­")

        if not all_posts:
            logger.error("âŒ æœªèƒ½è§£æåˆ°ä»»ä½•å¸–å­")
            return []

        # å¤„ç†å¸–å­å†…å®¹
        posts_with_content = []
        for i, post in enumerate(all_posts):
            rss_content = post.get('description', '')

            if rss_content:
                # æå–äº’åŠ¨æ•°æ®ï¼š"X ä¸ªå¸–å­ - Y ä½å‚ä¸è€…"
                replies_count = 0
                participants_count = 0
                try:
                    match = re.search(r'(\d+)\s*ä¸ªå¸–å­\s*-\s*(\d+)\s*ä½å‚ä¸è€…', rss_content)
                    if match:
                        replies_count = int(match.group(1))
                        participants_count = int(match.group(2))
                except Exception:
                    pass

                # æ¸…ç†HTMLæ ‡ç­¾ï¼Œå¹¶ç§»é™¤äº’åŠ¨ç»Ÿè®¡è¯­å¥
                clean_content = re.sub(r'<[^>]+>', ' ', rss_content)
                clean_content = re.sub(r'\d+\s*ä¸ªå¸–å­\s*-\s*\d+\s*ä½å‚ä¸è€…', '', clean_content)
                clean_content = ' '.join(clean_content.split())

                if len(clean_content.strip()) > 10:
                    post['content'] = clean_content
                    post['replies_count'] = replies_count
                    post['participants_count'] = participants_count
                    logger.info(f"  [{i+1}/{len(all_posts)}] {post['title'][:50]}... ({len(clean_content)} å­—ç¬¦)")
                else:
                    post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
            else:
                post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"

            posts_with_content.append(post)

            if i < len(all_posts) - 1:
                await asyncio.sleep(1)

        logger.info(f"âœ“ è·å–åˆ° {len(posts_with_content)} ç¯‡å¸–å­ï¼ˆä»…RSSæè¿°ï¼‰")

        # æ–°å¢ï¼šè®¿é—®æ¯ä¸ªå¸–å­è¯¦æƒ…é¡µæŠ“å–çœŸå®è¯„è®º
        posts_with_replies = await fetch_posts_with_replies(page, posts_with_content)

        return posts_with_replies

    except Exception as e:
        logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        try:
            if page:
                html = get_page_html(page)
                debug_file = f"linuxdo/debug_page_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info(f"ğŸ“„ å·²ä¿å­˜é¡µé¢HTMLåˆ°: {debug_file}")
        except Exception:
            pass
        raise e
    finally:
        if page:
            try:
                page.quit()
            except Exception:
                pass

# =============================================================================
# æ•°æ®åº“æ“ä½œ
# =============================================================================

async def create_posts_table():
    """åˆ›å»ºæ•°æ®åº“è¡¨"""
    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL, timeout=60)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                detailed_analysis TEXT,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("âœ“ æ•°æ®åº“è¡¨æ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """æ’å…¥å¸–å­æ•°æ®åˆ°æ•°æ®åº“"""
    max_db_retries = 3
    for attempt in range(max_db_retries):
        conn = None
        try:
            conn = await asyncpg.connect(NEON_DB_URL, timeout=60)
            logger.info(f"â³ å¼€å§‹æ’å…¥æ•°æ®åˆ°æ•°æ®åº“ (å°è¯• {attempt+1}/{max_db_retries})...")
            
            success_count = 0
            # ... (rest of the insertion logic remains the same, but indented) ...
            for post in posts_data:
                try:
                    post_id = post.get('id')
                    title = post.get('title')
                    url = post.get('link')
                    analysis = post.get('analysis', {})
                    core_issue = analysis.get('core_issue')
                    key_info = json.dumps(analysis.get('key_info', []))
                    post_type = analysis.get('post_type')
                    value_assessment = analysis.get('value_assessment')
                    detailed_analysis = analysis.get('detailed_analysis')

                    # æ–°å­—æ®µï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼šreplies_count / participants_count
                    replies_count = int(post.get('replies_count') or 0)
                    participants_count = int(post.get('participants_count') or 0)

                    # ä¼˜å…ˆå°è¯•æ’å…¥åŒ…å«æ–°åˆ—çš„è¯­å¥ï¼›å¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ—§è¯­å¥ï¼ˆå…¼å®¹æœªè¿ç§»çš„è¡¨ç»“æ„ï¼‰
                    try:
                        await conn.execute("""
                            INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                            ON CONFLICT (id) DO UPDATE SET
                                title = EXCLUDED.title,
                                url = EXCLUDED.url,
                                core_issue = EXCLUDED.core_issue,
                                key_info = EXCLUDED.key_info,
                                post_type = EXCLUDED.post_type,
                                value_assessment = EXCLUDED.value_assessment,
                                detailed_analysis = EXCLUDED.detailed_analysis,
                                replies_count = EXCLUDED.replies_count,
                                participants_count = EXCLUDED.participants_count,
                                timestamp = CURRENT_TIMESTAMP;
                        """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                    except Exception as insert_err:
                        # logger.warning(f"posts è¡¨ç¼ºå°‘æ–°åˆ—ï¼Œå›é€€æ—§æ’å…¥è¯­å¥: {insert_err}")
                        await conn.execute("""
                            INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                            ON CONFLICT (id) DO UPDATE SET
                                title = EXCLUDED.title,
                                url = EXCLUDED.url,
                                core_issue = EXCLUDED.core_issue,
                                key_info = EXCLUDED.key_info,
                                post_type = EXCLUDED.post_type,
                                value_assessment = EXCLUDED.value_assessment,
                                detailed_analysis = EXCLUDED.detailed_analysis,
                                timestamp = CURRENT_TIMESTAMP;
                        """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                    
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ æ’å…¥å¸–å­å¤±è´¥ [{post.get('id', 'N/A')}]: {e}")
                    continue
            
            logger.info(f"âœ“ æˆåŠŸæ’å…¥ {success_count}/{len(posts_data)} æ¡æ•°æ®")
            return success_count > 0

        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“æ“ä½œå¤±è´¥ (å°è¯• {attempt+1}): {e}")
            if attempt < max_db_retries - 1:
                await asyncio.sleep(5)
            else:
                logger.error(f"âŒ æœ€ç»ˆæ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
                return False
        finally:
            if conn:
                await conn.close()

# =============================================================================
# AIæŠ¥å‘Šç”Ÿæˆ
# =============================================================================

def generate_ai_analysis(posts_data):
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
    if not posts_data:
        logger.warning("âš ï¸ æ²¡æœ‰å¸–å­æ•°æ®")
        return {"summary_analysis": {"error": "æ²¡æœ‰å¸–å­æ•°æ®"}, "processed_posts": []}

    logger.info("â³ å¼€å§‹AIåˆ†æ...")
    processed_posts = []
    
    for i, post in enumerate(posts_data):
        logger.info(f"  [{i+1}/{len(posts_data)}] åˆ†æ: {post['title'][:40]}...")
        
        try:
            post['analysis'] = analyze_single_post_with_deepseek(post)
            processed_posts.append(post)
            
            if i < len(posts_data) - 1:
                time.sleep(AI_REQUEST_DELAY)
                
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            post['analysis'] = {
                "error": f"åˆ†æå¤±è´¥: {e}",
                "core_issue": "åˆ†æå¤±è´¥",
                "key_info": [],
                "post_type": "é”™è¯¯",
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }
            processed_posts.append(post)
            
    logger.info("âœ“ AIåˆ†æå®Œæˆ")

    return {
        "summary_analysis": {"status": "success"},
        "processed_posts": processed_posts
    }

# =============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# =============================================================================

def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Š"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"../data/linux.do_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})",
                "source": "Linux.do",
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("")
    
    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        return False
    
    try:
        # 1. åˆ›å»ºæ•°æ®åº“è¡¨
        db_available = True
        if not await create_posts_table():
            logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå°†ä»…ç”ŸæˆJSONæŠ¥å‘Š")
            db_available = False
        
        # 2. çˆ¬å–å¸–å­
        posts_data = await fetch_linuxdo_posts()
        
        if not posts_data:
            logger.error("âŒ æœªèƒ½è·å–å¸–å­æ•°æ®")
            return False
        
        logger.info(f"âœ“ è·å–åˆ° {len(posts_data)} ç¯‡å¸–å­")
        
        # 3. AIåˆ†æ
        report_data = generate_ai_analysis(posts_data)
        
        # 4. æ’å…¥æ•°æ®åº“
        if report_data.get('processed_posts') and db_available:
            await insert_posts_into_db(report_data['processed_posts'])
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        json_file = generate_json_report(report_data, len(posts_data))
        
        # å®Œæˆ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info(f"âœ“ ä»»åŠ¡å®Œæˆï¼")
        logger.info(f"  å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        logger.info(f"  å¤„ç†å¸–å­: {len(posts_data)} ç¯‡")
        logger.info(f"  ç”Ÿæˆæ–‡ä»¶: {json_file}")
        logger.info("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
