# scraper_optimized.py - ä¼˜åŒ–ç‰ˆLinux.doçˆ¬è™«ï¼ˆæœ¬åœ°PCè¿è¡Œç‰ˆï¼‰
# åŠŸèƒ½ï¼šçˆ¬å–Linux.doè®ºå›RSSï¼Œä½¿ç”¨DeepSeek AIåˆ†æï¼Œå­˜å…¥Neonæ•°æ®åº“
# ä½¿ç”¨æ–¹æ³•ï¼š
#   1. ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install playwright playwright-stealth asyncpg requests python-dotenv
#   2. å®‰è£…æµè§ˆå™¨: python -m playwright install chromium
#   3. é…ç½® .env æ–‡ä»¶ï¼ˆè§ä¸‹æ–¹é…ç½®è¯´æ˜ï¼‰
#   4. è¿è¡Œ: python scraper_optimized.py

import asyncio
import os
import re
import json
import logging
from datetime import datetime
from playwright.async_api import async_playwright
from playwright_stealth import Stealth
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg

# =============================================================================
# é…ç½®åŒºåŸŸ - æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹
# =============================================================================

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
import pathlib
env_path = pathlib.Path(__file__).parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# ä»£ç†é…ç½®ï¼ˆå¦‚æœä¸éœ€è¦ä»£ç†ï¼Œè®¾ç½®ä¸º Noneï¼‰
PROXY_URL = os.getenv("PROXY_URL", "http://127.0.0.1:10809")  # é»˜è®¤ä»£ç†åœ°å€
USE_PROXY = PROXY_URL and PROXY_URL.lower() != "none"  # æ˜¯å¦ä½¿ç”¨ä»£ç†

# APIé…ç½®
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# æ•°æ®åº“é…ç½®
NEON_DB_URL = os.getenv("DATABASE_URL")

# çˆ¬å–é…ç½®
WARM_UP_URL = "https://linux.do/"
RSS_URL = "https://linux.do/latest.rss"
POST_COUNT_LIMIT = int(os.getenv("POST_COUNT_LIMIT", "30"))  # çˆ¬å–å¸–å­æ•°é‡

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 5

# AIè¯·æ±‚é—´éš”ï¼ˆé¿å…è§¦å‘é€Ÿç‡é™åˆ¶ï¼‰
AI_REQUEST_DELAY = 3

# æµè§ˆå™¨é…ç½®
HEADLESS = os.getenv("HEADLESS", "true").lower() == "true"  # æ˜¯å¦æ— å¤´æ¨¡å¼

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# å¯åŠ¨æ£€æŸ¥
# =============================================================================

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®"""
    logger.info("=" * 80)
    logger.info("Linux.do çˆ¬è™« - æœ¬åœ°PCè¿è¡Œç‰ˆ")
    logger.info("=" * 80)
    
    issues = []
    
    # æ£€æŸ¥ä»£ç†é…ç½®
    if USE_PROXY:
        logger.info(f"âœ“ ä»£ç†æ¨¡å¼: å¯ç”¨ ({PROXY_URL})")
        os.environ['HTTP_PROXY'] = PROXY_URL
        os.environ['HTTPS_PROXY'] = PROXY_URL
    else:
        logger.info("âœ“ ä»£ç†æ¨¡å¼: ç¦ç”¨ï¼ˆç›´è¿ï¼‰")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not DEEPSEEK_API_KEY:
        issues.append("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
    else:
        logger.info(f"âœ“ DeepSeek APIå¯†é’¥: {DEEPSEEK_API_KEY[:8]}...")
    
    # æ£€æŸ¥æ•°æ®åº“URL
    if not NEON_DB_URL:
        issues.append("âŒ æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
    else:
        logger.info(f"âœ“ æ•°æ®åº“è¿æ¥: å·²é…ç½®")
    
    # æ£€æŸ¥é…ç½®
    logger.info(f"âœ“ çˆ¬å–æ•°é‡: {POST_COUNT_LIMIT} ç¯‡å¸–å­")
    logger.info(f"âœ“ æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´' if HEADLESS else 'æœ‰ç•Œé¢'}")
    logger.info(f"âœ“ AIè¯·æ±‚é—´éš”: {AI_REQUEST_DELAY} ç§’")
    logger.info("=" * 80)
    
    if issues:
        logger.error("\né…ç½®é—®é¢˜ï¼š")
        for issue in issues:
            logger.error(issue)
        logger.error("\nè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å˜é‡ï¼š")
        logger.error("  DEEPSEEK_API_KEY=your_api_key")
        logger.error("  DATABASE_URL=your_postgres_url")
        logger.error("  PROXY_URL=http://127.0.0.1:10809  # å¯é€‰ï¼Œä¸ç”¨ä»£ç†åˆ™è®¾ä¸º none")
        logger.error("  POST_COUNT_LIMIT=30  # å¯é€‰ï¼Œé»˜è®¤30")
        return False
    
    return True

# =============================================================================
# é‡è¯•è£…é¥°å™¨
# =============================================================================

def retry_on_failure(max_retries=3, delay=5):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥")
                        raise e
                    logger.info(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
            return None
        return wrapper
    return decorator

# =============================================================================
# AIåˆ†æå‡½æ•°
# =============================================================================

def analyze_single_post_with_deepseek(post):
    """ä½¿ç”¨DeepSeekå¯¹å•ä¸ªå¸–å­è¿›è¡Œæ·±åº¦åˆ†æ"""
    try:
        # æ¸…ç†å†…å®¹
        clean_content = re.sub(r'<.*?>', ' ', post.get('content', ''))
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        excerpt = (clean_content[:800] + '...') if len(clean_content) > 800 else clean_content

        if not excerpt or len(excerpt.strip()) < 10:
            logger.warning(f"âš ï¸ å¸–å­ '{post.get('title', 'N/A')}' å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡AIåˆ†æ")
            return {
                "error": "å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º",
                "core_issue": "N/A", 
                "key_info": [], 
                "post_type": "æœªçŸ¥", 
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }

        # æ„å»ºAIåˆ†ææç¤ºè¯
        prompt = f"""
        ä½ æ˜¯ä¸€åèµ„æ·±çš„è®ºå›å†…å®¹åˆ†æå¸ˆã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹å¸–å­å†…å®¹ï¼Œå¹¶ç”Ÿæˆä¸€ä»½**æ·±åº¦åˆ†ææŠ¥å‘Š**ï¼Œè®©è¯»è€…æ— éœ€æŸ¥çœ‹åŸæ–‡å³å¯å…¨é¢ç†è§£ã€‚
        ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

        **å¸–å­æ ‡é¢˜**: {post['title']}
        **å†…å®¹èŠ‚é€‰**: {excerpt}

        **è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
        {{
          "core_issue": "ç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­çš„æ ¸å¿ƒè®®é¢˜",
          "key_info": [
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹1",
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹2",
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹3"
          ],
          "post_type": "ä»[æŠ€æœ¯é—®ç­”, èµ„æºåˆ†äº«, æ–°é—»èµ„è®¯, ä¼˜æƒ æ´»åŠ¨, æ—¥å¸¸é—²èŠ, æ±‚åŠ©, è®¨è®º, äº§å“è¯„æµ‹]ä¸­é€‰æ‹©ä¸€ä¸ª",
          "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª",
          "detailed_analysis": "ç”Ÿæˆ300-800å­—çš„æ·±åº¦åˆ†æï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼ˆç”¨markdownæ ¼å¼ï¼‰ï¼š\\n\\n## ğŸ“‹ èƒŒæ™¯ä»‹ç»\\nç®€è¦è¯´æ˜è¿™ä¸ªè¯é¢˜ä¸ºä»€ä¹ˆé‡è¦ã€ç›¸å…³èƒŒæ™¯ä¿¡æ¯\\n\\n## ğŸ¯ æ ¸å¿ƒå†…å®¹\\nè¯¦ç»†å±•å¼€å¸–å­çš„ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ç‚¹\\n\\n## ğŸ’¡ æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚é€‚ç”¨ï¼‰\\n- å…·ä½“çš„æŠ€æœ¯æ–¹æ¡ˆã€å·¥å…·ã€ä»£ç è¦ç‚¹\\n- å®ç°æ­¥éª¤æˆ–æ¶æ„è®¾è®¡\\n- æ€§èƒ½ä¼˜åŒ–æˆ–é…ç½®æ–¹æ³•\\n\\n## ğŸ’¬ è®¨è®ºä»·å€¼\\n- è¿™ä¸ªè¯é¢˜å¯èƒ½å¼•å‘çš„è®¨è®ºæ–¹å‘\\n- ç¤¾åŒºå¯èƒ½å…³æ³¨çš„ç„¦ç‚¹\\n- å¸¸è§çš„ç–‘é—®æˆ–äº‰è®®ç‚¹\\n\\n## ğŸ”§ å®ç”¨ä»·å€¼\\n- å¦‚ä½•åº”ç”¨è¿™äº›ä¿¡æ¯\\n- ç›¸å…³èµ„æºé“¾æ¥æˆ–æ¨è\\n- æ³¨æ„äº‹é¡¹æˆ–é™åˆ¶\\n\\n## ğŸš€ æ€»ç»“ä¸å»ºè®®\\nè¶‹åŠ¿åˆ†æã€ä¸ªäººå»ºè®®æˆ–å»¶ä¼¸æ€è€ƒ"
        }}
        """
        
        # è°ƒç”¨DeepSeek API
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 2000,
            "temperature": 0.5
        }
        
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if USE_PROXY else None
        
        response = requests.post(
            DEEPSEEK_API_URL,
            headers=headers,
            json=data,
            proxies=proxies,
            timeout=90  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºéœ€è¦ç”Ÿæˆæ›´é•¿çš„æ·±åº¦åˆ†æ
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # è§£æJSON
            cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
            analysis_data = json.loads(cleaned_text)
            logger.info(f"âœ“ AIåˆ†ææˆåŠŸ: {post['title'][:40]}...")
            return analysis_data
        else:
            logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {
                "error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                "core_issue": "APIè°ƒç”¨å¤±è´¥", 
                "key_info": [], 
                "post_type": "é”™è¯¯", 
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }
        
    except json.JSONDecodeError as e:
        ai_resp = locals().get('ai_response', 'N/A')
        logger.error(f"âŒ JSONè§£æå¤±è´¥! AIè¿”å›: '{ai_resp[:100] if ai_resp else 'N/A'}...'")
        return {
            "error": "AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹", 
            "raw_response": ai_resp[:200] if ai_resp else "N/A",
            "core_issue": "AIåˆ†æå¤±è´¥", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }
    except requests.exceptions.Timeout:
        logger.error(f"âŒ DeepSeek APIè¯·æ±‚è¶…æ—¶")
        return {
            "error": "APIè¯·æ±‚è¶…æ—¶",
            "core_issue": "APIè¶…æ—¶", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {post.get('title', 'N/A')[:30]}... - {e}")
        return {
            "error": f"åˆ†æå¤±è´¥: {e}",
            "core_issue": "AIåˆ†æå¤±è´¥", 
            "key_info": [], 
            "post_type": "é”™è¯¯", 
            "value_assessment": "ä½",
            "detailed_analysis": ""
        }

# =============================================================================
# çˆ¬è™«æ ¸å¿ƒå‡½æ•°
# =============================================================================

@retry_on_failure(max_retries=MAX_RETRIES, delay=RETRY_DELAY)
async def fetch_linuxdo_posts():
    """çˆ¬å–Linux.doå¸–å­"""
    logger.info("ğŸš€ å¼€å§‹çˆ¬å–Linux.doå¸–å­...")
    
    async with async_playwright() as p:
        browser = None
        try:
            # å¯åŠ¨æµè§ˆå™¨
            launch_options = {
                "headless": HEADLESS,
                "args": ['--no-sandbox', '--disable-dev-shm-usage']
            }
            if USE_PROXY:
                launch_options["proxy"] = {"server": PROXY_URL}
            
            browser = await p.chromium.launch(**launch_options)
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                extra_http_headers={
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                }
            )
            
            # åº”ç”¨åçˆ¬è™«ç­–ç•¥
            stealth = Stealth()
            await stealth.apply_stealth_async(context)
            page = await context.new_page()

            # é¢„çƒ­ï¼šè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯
            logger.info(f"â³ è®¿é—®é¦–é¡µé¢„çƒ­: {WARM_UP_URL}")
            await page.goto(WARM_UP_URL, wait_until="domcontentloaded", timeout=60000)
            logger.info("âœ“ é¢„çƒ­å®Œæˆ")
            await asyncio.sleep(3)

            # è®¿é—®RSSæº
            logger.info(f"â³ è®¿é—®RSSæº: {RSS_URL}")
            await page.goto(RSS_URL, wait_until="networkidle", timeout=120000)
            await asyncio.sleep(2)

            # è·å–RSSå†…å®¹
            rss_text = await page.evaluate("document.documentElement.outerHTML")
            
            # ä¿å­˜è°ƒè¯•æ–‡ä»¶
            debug_filename = f"debug_rss_content_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write(rss_text)
            logger.info(f"âœ“ å·²ä¿å­˜è°ƒè¯•æ–‡ä»¶: {debug_filename} ({len(rss_text)} å­—ç¬¦)")

            # è§£æRSSå†…å®¹
            all_posts = []
            logger.info("â³ è§£æRSSå†…å®¹...")

            # æ–¹å¼1ï¼šç›´æ¥è§£æXML
            try:
                root = ET.fromstring(rss_text)
                items = root.findall('.//channel/item')
                logger.info(f"âœ“ æ‰¾åˆ° {len(items)} ä¸ªå¸–å­")
                
                for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                    title = item.find('title')
                    link = item.find('link')
                    description = item.find('description')
                    
                    if title is not None and link is not None:
                        title_text = title.text
                        link_text = link.text
                        description_text = description.text if description is not None else ""
                        
                        match = re.search(r'/t/[^/]+/(\d+)', link_text)
                        if match:
                            topic_id = match.group(1)
                            all_posts.append({
                                "title": title_text,
                                "link": link_text,
                                "id": topic_id,
                                "description": description_text
                            })
                        
            except ET.ParseError:
                # æ–¹å¼2ï¼šä»HTMLä¸­æå–XML
                logger.info("â³ å°è¯•ä»HTMLä¸­æå–XML...")
                try:
                    from bs4 import BeautifulSoup
                    import html
                    
                    soup = BeautifulSoup(rss_text, 'html.parser')
                    pre_tag = soup.find('pre')
                    
                    if pre_tag:
                        xml_content = html.unescape(pre_tag.get_text())
                        root = ET.fromstring(xml_content)
                        items = root.findall('.//channel/item')
                        
                        for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                            title = item.find('title')
                            link = item.find('link')
                            description = item.find('description')
                            
                            if title is not None and link is not None:
                                match = re.search(r'/t/[^/]+/(\d+)', link.text)
                                if match:
                                    all_posts.append({
                                        "title": title.text,
                                        "link": link.text,
                                        "id": match.group(1),
                                        "description": description.text if description is not None else ""
                                    })
                                    
                except Exception as e2:
                    logger.error(f"âŒ HTMLæå–å¤±è´¥: {e2}")

            # æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
            if not all_posts:
                logger.info("â³ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰...")
                title_pattern = r'<title>([^<]+)</title>'
                link_pattern = r'<link>([^<]+)</link>'
                titles = re.findall(title_pattern, rss_text)
                links = re.findall(link_pattern, rss_text)

                for i, (title, link) in enumerate(zip(titles, links)):
                    if i >= POST_COUNT_LIMIT:
                        break
                    match = re.search(r'/t/[^/]+/(\d+)', link)
                    if match:
                        all_posts.append({
                            "title": title,
                            "link": link,
                            "id": match.group(1),
                            "description": ""
                        })
            
            logger.info(f"âœ“ æˆåŠŸè§£æ {len(all_posts)} ç¯‡å¸–å­")
            
            if not all_posts:
                logger.error("âŒ æœªèƒ½è§£æåˆ°ä»»ä½•å¸–å­")
                return []
            
            # å¤„ç†å¸–å­å†…å®¹
            posts_with_content = []
            for i, post in enumerate(all_posts):
                rss_content = post.get('description', '')

                if rss_content:
                    # æå–äº’åŠ¨æ•°æ®ï¼š"X ä¸ªå¸–å­ - Y ä½å‚ä¸è€…"
                    replies_count = 0
                    participants_count = 0
                    try:
                        match = re.search(r'(\d+)\s*ä¸ªå¸–å­\s*-\s*(\d+)\s*ä½å‚ä¸è€…', rss_content)
                        if match:
                            replies_count = int(match.group(1))
                            participants_count = int(match.group(2))
                    except Exception:
                        pass

                    # æ¸…ç†HTMLæ ‡ç­¾ï¼Œå¹¶ç§»é™¤äº’åŠ¨ç»Ÿè®¡è¯­å¥
                    clean_content = re.sub(r'<[^>]+>', ' ', rss_content)
                    clean_content = re.sub(r'\d+\s*ä¸ªå¸–å­\s*-\s*\d+\s*ä½å‚ä¸è€…', '', clean_content)
                    clean_content = ' '.join(clean_content.split())

                    if len(clean_content.strip()) > 10:
                        post['content'] = clean_content
                        post['replies_count'] = replies_count
                        post['participants_count'] = participants_count
                        logger.info(f"  [{i+1}/{len(all_posts)}] {post['title'][:50]}... ({len(clean_content)} å­—ç¬¦)")
                    else:
                        post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                else:
                    post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                
                posts_with_content.append(post)

                if i < len(all_posts) - 1:
                    await asyncio.sleep(1)

            logger.info(f"âœ“ è·å–åˆ° {len(posts_with_content)} ç¯‡å¸–å­å†…å®¹")
            return posts_with_content

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            # å°è¯•ä¿å­˜é”™è¯¯æˆªå›¾
            try:
                if browser:
                    page = await browser.new_page()
                    screenshot_path = f"error_screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                    await page.screenshot(path=screenshot_path)
                    logger.info(f"âœ“ å·²ä¿å­˜é”™è¯¯æˆªå›¾: {screenshot_path}")
            except:
                pass
            raise e
        finally:
            if browser:
                await browser.close()

# =============================================================================
# æ•°æ®åº“æ“ä½œ
# =============================================================================

async def create_posts_table():
    """åˆ›å»ºæ•°æ®åº“è¡¨"""
    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                detailed_analysis TEXT,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("âœ“ æ•°æ®åº“è¡¨æ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """æ’å…¥å¸–å­æ•°æ®åˆ°æ•°æ®åº“"""
    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        logger.info("â³ å¼€å§‹æ’å…¥æ•°æ®åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []))
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                detailed_analysis = analysis.get('detailed_analysis')

                # æ–°å­—æ®µï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼šreplies_count / participants_count
                replies_count = int(post.get('replies_count') or 0)
                participants_count = int(post.get('participants_count') or 0)

                # ä¼˜å…ˆå°è¯•æ’å…¥åŒ…å«æ–°åˆ—çš„è¯­å¥ï¼›å¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ—§è¯­å¥ï¼ˆå…¼å®¹æœªè¿ç§»çš„è¡¨ç»“æ„ï¼‰
                try:
                    await conn.execute("""
                        INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            url = EXCLUDED.url,
                            core_issue = EXCLUDED.core_issue,
                            key_info = EXCLUDED.key_info,
                            post_type = EXCLUDED.post_type,
                            value_assessment = EXCLUDED.value_assessment,
                            detailed_analysis = EXCLUDED.detailed_analysis,
                            replies_count = EXCLUDED.replies_count,
                            participants_count = EXCLUDED.participants_count,
                            timestamp = CURRENT_TIMESTAMP;
                    """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                except Exception as insert_err:
                    logger.warning(f"posts è¡¨ç¼ºå°‘æ–°åˆ—ï¼Œå›é€€æ—§æ’å…¥è¯­å¥: {insert_err}")
                    await conn.execute("""
                        INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            url = EXCLUDED.url,
                            core_issue = EXCLUDED.core_issue,
                            key_info = EXCLUDED.key_info,
                            post_type = EXCLUDED.post_type,
                            value_assessment = EXCLUDED.value_assessment,
                            detailed_analysis = EXCLUDED.detailed_analysis,
                            timestamp = CURRENT_TIMESTAMP;
                    """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"âŒ æ’å…¥å¸–å­å¤±è´¥ [{post.get('id', 'N/A')}]: {e}")
                continue
        
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {success_count}/{len(posts_data)} æ¡æ•°æ®")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# =============================================================================
# AIæŠ¥å‘Šç”Ÿæˆ
# =============================================================================

def generate_ai_analysis(posts_data):
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
    if not posts_data:
        logger.warning("âš ï¸ æ²¡æœ‰å¸–å­æ•°æ®")
        return {"summary_analysis": {"error": "æ²¡æœ‰å¸–å­æ•°æ®"}, "processed_posts": []}

    logger.info("â³ å¼€å§‹AIåˆ†æ...")
    processed_posts = []
    
    for i, post in enumerate(posts_data):
        logger.info(f"  [{i+1}/{len(posts_data)}] åˆ†æ: {post['title'][:40]}...")
        
        try:
            post['analysis'] = analyze_single_post_with_deepseek(post)
            processed_posts.append(post)
            
            if i < len(posts_data) - 1:
                time.sleep(AI_REQUEST_DELAY)
                
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            post['analysis'] = {
                "error": f"åˆ†æå¤±è´¥: {e}",
                "core_issue": "åˆ†æå¤±è´¥",
                "key_info": [],
                "post_type": "é”™è¯¯",
                "value_assessment": "ä½",
                "detailed_analysis": ""
            }
            processed_posts.append(post)
            
    logger.info("âœ“ AIåˆ†æå®Œæˆ")

    return {
        "summary_analysis": {"status": "success"},
        "processed_posts": processed_posts
    }

# =============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# =============================================================================

def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Š"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"../data/linux.do_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})",
                "source": "Linux.do",
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("")
    
    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        return False
    
    try:
        # 1. åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            return False
        
        # 2. çˆ¬å–å¸–å­
        posts_data = await fetch_linuxdo_posts()
        
        if not posts_data:
            logger.error("âŒ æœªèƒ½è·å–å¸–å­æ•°æ®")
            return False
        
        logger.info(f"âœ“ è·å–åˆ° {len(posts_data)} ç¯‡å¸–å­")
        
        # 3. AIåˆ†æ
        report_data = generate_ai_analysis(posts_data)
        
        # 4. æ’å…¥æ•°æ®åº“
        if report_data.get('processed_posts'):
            await insert_posts_into_db(report_data['processed_posts'])
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        json_file = generate_json_report(report_data, len(posts_data))
        
        # å®Œæˆ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info(f"âœ“ ä»»åŠ¡å®Œæˆï¼")
        logger.info(f"  å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        logger.info(f"  å¤„ç†å¸–å­: {len(posts_data)} ç¯‡")
        logger.info(f"  ç”Ÿæˆæ–‡ä»¶: {json_file}")
        logger.info("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)




