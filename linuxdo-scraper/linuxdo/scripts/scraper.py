# scraper_improved.py - æ”¹è¿›ç‰ˆçˆ¬è™«ï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
import asyncio
import os
import re
import json
import logging
from datetime import datetime
from playwright.async_api import async_playwright
from playwright_stealth import Stealth
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- ä»£ç†é…ç½® ---
proxy_for_all = "http://127.0.0.1:10809"
os.environ['HTTP_PROXY'] = proxy_for_all
os.environ['HTTPS_PROXY'] = proxy_for_all
logger.info(f"--- è„šæœ¬å·²é…ç½®ä½¿ç”¨ HTTP ä»£ç†: {proxy_for_all} ---")

# --- é…ç½® ---
# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
import pathlib
env_path = pathlib.Path(__file__).parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)
WARM_UP_URL = "https://linux.do/" 
RSS_URL = "https://linux.do/latest.rss" 
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
NEON_DB_URL = os.getenv("DATABASE_URL")
POST_COUNT_LIMIT = 30
MAX_RETRIES = 3
RETRY_DELAY = 5

# --- é‡è¯•è£…é¥°å™¨ ---
def retry_on_failure(max_retries=3, delay=5):
    def decorator(func):
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥")
                        raise e
                    logger.info(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
            return None
        return wrapper
    return decorator

# --- æ”¹è¿›çš„AIåˆ†æå‡½æ•° ---
def analyze_single_post_with_deepseek(post, retry_count=0):
    """ä½¿ç”¨DeepSeekå¯¹å•ä¸ªå¸–å­è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œå¹¶è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ã€‚"""
    try:
        clean_content = re.sub(r'<.*?>', ' ', post.get('content', ''))
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()
        excerpt = (clean_content[:800] + '...') if len(clean_content) > 800 else clean_content

        if not excerpt or len(excerpt.strip()) < 10:
            logger.warning(f"å¸–å­ '{post.get('title', 'N/A')}' å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡AIåˆ†æ")
            return {
                "error": "å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒAIæ‘˜è¦ã€‚",
                "core_issue": "N/A", "key_info": [], "post_type": "æœªçŸ¥", "value_assessment": "ä½"
            }

        # ä½¿ç”¨DeepSeek APIè¿›è¡Œåˆ†æ
        prompt = f"""
        ä½ æ˜¯ä¸€åä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹è®ºå›å¸–å­å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœã€‚
        ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

        **å¸–å­æ ‡é¢˜**: {post['title']}
        **å†…å®¹èŠ‚é€‰**: {excerpt}

        **è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
        {{
          "core_issue": "è¿™é‡Œç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­çš„æ ¸å¿ƒè®®é¢˜",
          "key_info": [
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹1",
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹2",
            "å…³é”®ä¿¡æ¯æˆ–è§£å†³æ–¹æ¡ˆç‚¹3"
          ],
          "post_type": "ä»[æŠ€æœ¯é—®ç­”, èµ„æºåˆ†äº«, æ–°é—»èµ„è®¯, ä¼˜æƒ æ´»åŠ¨, æ—¥å¸¸é—²èŠ, æ±‚åŠ©, è®¨è®º, äº§å“è¯„æµ‹]ä¸­é€‰æ‹©ä¸€ä¸ª",
          "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª",
          "detailed_analysis": "ç”Ÿæˆ200-400å­—çš„é€‚åº¦è¯¦ç»†åˆ†æï¼ˆç”¨markdownæ ¼å¼ï¼‰ï¼š\\n\\n## ğŸ“‹ å†…å®¹æ¦‚è¿°\\nç®€è¦è¯´æ˜è¿™ä¸ªè¯é¢˜çš„èƒŒæ™¯å’Œè¦ç‚¹ï¼ˆ2-3å¥è¯ï¼‰\\n\\n## ğŸ’¡ ä¸»è¦å†…å®¹\\nå±•å¼€å¸–å­çš„æ ¸å¿ƒå†…å®¹ã€å…³é”®è§‚ç‚¹æˆ–è§£å†³æ–¹æ¡ˆï¼ˆ100-150å­—ï¼‰\\n\\n## ğŸ’¬ è®¨è®ºè¦ç‚¹\\nè¯´æ˜å¸–å­çš„å®ç”¨æ€§ã€é€‚ç”¨åœºæ™¯æˆ–å¯èƒ½å¼•å‘çš„è®¨è®ºæ–¹å‘ï¼ˆ50-100å­—ï¼‰\\n\\n## ğŸ”§ å®ç”¨ä»·å€¼\\nè¿™ä¸ªä¿¡æ¯å¯¹è¯»è€…æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Œå¦‚ä½•åº”ç”¨æˆ–å‚è€ƒï¼ˆ50-100å­—ï¼‰"
        }}
        """
        
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 400,
            "temperature": 0.3
        }
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies={"http": proxy_for_all, "https": proxy_for_all},
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # å°è¯•è§£æAIè¿”å›çš„æ–‡æœ¬ä¸ºJSON
            cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
            analysis_data = json.loads(cleaned_text)
            logger.info(f"å¸–å­ '{post['title'][:30]}...' AIåˆ†ææˆåŠŸ")
            return analysis_data
        else:
            logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {
                "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                "core_issue": "APIè°ƒç”¨å¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
            }
        
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥! AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹: '{ai_response[:100] if 'ai_response' in locals() else 'N/A'}...'")
        return {
            "error": "AIè¿”å›äº†éJSONæ ¼å¼çš„å†…å®¹", 
            "raw_response": ai_response if 'ai_response' in locals() else "N/A",
            "core_issue": "AIåˆ†æå¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
        }
    except requests.exceptions.Timeout:
        logger.error(f"DeepSeek APIè¯·æ±‚è¶…æ—¶")
        return {
            "error": "APIè¯·æ±‚è¶…æ—¶",
            "core_issue": "APIè¶…æ—¶", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
        }
    except Exception as e:
        logger.error(f"å¯¹å¸–å­ '{post.get('title', 'N/A')[:20]}...' çš„æ‘˜è¦å¤±è´¥: {e}")
        return {
            "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}",
            "core_issue": "AIåˆ†æå¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
        }

# --- æ”¹è¿›çš„çˆ¬è™«å‡½æ•° ---
@retry_on_failure(max_retries=MAX_RETRIES, delay=RETRY_DELAY)
async def fetch_linuxdo_posts_improved():
    """æ”¹è¿›ç‰ˆçˆ¬è™«å‡½æ•°ï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    logger.info("å¼€å§‹æ‰§è¡Œæ”¹è¿›ç‰ˆçˆ¬å–ä»»åŠ¡...")
    
    async with async_playwright() as p:
        browser = None
        try:
            browser = await p.chromium.launch(
                headless=True,
                proxy={"server": proxy_for_all},
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                extra_http_headers={
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "Cache-Control": "max-age=0"
                }
            )
            
            stealth = Stealth()
            await stealth.apply_stealth_async(context)
            page = await context.new_page()

            # é¢„çƒ­é˜¶æ®µ
            logger.info(f"æ­£åœ¨è®¿é—®é¦–é¡µè¿›è¡Œé¢„çƒ­: {WARM_UP_URL}")
            await page.goto(WARM_UP_URL, wait_until="domcontentloaded", timeout=60000)
            logger.info("é¢„çƒ­å®Œæˆï¼Œä¼šè¯å·²å»ºç«‹ã€‚")
            await asyncio.sleep(3)

            # è®¿é—®RSSæº
            logger.info(f"æ­£åœ¨é€šè¿‡æµè§ˆå™¨ç›´æ¥è®¿é—® RSS æº: {RSS_URL}")
            await page.goto(RSS_URL, wait_until="networkidle", timeout=120000)
            await asyncio.sleep(2)

            rss_text = await page.evaluate("document.documentElement.outerHTML")
            
            # ä¿å­˜è°ƒè¯•å†…å®¹
            debug_filename = f"debug_rss_content_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write(rss_text)
            logger.info(f"å·²ä¿å­˜è°ƒè¯•å†…å®¹åˆ° {debug_filename} (é•¿åº¦: {len(rss_text)} å­—ç¬¦)")

            all_posts = []
            logger.info("å¼€å§‹è§£æRSSå†…å®¹...")

            # æ–¹å¼1ï¼šç›´æ¥è§£æXML
            try:
                root = ET.fromstring(rss_text)
                items = root.findall('.//channel/item')
                logger.info(f"æ‰¾åˆ° {len(items)} ä¸ªitemæ ‡ç­¾")
                
                for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                    title = item.find('title')
                    link = item.find('link')
                    description = item.find('description')
                    
                    if title is not None and link is not None:
                        title_text = title.text
                        link_text = link.text
                        description_text = description.text if description is not None else ""
                        
                        logger.info(f"  è§£æå¸–å­ {i+1}: {title_text[:50]}... -> {link_text}")
                        
                        match = re.search(r'/t/[^/]+/(\d+)', link_text)
                        if match:
                            topic_id = match.group(1)
                            all_posts.append({
                                "title": title_text,
                                "link": link_text,
                                "id": topic_id,
                                "description": description_text
                            })
                            logger.info(f"    æˆåŠŸè§£æï¼ŒID: {topic_id}, æè¿°é•¿åº¦: {len(description_text)}")
                        else:
                            logger.warning(f"    æ— æ³•åŒ¹é…ID: {link_text}")
                    else:
                        logger.warning(f"  å¸–å­ {i+1} ç¼ºå°‘titleæˆ–linkæ ‡ç­¾")
                        
            except ET.ParseError as e:
                logger.error(f"XMLè§£æå¤±è´¥: {e}")
                all_posts = []

            # æ–¹å¼2ï¼šä»HTMLä¸­æå–XMLå†…å®¹
            if not all_posts:
                logger.info("å°è¯•æ–¹å¼2ï¼šä»HTMLä¸­æå–XMLå†…å®¹")
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(rss_text, 'html.parser')
                    pre_tag = soup.find('pre')
                    
                    if pre_tag:
                        xml_content = pre_tag.get_text()
                        logger.info(f"ä»preæ ‡ç­¾æå–åˆ° {len(xml_content)} å­—ç¬¦çš„XMLå†…å®¹")
                        
                        import html
                        xml_content = html.unescape(xml_content)
                        root = ET.fromstring(xml_content)
                        items = root.findall('.//channel/item')
                        logger.info(f"æ‰¾åˆ° {len(items)} ä¸ªitemæ ‡ç­¾")
                        
                        for i, item in enumerate(items[:POST_COUNT_LIMIT]):
                            title = item.find('title')
                            link = item.find('link')
                            description = item.find('description')
                            
                            if title is not None and link is not None:
                                title_text = title.text
                                link_text = link.text
                                description_text = description.text if description is not None else ""
                                
                                logger.info(f"  è§£æå¸–å­ {i+1}: {title_text[:50]}... -> {link_text}")
                                
                                match = re.search(r'/t/[^/]+/(\d+)', link_text)
                                if match:
                                    topic_id = match.group(1)
                                    all_posts.append({
                                        "title": title_text, 
                                        "link": link_text, 
                                        "id": topic_id, 
                                        "description": description_text
                                    })
                                    logger.info(f"    æˆåŠŸè§£æï¼ŒID: {topic_id}")
                                else:
                                    logger.warning(f"    æ— æ³•åŒ¹é…ID: {link_text}")
                            else:
                                logger.warning(f"  å¸–å­ {i+1} ç¼ºå°‘titleæˆ–linkæ ‡ç­¾")
                    else:
                        logger.warning("æœªæ‰¾åˆ°preæ ‡ç­¾")
                        
                except Exception as e2:
                    logger.error(f"HTMLæå–è§£æä¹Ÿå¤±è´¥: {e2}")

            # æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–åŸºæœ¬ä¿¡æ¯ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
            if not all_posts:
                logger.info("å°è¯•æ–¹å¼3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–åŸºæœ¬ä¿¡æ¯ (å…œåº•æ–¹æ¡ˆ)")
                title_pattern = r'<title>([^<]+)</title>'
                link_pattern = r'<link>([^<]+)</link>'
                titles = re.findall(title_pattern, rss_text)
                links = re.findall(link_pattern, rss_text)

                for i, (title, link) in enumerate(zip(titles, links)):
                    if i >= POST_COUNT_LIMIT:
                        break
                    match = re.search(r'/t/[^/]+/(\d+)', link)
                    if match:
                        all_posts.append({
                            "title": title, 
                            "link": link, 
                            "id": match.group(1), 
                            "description": ""
                        })
            
            logger.info(f"ä» RSS æºæˆåŠŸè§£æåˆ° {len(all_posts)} ä¸ªå¸–å­ã€‚")
            
            if not all_posts:
                logger.error("æœªèƒ½è§£æåˆ°ä»»ä½•å¸–å­ï¼Œå¯èƒ½æ˜¯RSSæºé—®é¢˜æˆ–ç½‘ç»œé—®é¢˜")
                return []
            
            # å¤„ç†å¸–å­å†…å®¹
            posts_with_content = []
            for i, post in enumerate(all_posts):
                logger.info(f"  > æ­£åœ¨å¤„ç†å¸–å­: {post['title']}")

                # ä¼˜å…ˆä½¿ç”¨RSSæè¿°ä¸­çš„å†…å®¹ä½œä¸ºå¸–å­çš„'content'
                rss_content = post.get('description', '')

                # æ¸…ç†RSSæè¿°å†…å®¹ï¼ˆç§»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½ï¼‰
                if rss_content:
                    # æå–äº’åŠ¨æ•°æ®ï¼š"X ä¸ªå¸–å­ - Y ä½å‚ä¸è€…"
                    replies_count = 0
                    participants_count = 0
                    try:
                        match = re.search(r'(\d+)\s*ä¸ªå¸–å­\s*-\s*(\d+)\s*ä½å‚ä¸è€…', rss_content)
                        if match:
                            replies_count = int(match.group(1))
                            participants_count = int(match.group(2))
                    except Exception:
                        pass

                    clean_content = re.sub(r'<[^>]+>', ' ', rss_content)
                    clean_content = re.sub(r'\d+\s*ä¸ªå¸–å­\s*-\s*\d+\s*ä½å‚ä¸è€…', '', clean_content)
                    clean_content = ' '.join(clean_content.split())
                    clean_content = clean_content.replace('\n', ' ').replace('\r', ' ')
                    clean_content = ' '.join(clean_content.split())

                    if len(clean_content.strip()) > 10:
                        post['content'] = clean_content
                        post['replies_count'] = replies_count
                        post['participants_count'] = participants_count
                        posts_with_content.append(post)
                        logger.info(f"    + ä½¿ç”¨RSSæè¿°å†…å®¹ (é•¿åº¦: {len(clean_content)} å­—ç¬¦)")
                    else:
                        post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                        posts_with_content.append(post)
                        logger.info(f"    + RSSå†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹")
                else:
                    post['content'] = f"å¸–å­æ ‡é¢˜ï¼š{post['title']}"
                    posts_with_content.append(post)
                    logger.info(f"    + ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹")

                if i < len(all_posts) - 1:
                    await asyncio.sleep(1)

            logger.info(f"æˆåŠŸè·å–äº† {len(posts_with_content)} ä¸ªå¸–å­çš„è¯¦ç»†å†…å®¹ã€‚")
            return posts_with_content

        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            try:
                if browser:
                    page = await browser.new_page()
                    await page.screenshot(path=f"error_screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            except Exception as screenshot_error:
                logger.error(f"æ— æ³•ä¿å­˜é”™è¯¯æˆªå›¾: {screenshot_error}")
            raise e
        finally:
            if browser:
                await browser.close()
                logger.info("æµè§ˆå™¨å·²å…³é—­ã€‚")

# --- æ”¹è¿›çš„æ•°æ®åº“æ“ä½œ ---
async def create_posts_table():
    """è¿æ¥åˆ°Neonæ•°æ®åº“å¹¶åˆ›å»ºpostsè¡¨ï¼Œå¦‚æœå®ƒä¸å­˜åœ¨çš„è¯ã€‚"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("æ•°æ®åº“è¡¨ 'posts' æ£€æŸ¥æˆ–åˆ›å»ºæˆåŠŸã€‚")
        return True
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¤„ç†åçš„å¸–å­æ•°æ®æ’å…¥åˆ°Neonæ•°æ®åº“ä¸­ã€‚"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        logger.info("å¼€å§‹å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []))
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                detailed_analysis = analysis.get('detailed_analysis')

                replies_count = int(post.get('replies_count') or 0)
                participants_count = int(post.get('participants_count') or 0)

                try:
                    await conn.execute("""
                        INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            url = EXCLUDED.url,
                            core_issue = EXCLUDED.core_issue,
                            key_info = EXCLUDED.key_info,
                            post_type = EXCLUDED.post_type,
                            value_assessment = EXCLUDED.value_assessment,
                            detailed_analysis = EXCLUDED.detailed_analysis,
                            replies_count = EXCLUDED.replies_count,
                            participants_count = EXCLUDED.participants_count,
                            timestamp = CURRENT_TIMESTAMP;
                    """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, replies_count, participants_count)
                except Exception as insert_err:
                    logger.warning(f"posts è¡¨ç¼ºå°‘æ–°åˆ—ï¼Œå›é€€æ—§æ’å…¥è¯­å¥: {insert_err}")
                    await conn.execute("""
                        INSERT INTO posts (id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            url = EXCLUDED.url,
                            core_issue = EXCLUDED.core_issue,
                            key_info = EXCLUDED.key_info,
                            post_type = EXCLUDED.post_type,
                            value_assessment = EXCLUDED.value_assessment,
                            detailed_analysis = EXCLUDED.detailed_analysis,
                            timestamp = CURRENT_TIMESTAMP;
                    """, post_id, title, url, core_issue, key_info, post_type, value_assessment, detailed_analysis)
                
                logger.info(f"  - å¸–å­ '{title[:30]}...' (ID: {post_id}) å·²æ’å…¥/æ›´æ–°ã€‚")
                success_count += 1
                
            except Exception as e:
                logger.error(f"æ’å…¥å¸–å­ {post.get('id', 'N/A')} å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸæ’å…¥/æ›´æ–°äº† {success_count}/{len(posts_data)} æ¡å¸–å­æ•°æ®åˆ°æ•°æ®åº“ã€‚")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"æ’å…¥å¸–å­æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# --- æ”¹è¿›çš„AIæŠ¥å‘Šç”Ÿæˆ ---
def generate_ai_summary_report(posts_data):
    """ç”ŸæˆAIæ‘˜è¦æŠ¥å‘Šï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
    if not posts_data:
        logger.warning("æ²¡æœ‰å¸–å­æ•°æ®ï¼Œç”Ÿæˆç©ºæŠ¥å‘Š")
        return {"summary_analysis": {"error": "æ²¡æœ‰å¸–å­æ•°æ®"}, "processed_posts": []}

    logger.info("\n--- å¼€å§‹ç”Ÿæˆé€å¸–ç»“æ„åŒ–åˆ†æ ---")
    processed_posts = [] 
    
    for i, post in enumerate(posts_data):
        logger.info(f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(posts_data)} ç¯‡: {post['title']}")
        
        try:
            # å°†åˆ†æç»“æœï¼ˆä¸€ä¸ªå­—å…¸ï¼‰å­˜å…¥ 'analysis' é”®
            post['analysis'] = analyze_single_post_with_deepseek(post)
            processed_posts.append(post)
            
            if i < len(posts_data) - 1:
                logger.info("    ...ç­‰å¾…3ç§’ä»¥éµå®ˆAPIé€Ÿç‡é™åˆ¶...")
                time.sleep(3)
                
        except Exception as e:
            logger.error(f"åˆ†æå¸–å­ {post.get('title', 'N/A')} å¤±è´¥: {e}")
            post['analysis'] = {
                "error": f"åˆ†æå¤±è´¥: {e}",
                "core_issue": "åˆ†æå¤±è´¥", "key_info": [], "post_type": "é”™è¯¯", "value_assessment": "ä½"
            }
            processed_posts.append(post)
            
    logger.info("--- é€å¸–åˆ†æå…¨éƒ¨å®Œæˆ ---\n")

    # ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Š
    logger.info("--- å¼€å§‹ç”Ÿæˆä»Šæ—¥æ•´ä½“æ´å¯ŸæŠ¥å‘Š (JSONæ ¼å¼) ---")
    summaries_for_prompt = []
    for post in processed_posts:
        if 'error' not in post['analysis']:
            summaries_for_prompt.append({
                "title": post['title'],
                "analysis": post['analysis']
            })

    if not summaries_for_prompt:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å¸–å­åˆ†æç»“æœï¼Œç”Ÿæˆé»˜è®¤æŠ¥å‘Š")
        summary_analysis = {
            "overview": "ä»Šæ—¥æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„å¸–å­åˆ†æç»“æœã€‚",
            "highlights": {"tech_savvy": [], "resources_deals": [], "hot_topics": []},
            "conclusion": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚"
        }
    else:
        # ä½¿ç”¨json.dumpsæ¥åˆ›å»ºä¸€ä¸ªç´§å‡‘çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
        all_summaries_text = json.dumps(summaries_for_prompt, ensure_ascii=False, indent=2)

        overall_prompt = f"""
        ä½ æ˜¯ä¸€åèµ„æ·±çš„è®ºå›å†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©è®ºå›çƒ­é—¨å¸–å­çš„JSONæ ¼å¼æ‘˜è¦åˆ—è¡¨ã€‚
        è¯·æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½é«˜åº¦æµ“ç¼©çš„ä¸­æ–‡"ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ"æŠ¥å‘Šï¼Œå¹¶ä¸¥æ ¼ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ã€‚
        ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

        **ä»Šæ—¥å¸–å­æ‘˜è¦åˆé›† (JSONæ ¼å¼):**
        {all_summaries_text}
        ---
        **è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
        {{
          "overview": "ç”¨ä¸€ä¸¤å¥è¯æ€»ç»“ä»Šå¤©ç¤¾åŒºçš„æ•´ä½“æ°›å›´å’Œè®¨è®ºç„¦ç‚¹ã€‚",
          "highlights": {{
            "tech_savvy": ["æç‚¼1-3æ¡æœ€ç¡¬æ ¸çš„æŠ€æœ¯å¹²è´§"],
            "resources_deals": ["æç‚¼1-3æ¡æœ€å€¼å¾—å…³æ³¨çš„ä¼˜æƒ æˆ–èµ„æºåˆ†äº«"],
            "hot_topics": ["æç‚¼1-3ä¸ªå¼•å‘æœ€å¹¿æ³›è®¨è®ºçš„è¯é¢˜"]
          }},
          "conclusion": "ç”¨ä¸€å¥è¯å¯¹ä»Šå¤©çš„å†…å®¹åšä¸ªé£è¶£æˆ–æ·±åˆ»çš„æ€»ç»“ã€‚"
        }}
        """
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": overall_prompt}
                ],
                "max_tokens": 800,
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=data,
                proxies={"http": proxy_for_all, "https": proxy_for_all},
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result['choices'][0]['message']['content']
                cleaned_text = ai_response.strip().replace("```json", "").replace("```", "").strip()
                summary_analysis = json.loads(cleaned_text)
                logger.info("--- æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæ¯• ---")
            else:
                logger.error(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šå¤±è´¥: {response.status_code} - {response.text}")
                summary_analysis = {
                    "error": f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}",
                    "overview": "ä»Šæ—¥AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚",
                    "highlights": {},
                    "conclusion": ""
                }
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šå¤±è´¥: {e}")
            summary_analysis = {
                "error": f"AIç”Ÿæˆæ•´ä½“æŠ¥å‘Šå¤±è´¥: {e}",
                "overview": "ä»Šæ—¥AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚",
                "highlights": {},
                "conclusion": ""
            }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- æ”¹è¿›çš„JSONæŠ¥å‘Šç”Ÿæˆ ---
def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Šæ–‡ä»¶ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"../data/linux.do_report_{today_str}.json"

        # å‡†å¤‡æœ€ç»ˆçš„JSONç»“æ„
        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})",
                "source": "Linux.do",
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- æ”¹è¿›çš„MarkdownæŠ¥å‘Šç”Ÿæˆ ---
def generate_markdown_report(report_data, posts_count):
    """ç”ŸæˆMarkdownæŠ¥å‘Šï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"../reports/Linux.do_Daily_Report_{today_str}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# Linux.do æ¯æ—¥çƒ­å¸–æŠ¥å‘Š ({today_str})\n\n")
            
            # æ·»åŠ çŠ¶æ€ä¿¡æ¯
            if posts_count == 0:
                f.write("âš ï¸ **æ³¨æ„**: ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–RSSæºå¼‚å¸¸ã€‚\n\n")
            
            # æ¸²æŸ“ç²¾åæç‚¼éƒ¨åˆ†
            f.write("## ğŸš€ ä»Šæ—¥ç²¾åæç‚¼\n\n")
            summary = report_data.get('summary_analysis', {})
            
            if 'error' in summary:
                f.write(f"âŒ **é”™è¯¯**: {summary.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n")
            else:
                f.write(f"**ä»Šæ—¥æ¦‚è§ˆ:** {summary.get('overview', 'N/A')}\n\n")
                f.write(f"**é«˜ä»·å€¼ä¿¡æ¯é€Ÿé€’:**\n")
                highlights = summary.get('highlights', {})
                if highlights.get('tech_savvy'):
                    f.write(f"*   **æŠ€æœ¯å¹²è´§:**\n")
                    for item in highlights['tech_savvy']:
                        f.write(f"    *   {item}\n")
                if highlights.get('resources_deals'):
                    f.write(f"*   **ä¼˜æƒ /èµ„æº**:\n")
                    for item in highlights['resources_deals']:
                        f.write(f"    *   {item}\n")
                if highlights.get('hot_topics'):
                    f.write(f"*   **çƒ­è®®è¯é¢˜**:\n")
                    for item in highlights['hot_topics']:
                        f.write(f"    *   {item}\n")
                f.write(f"\n**ä»Šæ—¥ç»“è¯­:** {summary.get('conclusion', 'N/A')}\n\n")
            
            f.write("---\n\n")

            # æ¸²æŸ“é€å¸–æ‘˜è¦éƒ¨åˆ†
            f.write("## ğŸ“° é€å¸–æ‘˜è¦ä¸åˆ†æ\n\n")
            posts = report_data.get('processed_posts', [])
            if posts:
                for post in posts:
                    title = post.get('title', 'æ— æ ‡é¢˜')
                    link = post.get('link', '#')
                    analysis = post.get('analysis', {})
                    
                    f.write(f"### [{title}]({link})\n\n")
                    # ä½¿ç”¨Markdownå¼•ç”¨æ ¼å¼å±•ç¤ºç»“æ„åŒ–åˆ†æç»“æœ
                    f.write(f"> 1.  **æ ¸å¿ƒè®®é¢˜**: {analysis.get('core_issue', 'N/A')}\n")
                    f.write(f"> 2.  **å…³é”®ä¿¡æ¯/è§£å†³æ–¹æ¡ˆ**:\n")
                    for info in analysis.get('key_info', []):
                        f.write(f">     *   {info}\n")
                    if not analysis.get('key_info'):
                         f.write(f">     *   æ— \n")
                    f.write(f"> 3.  **å¸–å­ç±»å‹**: {analysis.get('post_type', 'N/A')}\n")
                    f.write(f"> 4.  **ä»·å€¼è¯„ä¼°**: {analysis.get('value_assessment', 'N/A')}\n\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°æ–°å¸–å­ã€‚\n")
                
            # æ¸²æŸ“åŸå§‹å¸–å­åˆ—è¡¨
            f.write("---\n\n")
            f.write(f"## ğŸ“‹ åŸå§‹å¸–å­åˆ—è¡¨ (å…± {posts_count} ç¯‡)\n\n")
            if posts:
                for post in posts:
                    f.write(f"- [{post.get('title', 'æ— æ ‡é¢˜')}]({post.get('link', '#')})\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•åŸå§‹å¸–å­ã€‚\n")

        logger.info(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- æ”¹è¿›çš„ä¸»å‡½æ•° ---
async def main():
    """æ”¹è¿›çš„ä¸»å‡½æ•°ï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œç›‘æ§"""
    start_time = datetime.now()
    logger.info("=== å¼€å§‹æ‰§è¡Œæ”¹è¿›ç‰ˆçˆ¬è™«ä»»åŠ¡ ===")
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not DEEPSEEK_API_KEY:
            logger.error("æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return False
            
        if not NEON_DB_URL:
            logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
            return False
        
        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            logger.error("æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        # çˆ¬å–å¸–å­æ•°æ®
        posts_data = await fetch_linuxdo_posts_improved()
        
        if not posts_data:
            logger.error("æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­æ•°æ®")
            # å³ä½¿å¤±è´¥ï¼Œä¹Ÿç”Ÿæˆä¸€ä¸ªç©ºçš„æŠ¥å‘Šæ–‡ä»¶
            empty_data = {
                "summary_analysis": {
                    "error": "æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–RSSæºå¼‚å¸¸",
                    "overview": "ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ã€‚"
                }, 
                "processed_posts": []
            }
            generate_json_report(empty_data, 0)
            generate_markdown_report(empty_data, 0)
            return False
        
        logger.info(f"æˆåŠŸè·å–åˆ° {len(posts_data)} æ¡å¸–å­æ•°æ®")
        
        # ç”ŸæˆAIæŠ¥å‘Š
        report_data = generate_ai_summary_report(posts_data)
        
        # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“
        if report_data.get('processed_posts'):
            db_success = await insert_posts_into_db(report_data['processed_posts'])
            if not db_success:
                logger.error("æ•°æ®åº“æ’å…¥å¤±è´¥")
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        json_file = generate_json_report(report_data, len(posts_data))
        md_file = generate_markdown_report(report_data, len(posts_data))
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info(f"=== ä»»åŠ¡å®Œæˆ ===")
        logger.info(f"å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        logger.info(f"å¤„ç†å¸–å­: {len(posts_data)} æ¡")
        logger.info(f"ç”Ÿæˆæ–‡ä»¶: {json_file}, {md_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    if not success:
        logger.error("çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        exit(1)
    else:
        logger.info("çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
