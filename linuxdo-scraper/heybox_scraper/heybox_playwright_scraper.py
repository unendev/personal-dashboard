#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°é»‘ç›’Playwrightçˆ¬è™« - åŸºäºMCPæµ‹è¯•éªŒè¯çš„æ–¹æ¡ˆ
ä½¿ç”¨ Playwright æ— å¤´æµè§ˆå™¨ + x_xhh_tokenid è®¤è¯

æµ‹è¯•éªŒè¯ï¼š2025-10-25 âœ…
- Tokenè®¤è¯æˆåŠŸ
- å®‰å…¨éªŒè¯å·²ç»•è¿‡
- é¡µé¢æ­£å¸¸åŠ è½½å¸–å­å†…å®¹

ä½¿ç”¨æ–¹æ³•ï¼š
  1. é…ç½® .env æ–‡ä»¶ä¸­çš„ HEYBOX_TOKEN_ID
  2. å®‰è£…Playwright: pip install playwright playwright-stealth
  3. å®‰è£…æµè§ˆå™¨: python -m playwright install chromium
  4. è¿è¡Œ: python heybox_playwright_scraper.py
"""

import asyncio
import os
import json
import logging
import time
from datetime import datetime
from typing import List, Dict, Any
from playwright.async_api import async_playwright, Page
from playwright_stealth import Stealth
import asyncpg
import re

# å¯¼å…¥é…ç½®
from config import (
    HEYBOX_TOKEN_ID, HEYBOX_HOME_URL,
    POST_LIMIT, COMMENT_LIMIT, REQUEST_INTERVAL,
    MAX_RETRIES, RETRY_DELAY, AI_REQUEST_DELAY,
    DEEPSEEK_API_KEY, DEEPSEEK_API_URL,
    DATABASE_URL, USE_PROXY, get_proxies, check_config
)

# ========== æ—¥å¿—é…ç½® ==========
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('logs/heybox_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========== æµè§ˆå™¨åˆå§‹åŒ– ==========

async def init_browser_with_token(page: Page, token: str):
    """
    åˆå§‹åŒ–æµè§ˆå™¨å¹¶æ³¨å…¥Token
    
    åŸºäºMCPæµ‹è¯•éªŒè¯çš„æ–¹æ³•ï¼š
    1. è®¿é—®é¦–é¡µ
    2. æ³¨å…¥tokenåˆ°localStorageã€sessionStorageå’Œcookie
    3. è‡ªåŠ¨ç»•è¿‡å®‰å…¨éªŒè¯
    """
    logger.info(f"ğŸŒ è®¿é—®é¦–é¡µ: {HEYBOX_HOME_URL}")
    
    try:
        # è®¿é—®é¦–é¡µ
        await page.goto(HEYBOX_HOME_URL, wait_until='domcontentloaded', timeout=60000)
        logger.info("  âœ“ é¡µé¢åŠ è½½å®Œæˆ")
        
        # æ³¨å…¥tokenï¼ˆMCPæµ‹è¯•éªŒè¯çš„æ–¹æ³•ï¼‰
        await page.evaluate(f"""
            () => {{
                const token = "{token}";
                localStorage.setItem('x_xhh_tokenid', token);
                sessionStorage.setItem('x_xhh_tokenid', token);
                document.cookie = `x_xhh_tokenid=${{token}}; path=/; domain=.xiaoheihe.cn`;
            }}
        """)
        logger.info("  âœ“ Tokenæ³¨å…¥æˆåŠŸ")
        
        # ç­‰å¾…ä¸€ä¸‹è®©é¡µé¢ååº”
        await asyncio.sleep(2)
        
        # åˆ·æ–°é¡µé¢ä½¿tokenç”Ÿæ•ˆ
        await page.reload(wait_until='domcontentloaded')
        logger.info("  âœ“ é¡µé¢åˆ·æ–°ï¼ŒTokenå·²æ¿€æ´»")
        
        # å†ç­‰å¾…å†…å®¹åŠ è½½
        await asyncio.sleep(3)
        
        return True
        
    except Exception as e:
        logger.error(f"  âœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

# ========== æ•°æ®æå– ==========

async def extract_posts_from_page(page: Page, limit: int = POST_LIMIT) -> List[Dict]:
    """
    ä»é¡µé¢æå–å¸–å­æ•°æ®
    
    åŸºäºé¡µé¢å®é™…ç»“æ„æå–ï¼ˆMCPæµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„ï¼‰
    """
    logger.info(f"ğŸ“ å¼€å§‹æå–å¸–å­æ•°æ®ï¼ˆç›®æ ‡{limit}æ¡ï¼‰...")
    
    try:
        # è·å–é¡µé¢HTML
        content = await page.content()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–linkæ•°æ®ï¼ˆä»hrefä¸­æå–IDï¼‰
        post_ids = re.findall(r'/app/bbs/link/(\d+)\?', content)
        logger.info(f"  æ‰¾åˆ° {len(post_ids)} ä¸ªå¸–å­ID")
        
        # è·å–é¡µé¢çš„çº¯æ–‡æœ¬å†…å®¹ç”¨äºæå–
        posts_data = await page.evaluate("""
            () => {
                const posts = [];
                const links = document.querySelectorAll('a[href*="/app/bbs/link/"]');
                
                links.forEach((link, index) => {
                    if (index >= 20) return;
                    
                    const href = link.href || link.getAttribute('href');
                    const fullText = link.textContent || '';
                    
                    if (href && fullText) {
                        posts.push({
                            href: href,
                            text: fullText.trim()
                        });
                    }
                });
                
                return posts;
            }
        """)
        
        logger.info(f"  æå–åˆ° {len(posts_data)} ä¸ªå¸–å­çš„åŸå§‹æ•°æ®")
        
        # è§£ææå–çš„æ•°æ®
        posts = []
        for item in posts_data[:limit]:
            try:
                # æå–ID
                id_match = re.search(r'/link/(\d+)', item['href'])
                if not id_match:
                    continue
                
                post_id = id_match.group(1)
                text = item['text']
                
                # åˆ†å‰²æ–‡æœ¬ï¼ˆæ ¼å¼ï¼šä½œè€… Lv.X | æ ‡é¢˜ | æ‘˜è¦ | ç‚¹èµæ•° | è¯„è®ºæ•°ï¼‰
                parts = [p.strip() for p in text.split('  ') if p.strip()]
                
                # æå–æ•°å­—ï¼ˆç‚¹èµå’Œè¯„è®ºï¼‰
                numbers = re.findall(r'\b(\d+)\b', text)
                
                # è§£æå„éƒ¨åˆ†
                author = parts[0].replace('Lv.', '').replace('  ', '').strip() if parts else ''
                title = parts[1] if len(parts) > 1 else ''
                summary = parts[2] if len(parts) > 2 else ''
                
                likes = int(numbers[-2]) if len(numbers) >= 2 else 0
                comments = int(numbers[-1]) if len(numbers) >= 1 else 0
                
                post = {
                    'id': post_id,
                    'title': title,
                    'summary': summary,
                    'author': author,
                    'url': item['href'],
                    'likes_count': likes,
                    'comments_count': comments,
                    'created_time': int(time.time())
                }
                
                posts.append(post)
                logger.debug(f"    [{len(posts)}] {title[:30]}...")
                
            except Exception as e:
                logger.warning(f"    è§£æå¸–å­å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸæå– {len(posts)} ä¸ªå¸–å­")
        return posts
        
    except Exception as e:
        logger.error(f"âŒ æå–å¤±è´¥: {e}")
        return []

async def extract_comments(page: Page, post_id: str, post_url: str) -> List[Dict]:
    """æå–å¸–å­è¯„è®º"""
    logger.info(f"  ğŸ’¬ æŠ“å–è¯„è®º: {post_id}")
    
    try:
        # è®¿é—®å¸–å­è¯¦æƒ…é¡µ
        await page.goto(post_url, wait_until='domcontentloaded', timeout=30000)
        await asyncio.sleep(2)
        
        # å°è¯•æå–è¯„è®ºï¼ˆéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´ï¼‰
        comments_data = await page.evaluate("""
            () => {
                const comments = [];
                // TODO: æ ¹æ®å®é™…è¯„è®ºç»“æ„æå–
                return comments;
            }
        """)
        
        logger.info(f"    âœ“ è·å–åˆ° {len(comments_data)} æ¡è¯„è®º")
        return comments_data
        
    except Exception as e:
        logger.warning(f"    âœ— è¯„è®ºæŠ“å–å¤±è´¥: {e}")
        return []

# ========== AIåˆ†æ ==========

def analyze_with_ai(post: Dict, comments: List[Dict]) -> Dict:
    """ä½¿ç”¨DeepSeek AIåˆ†æ"""
    logger.info(f"  ğŸ¤– AIåˆ†æ: {post['title'][:30]}...")
    
    if not DEEPSEEK_API_KEY:
        return {
            'core_issue': post.get('summary', '')[:100],
            'key_info': [post['title']],
            'post_type': 'æœªåˆ†ç±»',
            'value_assessment': 'ä¸­',
            'detailed_analysis': ''
        }
    
    import requests
    
    comments_text = "\n".join([f"- {c.get('author', '')}: {c.get('content', '')[:100]}" for c in comments[:5]])
    
    prompt = f"""ä½ æ˜¯ä¸“ä¸šæ¸¸æˆç¤¾åŒºå†…å®¹åˆ†æå¸ˆã€‚åˆ†æä»¥ä¸‹å°é»‘ç›’å¸–å­ï¼š

æ ‡é¢˜ï¼š{post['title']}
ä½œè€…ï¼š{post['author']}
å†…å®¹ï¼š{post.get('summary', '')}
äº’åŠ¨ï¼š{post['likes_count']}èµ / {post['comments_count']}è¯„è®º

è¯„è®ºåŒºï¼ˆå‰5æ¡ï¼‰ï¼š
{comments_text if comments_text else 'æš‚æ— è¯„è®º'}

è¿”å›JSONæ ¼å¼åˆ†æï¼ˆåªè¿”å›JSONï¼‰ï¼š
{{
  "core_issue": "æ ¸å¿ƒè®®é¢˜ä¸€å¥è¯",
  "key_info": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3"],
  "post_type": "æ¸¸æˆèµ„è®¯/æ¸¸æˆæ”»ç•¥/ç©å®¶è®¨è®º/ç¡¬ä»¶è¯„æµ‹/æ±‚åŠ©é—®ç­”/å…¶ä»–",
  "value_assessment": "é«˜/ä¸­/ä½",
  "detailed_analysis": "## ğŸ“‹ å†…å®¹æ¦‚è¿°\\n...\\n\\n## ğŸ’¡ å…³é”®ä¿¡æ¯\\n..."
}}"""
    
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šæ¸¸æˆç¤¾åŒºåˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.5,
                "max_tokens": 2000
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                analysis = json.loads(json_match.group())
                logger.info(f"    âœ“ AIåˆ†æå®Œæˆ")
                time.sleep(AI_REQUEST_DELAY)
                return analysis
                
    except Exception as e:
        logger.warning(f"    âœ— AIåˆ†æå¤±è´¥: {e}")
    
    return {
        'core_issue': post.get('summary', post['title'])[:100],
        'key_info': [post['title']],
        'post_type': 'æœªåˆ†ç±»',
        'value_assessment': 'ä¸­',
        'detailed_analysis': f"## å†…å®¹\n\n{post.get('summary', '')}"
    }

# ========== æ•°æ®åº“å­˜å‚¨ ==========

async def save_to_database(posts_with_analysis: List[Dict]):
    """ä¿å­˜åˆ°PostgreSQL"""
    logger.info(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    
    if not DATABASE_URL:
        logger.error("âŒ æœªé…ç½®DATABASE_URL")
        return False
    
    db_url = DATABASE_URL.split('?')[0] if '?' in DATABASE_URL else DATABASE_URL
    
    try:
        conn = await asyncpg.connect(db_url)
        logger.info("  âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        saved_posts = 0
        saved_comments = 0
        
        for post in posts_with_analysis:
            try:
                await conn.execute('''
                    INSERT INTO heybox_posts (
                        id, title, url, author, cover_image,
                        content_summary, likes_count, comments_count,
                        core_issue, key_info, post_type,
                        value_assessment, detailed_analysis, timestamp
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                    ON CONFLICT (id) DO UPDATE SET
                        likes_count = $7,
                        comments_count = $8,
                        timestamp = $14
                ''', 
                    post['id'], post['title'], post['url'],
                    post.get('author'), None,
                    post.get('summary', '')[:1000], post['likes_count'],
                    post['comments_count'],
                    post['analysis']['core_issue'],
                    json.dumps(post['analysis']['key_info']),
                    post['analysis']['post_type'],
                    post['analysis']['value_assessment'],
                    post['analysis']['detailed_analysis'],
                    datetime.fromtimestamp(post.get('created_time', time.time()))
                )
                saved_posts += 1
                
                # ä¿å­˜è¯„è®º
                for comment in post.get('comments', []):
                    try:
                        await conn.execute('''
                            INSERT INTO heybox_comments (
                                id, post_id, author, content, likes_count,
                                created_at, parent_id, depth
                            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                            ON CONFLICT (id) DO NOTHING
                        ''',
                            comment['id'], post['id'], comment.get('author', ''),
                            comment.get('content', ''), comment.get('likes_count', 0),
                            datetime.fromtimestamp(comment.get('created_time', time.time())),
                            comment.get('parent_id'), comment.get('depth', 0)
                        )
                        saved_comments += 1
                    except Exception as e:
                        logger.warning(f"    ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
                        
            except Exception as e:
                logger.warning(f"    ä¿å­˜å¸–å­å¤±è´¥ {post['id']}: {e}")
        
        await conn.close()
        logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {saved_posts}ä¸ªå¸–å­, {saved_comments}æ¡è¯„è®º")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return False

# ========== ä¸»æµç¨‹ ==========

async def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    logger.info("=" * 80)
    logger.info("ğŸ® å°é»‘ç›’Playwrightçˆ¬è™«å¯åŠ¨")
    logger.info("=" * 80)
    
    # æ£€æŸ¥é…ç½®
    issues = check_config()
    if issues:
        logger.error("\né…ç½®é—®é¢˜ï¼š")
        for issue in issues:
            logger.error(f"  {issue}")
        return
    
    logger.info(f"\né…ç½®ä¿¡æ¯ï¼š")
    logger.info(f"  - ç›®æ ‡å¸–å­æ•°: {POST_LIMIT}")
    logger.info(f"  - Tokenå·²é…ç½®: æ˜¯")
    logger.info(f"  - AIåˆ†æ: {'æ˜¯' if DEEPSEEK_API_KEY else 'å¦'}")
    logger.info(f"  - ä½¿ç”¨ä»£ç†: {'æ˜¯' if USE_PROXY else 'å¦'}")
    
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        launch_options = {
            "headless": True,
            "args": ['--no-sandbox', '--disable-dev-shm-usage']
        }
        
        if USE_PROXY:
            proxies = get_proxies()
            if proxies and proxies.get('http'):
                launch_options["proxy"] = {"server": proxies['http']}
        
        browser = await p.chromium.launch(**launch_options)
        logger.info("âœ“ æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆåçˆ¬è™«è®¾ç½®ï¼‰
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9",
            }
        )
        
        # åº”ç”¨åçˆ¬è™«
        stealth = Stealth()
        await stealth.apply_stealth_async(context)
        
        page = await context.new_page()
        logger.info("âœ“ é¡µé¢åˆ›å»ºæˆåŠŸ")
        
        # åˆå§‹åŒ–å¹¶æ³¨å…¥Token
        if not await init_browser_with_token(page, HEYBOX_TOKEN_ID):
            logger.error("âŒ åˆå§‹åŒ–å¤±è´¥")
            await browser.close()
            return
        
        # æå–å¸–å­
        posts = await extract_posts_from_page(page, POST_LIMIT)
        if not posts:
            logger.error("âŒ æœªèƒ½æå–å¸–å­æ•°æ®")
            await browser.close()
            return
        
        logger.info(f"\nç¬¬1æ­¥å®Œæˆï¼šæå–åˆ° {len(posts)} ä¸ªå¸–å­\n")
        
        # æå–è¯„è®º
        for i, post in enumerate(posts, 1):
            logger.info(f"[{i}/{len(posts)}] å¤„ç†: {post['title'][:40]}")
            
            # è·å–è¯„è®º
            comments = await extract_comments(page, post['id'], post['url'])
            post['comments'] = comments
            
            await asyncio.sleep(REQUEST_INTERVAL)
        
        logger.info(f"\nç¬¬2æ­¥å®Œæˆï¼šè·å–è¯„è®º\n")
        
        # AIåˆ†æ
        logger.info("å¼€å§‹AIåˆ†æ...")
        for i, post in enumerate(posts, 1):
            logger.info(f"[{i}/{len(posts)}] åˆ†æ: {post['title'][:40]}")
            analysis = analyze_with_ai(post, post.get('comments', []))
            post['analysis'] = analysis
        
        logger.info(f"\nç¬¬3æ­¥å®Œæˆï¼šAIåˆ†æ\n")
        
        # ä¿å­˜æ•°æ®åº“
        await save_to_database(posts)
        
        # ä¿å­˜JSONå¤‡ä»½
        os.makedirs('data', exist_ok=True)
        today = datetime.now().strftime("%Y-%m-%d")
        output_file = f"data/heybox_report_{today}.json"
        
        report = {
            'meta': {
                'report_date': today,
                'title': f'å°é»‘ç›’æ¯æ—¥æŠ¥å‘Š ({today})',
                'post_count': len(posts),
                'generation_time': datetime.now().isoformat()
            },
            'posts': posts
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… JSONå¤‡ä»½å·²ä¿å­˜: {output_file}")
        
        # å…³é—­æµè§ˆå™¨
        await browser.close()
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ çˆ¬è™«æ‰§è¡Œå®Œæˆï¼")
    logger.info("=" * 80)

if __name__ == "__main__":
    asyncio.run(main())

