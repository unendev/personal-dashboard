#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°é»‘ç›’APIçˆ¬è™«
ä½¿ç”¨ x_xhh_tokenid è®¤è¯ï¼ŒæŠ“å–é¦–é¡µæœ€æ–°å¸–å­å’Œè¯„è®º

ä½¿ç”¨æ–¹æ³•ï¼š
  1. é…ç½® .env æ–‡ä»¶ä¸­çš„ HEYBOX_TOKEN_ID
  2. è¿è¡Œ: python heybox_api_scraper.py
"""

import asyncio
import os
import json
import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
import requests
from dotenv import load_dotenv
import asyncpg

# å¯¼å…¥é…ç½®
from config import (
    HEYBOX_TOKEN_ID, HEYBOX_BASE_URL, HEYBOX_HOME_URL,
    POST_LIMIT, COMMENT_LIMIT, REQUEST_INTERVAL,
    MAX_RETRIES, RETRY_DELAY, AI_REQUEST_DELAY,
    DEEPSEEK_API_KEY, DEEPSEEK_API_URL,
    DATABASE_URL, IS_GITHUB_ACTIONS,
    get_auth_headers, get_proxies, check_config
)

# ========== æ—¥å¿—é…ç½® ==========
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('logs/heybox_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========== é‡è¯•è£…é¥°å™¨ ==========
def retry_on_failure(max_retries=MAX_RETRIES, delay=RETRY_DELAY):
    """è¯·æ±‚å¤±è´¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"æ‰€æœ‰é‡è¯•å¤±è´¥: {func.__name__}")
                        raise e
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

# ========== API è¯·æ±‚å‡½æ•° ==========

@retry_on_failure()
def fetch_home_feed(limit=POST_LIMIT) -> List[Dict]:
    """
    è·å–é¦–é¡µä¿¡æ¯æµ
    
    æ³¨æ„ï¼šè¿™é‡Œçš„APIç«¯ç‚¹éœ€è¦æ ¹æ®å®é™…æŠ“åŒ…ç»“æœè°ƒæ•´
    å¯èƒ½çš„ç«¯ç‚¹ï¼š
    - /bbs/app/api/feed/home
    - /bbs/app/api/link/feed/home
    - /v1/feed/timeline
    """
    logger.info(f"ğŸ” å¼€å§‹æŠ“å–é¦–é¡µä¿¡æ¯æµï¼ˆç›®æ ‡{limit}æ¡ï¼‰...")
    
    # TODO: æ ¹æ®å®é™…APIè°ƒæ•´endpointå’Œå‚æ•°
    # è¿™é‡Œæä¾›å‡ ä¸ªå¯èƒ½çš„å°è¯•æ–¹æ¡ˆ
    
    possible_endpoints = [
        "/bbs/app/api/feed/home",
        "/bbs/app/api/link/feed/home",
        "/v1/feed/timeline",
        "/bbs/app/link/feed/home"
    ]
    
    headers = get_auth_headers()
    proxies = get_proxies()
    
    # å°è¯•ä¸åŒçš„endpoint
    for endpoint in possible_endpoints:
        url = f"{HEYBOX_BASE_URL}{endpoint}"
        
        try:
            logger.info(f"  å°è¯•API: {url}")
            
            # å¸¸è§å‚æ•°
            params = {
                "limit": limit,
                "offset": 0,
                "time": int(time.time() * 1000)
            }
            
            response = requests.get(
                url,
                headers=headers,
                params=params,
                proxies=proxies,
                timeout=30
            )
            
            logger.info(f"  å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"  âœ“ æˆåŠŸè·å–æ•°æ®")
                logger.debug(f"  å“åº”ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                
                # è§£æå“åº”æ•°æ®ï¼ˆæ ¹æ®å®é™…ç»“æ„è°ƒæ•´ï¼‰
                posts = parse_feed_response(data)
                if posts:
                    logger.info(f"âœ… æˆåŠŸè§£æ{len(posts)}ä¸ªå¸–å­")
                    return posts
                    
        except Exception as e:
            logger.debug(f"  âœ— {endpoint} å¤±è´¥: {e}")
            continue
    
    logger.error("âŒ æ‰€æœ‰APIç«¯ç‚¹å°è¯•å‡å¤±è´¥")
    logger.info("ğŸ’¡ æç¤ºï¼šè¯·æ‰‹åŠ¨æŠ“åŒ…ç¡®è®¤APIåœ°å€ï¼Œå¹¶æ›´æ–°è„šæœ¬")
    return []

def parse_feed_response(data: Dict) -> List[Dict]:
    """
    è§£æAPIå“åº”æ•°æ®
    
    éœ€è¦æ ¹æ®å®é™…å“åº”ç»“æ„è°ƒæ•´
    """
    posts = []
    
    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
    possible_keys = ['data', 'list', 'items', 'feeds', 'result']
    
    feed_list = None
    for key in possible_keys:
        if key in data:
            feed_list = data[key]
            if isinstance(feed_list, list) and len(feed_list) > 0:
                logger.info(f"  æ‰¾åˆ°æ•°æ®åˆ—è¡¨: {key}")
                break
    
    if not feed_list:
        logger.warning("  æœªæ‰¾åˆ°å¸–å­åˆ—è¡¨æ•°æ®")
        return []
    
    for item in feed_list[:POST_LIMIT]:
        try:
            # æ ¹æ®å®é™…å­—æ®µè°ƒæ•´
            post = {
                'id': str(item.get('link_id') or item.get('id') or item.get('post_id')),
                'title': item.get('title', ''),
                'content': item.get('content', '') or item.get('desc', ''),
                'author': item.get('username', '') or item.get('author_name', ''),
                'author_id': str(item.get('userid', '') or item.get('author_id', '')),
                'cover_image': item.get('image', '') or item.get('cover', ''),
                'game_tag': item.get('tag', '') or item.get('game_name', ''),
                'likes_count': int(item.get('like_count', 0) or item.get('likes', 0)),
                'comments_count': int(item.get('comment_count', 0) or item.get('comments', 0)),
                'views_count': int(item.get('view_count', 0) or item.get('views', 0)),
                'created_time': item.get('created_time', 0) or item.get('publish_time', 0),
                'url': f"https://www.xiaoheihe.cn/link/{item.get('link_id', item.get('id', ''))}"
            }
            
            if post['id'] and post['title']:
                posts.append(post)
                
        except Exception as e:
            logger.warning(f"  è§£æå¸–å­å¤±è´¥: {e}")
            continue
    
    return posts

@retry_on_failure()
def fetch_post_comments(post_id: str, limit=COMMENT_LIMIT) -> List[Dict]:
    """
    è·å–å¸–å­è¯„è®º
    
    å¯èƒ½çš„endpointï¼š
    - /bbs/app/api/comment/list
    - /bbs/app/link/tree
    """
    logger.info(f"  ğŸ“ æŠ“å–å¸–å­ {post_id} çš„è¯„è®º...")
    
    headers = get_auth_headers()
    proxies = get_proxies()
    
    possible_endpoints = [
        f"/bbs/app/api/link/tree?link_id={post_id}",
        f"/bbs/app/api/comment/list?link_id={post_id}",
    ]
    
    for endpoint in possible_endpoints:
        url = f"{HEYBOX_BASE_URL}{endpoint}"
        
        try:
            response = requests.get(
                url,
                headers=headers,
                proxies=proxies,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                comments = parse_comments_response(data, post_id)
                if comments:
                    logger.info(f"    âœ“ è·å–åˆ°{len(comments)}æ¡è¯„è®º")
                    return comments[:limit]
                    
        except Exception as e:
            logger.debug(f"    âœ— {endpoint} å¤±è´¥: {e}")
            continue
    
    logger.warning(f"    æœªèƒ½è·å–å¸–å­ {post_id} çš„è¯„è®º")
    return []

def parse_comments_response(data: Dict, post_id: str) -> List[Dict]:
    """è§£æè¯„è®ºå“åº”"""
    comments = []
    
    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
    comment_list = data.get('comments') or data.get('list') or data.get('data') or []
    
    if not comment_list:
        return []
    
    for item in comment_list:
        try:
            comment = {
                'id': str(item.get('comment_id') or item.get('id')),
                'post_id': post_id,
                'author': item.get('username', '') or item.get('author_name', ''),
                'content': item.get('content', '') or item.get('text', ''),
                'likes_count': int(item.get('like_count', 0) or item.get('likes', 0)),
                'created_time': item.get('created_time', 0) or item.get('publish_time', 0),
                'parent_id': str(item.get('parent_id', '') or ''),
                'depth': int(item.get('depth', 0))
            }
            
            if comment['id'] and comment['content']:
                comments.append(comment)
                
        except Exception as e:
            logger.warning(f"    è§£æè¯„è®ºå¤±è´¥: {e}")
            continue
    
    return comments

# ========== DeepSeek AI åˆ†æ ==========

def analyze_post_with_ai(post: Dict, comments: List[Dict]) -> Dict:
    """
    ä½¿ç”¨DeepSeek AIåˆ†æå¸–å­å’Œè¯„è®º
    """
    logger.info(f"  ğŸ¤– AIåˆ†æ: {post['title'][:30]}...")
    
    if not DEEPSEEK_API_KEY:
        logger.warning("    æœªé…ç½®DEEPSEEK_API_KEYï¼Œè·³è¿‡AIåˆ†æ")
        return {
            'core_issue': post.get('content', '')[:100],
            'key_info': [],
            'post_type': 'æœªåˆ†ç±»',
            'value_assessment': 'ä¸­',
            'detailed_analysis': ''
        }
    
    # æ„å»ºprompt
    top_comments = comments[:5]  # å–å‰5æ¡è¯„è®º
    comments_text = "\n".join([f"- {c['author']}: {c['content'][:100]}" for c in top_comments])
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¸¸æˆç¤¾åŒºå†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å°é»‘ç›’å¸–å­ï¼š

æ ‡é¢˜ï¼š{post['title']}
ä½œè€…ï¼š{post['author']}
å†…å®¹æ‘˜è¦ï¼š{post.get('content', '')[:500]}
æ¸¸æˆæ ‡ç­¾ï¼š{post.get('game_tag', 'æ— ')}
äº’åŠ¨æ•°æ®ï¼š{post['likes_count']}èµ / {post['comments_count']}è¯„è®º

è¯„è®ºåŒºè®¨è®ºè¦ç‚¹ï¼ˆå‰5æ¡ï¼‰ï¼š
{comments_text if comments_text else 'æš‚æ— è¯„è®º'}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š
{{
  "core_issue": "æ ¸å¿ƒè®®é¢˜ä¸€å¥è¯æ€»ç»“",
  "key_info": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3"],
  "post_type": "æ¸¸æˆèµ„è®¯/æ¸¸æˆæ”»ç•¥/ç©å®¶è®¨è®º/ç¡¬ä»¶è¯„æµ‹/æ±‚åŠ©é—®ç­”/å…¶ä»–",
  "value_assessment": "é«˜/ä¸­/ä½",
  "detailed_analysis": "## ğŸ“‹ å†…å®¹æ¦‚è¿°\\n...\\n\\n## ğŸ’¡ å…³é”®ä¿¡æ¯\\n...\\n\\n## ğŸ’¬ ç¤¾åŒºè®¨è®º\\n..."
}}"""
    
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¸¸æˆç¤¾åŒºå†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææ¸¸æˆç›¸å…³å¸–å­å’Œç¤¾åŒºè®¨è®ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.5,
                "max_tokens": 2000
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                analysis = json.loads(json_match.group())
                logger.info(f"    âœ“ AIåˆ†æå®Œæˆ")
                time.sleep(AI_REQUEST_DELAY)  # é™é€Ÿ
                return analysis
                
    except Exception as e:
        logger.warning(f"    AIåˆ†æå¤±è´¥: {e}")
    
    # è¿”å›é»˜è®¤åˆ†æ
    return {
        'core_issue': post.get('content', post['title'])[:100],
        'key_info': [post['title']],
        'post_type': 'æœªåˆ†ç±»',
        'value_assessment': 'ä¸­',
        'detailed_analysis': f"## ğŸ“‹ å†…å®¹æ¦‚è¿°\n\n{post.get('content', '')[:200]}"
    }

# ========== æ•°æ®åº“æ“ä½œ ==========

async def save_to_database(posts_with_analysis: List[Dict]):
    """ä¿å­˜æ•°æ®åˆ°PostgreSQL"""
    logger.info(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    
    if not DATABASE_URL:
        logger.error("âŒ æœªé…ç½®DATABASE_URL")
        return False
    
    # æ¸…ç†URLï¼ˆasyncpgä¸æ”¯æŒæŸ¥è¯¢å‚æ•°ï¼‰
    db_url = DATABASE_URL.split('?')[0] if '?' in DATABASE_URL else DATABASE_URL
    
    try:
        conn = await asyncpg.connect(db_url)
        logger.info("  âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        saved_posts = 0
        saved_comments = 0
        
        for post in posts_with_analysis:
            try:
                # æ’å…¥å¸–å­ï¼ˆå»é‡ï¼‰
                await conn.execute('''
                    INSERT INTO heybox_posts (
                        id, title, url, author, avatar_url, cover_image,
                        content_summary, likes_count, comments_count, views_count,
                        game_tag, core_issue, key_info, post_type,
                        value_assessment, detailed_analysis, timestamp
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17)
                    ON CONFLICT (id) DO UPDATE SET
                        likes_count = $8,
                        comments_count = $9,
                        timestamp = $17
                ''', 
                    post['id'], post['title'], post['url'],
                    post.get('author'), None, post.get('cover_image'),
                    post.get('content', '')[:1000], post['likes_count'],
                    post['comments_count'], post.get('views_count', 0),
                    post.get('game_tag'), post['analysis']['core_issue'],
                    json.dumps(post['analysis']['key_info']),
                    post['analysis']['post_type'],
                    post['analysis']['value_assessment'],
                    post['analysis']['detailed_analysis'],
                    datetime.fromtimestamp(post.get('created_time', time.time()))
                )
                saved_posts += 1
                
                # æ’å…¥è¯„è®º
                for comment in post.get('comments', []):
                    try:
                        await conn.execute('''
                            INSERT INTO heybox_comments (
                                id, post_id, author, content, likes_count,
                                created_at, parent_id, depth
                            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                            ON CONFLICT (id) DO UPDATE SET
                                likes_count = $5
                        ''',
                            comment['id'], post['id'], comment['author'],
                            comment['content'], comment['likes_count'],
                            datetime.fromtimestamp(comment.get('created_time', time.time())),
                            comment.get('parent_id') or None,
                            comment.get('depth', 0)
                        )
                        saved_comments += 1
                    except Exception as e:
                        logger.warning(f"    ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
                        
            except Exception as e:
                logger.warning(f"    ä¿å­˜å¸–å­å¤±è´¥ {post['id']}: {e}")
        
        await conn.close()
        logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {saved_posts}ä¸ªå¸–å­, {saved_comments}æ¡è¯„è®º")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return False

# ========== ä¸»æµç¨‹ ==========

async def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    logger.info("=" * 80)
    logger.info("ğŸ® å°é»‘ç›’çˆ¬è™«å¯åŠ¨")
    logger.info("=" * 80)
    
    # æ£€æŸ¥é…ç½®
    issues = check_config()
    if issues:
        logger.error("\né…ç½®é—®é¢˜ï¼š")
        for issue in issues:
            logger.error(f"  {issue}")
        return
    
    logger.info(f"\né…ç½®ä¿¡æ¯ï¼š")
    logger.info(f"  - ç›®æ ‡å¸–å­æ•°: {POST_LIMIT}")
    logger.info(f"  - è¯„è®ºæ•°é™åˆ¶: {COMMENT_LIMIT}")
    logger.info(f"  - Tokenå·²é…ç½®: æ˜¯")
    logger.info(f"  - AIåˆ†æ: {'æ˜¯' if DEEPSEEK_API_KEY else 'å¦'}")
    
    # ç¬¬1æ­¥ï¼šè·å–é¦–é¡µä¿¡æ¯æµ
    posts = fetch_home_feed(POST_LIMIT)
    if not posts:
        logger.error("âŒ æœªèƒ½è·å–å¸–å­æ•°æ®")
        logger.info("\nğŸ’¡ è°ƒè¯•å»ºè®®ï¼š")
        logger.info("  1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰è®¿é—®å°é»‘ç›’é¦–é¡µ")
        logger.info("  2. æŸ¥çœ‹Networkæ ‡ç­¾ï¼Œæ‰¾åˆ°è·å–å¸–å­åˆ—è¡¨çš„APIè¯·æ±‚")
        logger.info("  3. è®°å½•APIåœ°å€ã€è¯·æ±‚å¤´ã€å‚æ•°")
        logger.info("  4. æ›´æ–° fetch_home_feed() å‡½æ•°ä¸­çš„APIé…ç½®")
        return
    
    logger.info(f"\nç¬¬1æ­¥å®Œæˆï¼šè·å–åˆ°{len(posts)}ä¸ªå¸–å­\n")
    
    # ç¬¬2æ­¥ï¼šè·å–æ¯ä¸ªå¸–å­çš„è¯„è®º
    for i, post in enumerate(posts, 1):
        logger.info(f"[{i}/{len(posts)}] å¤„ç†: {post['title'][:40]}")
        
        # è·å–è¯„è®º
        comments = fetch_post_comments(post['id'], COMMENT_LIMIT)
        post['comments'] = comments
        
        time.sleep(REQUEST_INTERVAL)  # é™é€Ÿ
    
    logger.info(f"\nç¬¬2æ­¥å®Œæˆï¼šè·å–è¯„è®º\n")
    
    # ç¬¬3æ­¥ï¼šAIåˆ†æ
    logger.info("å¼€å§‹AIåˆ†æ...")
    for i, post in enumerate(posts, 1):
        logger.info(f"[{i}/{len(posts)}] åˆ†æ: {post['title'][:40]}")
        analysis = analyze_post_with_ai(post, post.get('comments', []))
        post['analysis'] = analysis
    
    logger.info(f"\nç¬¬3æ­¥å®Œæˆï¼šAIåˆ†æ\n")
    
    # ç¬¬4æ­¥ï¼šä¿å­˜åˆ°æ•°æ®åº“
    success = await save_to_database(posts)
    
    # ç¬¬5æ­¥ï¼šä¿å­˜JSONå¤‡ä»½
    if success:
        os.makedirs('data', exist_ok=True)
        today = datetime.now().strftime("%Y-%m-%d")
        output_file = f"data/heybox_report_{today}.json"
        
        report = {
            'meta': {
                'report_date': today,
                'title': f'å°é»‘ç›’æ¯æ—¥æŠ¥å‘Š ({today})',
                'post_count': len(posts),
                'generation_time': datetime.now().isoformat()
            },
            'posts': posts
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… JSONå¤‡ä»½å·²ä¿å­˜: {output_file}")
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ çˆ¬è™«æ‰§è¡Œå®Œæˆï¼")
    logger.info("=" * 80)

if __name__ == "__main__":
    asyncio.run(main())



