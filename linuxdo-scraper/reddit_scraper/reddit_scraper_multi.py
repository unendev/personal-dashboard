# reddit_scraper_multi.py - å¤šSubredditçˆ¬è™« + ä¸­æ–‡ç¿»è¯‘ä¼˜åŒ–
import asyncio
import os
import re
import json
import logging
from datetime import datetime
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg
import ssl

# --- é…ç½®æ—¥å¿— ---
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reddit_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- é…ç½® ---
load_dotenv()

# æ”¯æŒå¤šä¸ªsubreddit
SUBREDDITS = [
    "technology",    # ç§‘æŠ€
    "gamedev",       # ç‹¬ç«‹æ¸¸æˆå¼€å‘
    "godot",         # Godotå¼•æ“
    "Unity3D",       # Unityå¼•æ“
    "unrealengine"   # è™šå¹»å¼•æ“
]

POST_COUNT_PER_SUB = 5  # æ¯ä¸ªsubredditå–5ä¸ªå¸–å­
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
NEON_DB_URL = os.getenv("DATABASE_URL")

# asyncpgä¸æ”¯æŒURLä¸­çš„æŸ¥è¯¢å‚æ•°ï¼Œéœ€è¦æ¸…ç†
if NEON_DB_URL:
    original_url = NEON_DB_URL
    if '?' in NEON_DB_URL:
        NEON_DB_URL = NEON_DB_URL.split('?')[0]
        logger.info("å·²æ¸…ç†DATABASE_URLä¸­çš„æŸ¥è¯¢å‚æ•°")
    
    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºURLæ ¼å¼ï¼ˆéšè—å¯†ç ï¼‰
    import re
    safe_url = re.sub(r'://([^:]+):([^@]+)@', r'://\1:****@', NEON_DB_URL)
    logger.info(f"DATABASE_URLæ ¼å¼: {safe_url}")
    
    # è§£æURLå„éƒ¨åˆ†
    try:
        from urllib.parse import urlparse
        parsed = urlparse(NEON_DB_URL)
        logger.info(f"  - åè®®: {parsed.scheme}")
        logger.info(f"  - ä¸»æœº: {parsed.hostname}")
        logger.info(f"  - ç«¯å£: {parsed.port}")
        logger.info(f"  - æ•°æ®åº“: {parsed.path}")
        logger.info(f"  - ç”¨æˆ·å: {parsed.username}")
    except Exception as e:
        logger.error(f"è§£æDATABASE_URLå¤±è´¥: {e}")
else:
    logger.error("DATABASE_URL ä¸ºç©ºï¼")

# GitHub Actionsç¯å¢ƒæ£€æµ‹ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰
IS_GITHUB_ACTIONS = os.getenv("GITHUB_ACTIONS") == "true"
if not IS_GITHUB_ACTIONS and os.path.exists(".env"):
    # æœ¬åœ°å¼€å‘ç¯å¢ƒå¯ä»¥ä½¿ç”¨ä»£ç†
    proxy_for_all = os.getenv("PROXY_URL", "")
    if proxy_for_all:
        os.environ['HTTP_PROXY'] = proxy_for_all
        os.environ['HTTPS_PROXY'] = proxy_for_all
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_for_all}")

# --- Reddit çˆ¬è™«å‡½æ•° ---
def fetch_reddit_posts_from_subreddit(subreddit):
    """ä»å•ä¸ªSubreddit RSSè·å–å¸–å­æ•°æ®"""
    RSS_URL = f"https://www.reddit.com/r/{subreddit}/.rss?sort=hot"
    logger.info(f"  å¼€å§‹çˆ¬å– r/{subreddit}...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    proxies = {}
    if not IS_GITHUB_ACTIONS and os.getenv("PROXY_URL"):
        proxy_url = os.getenv("PROXY_URL")
        proxies = {"http": proxy_url, "https": proxy_url}
    
    try:
        response = requests.get(RSS_URL, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        posts = []
        
        for i, entry in enumerate(root.findall('{http://www.w3.org/2005/Atom}entry')):
            if i >= POST_COUNT_PER_SUB:
                break
                
            title_tag = entry.find('{http://www.w3.org/2005/Atom}title')
            link_tag = entry.find('{http://www.w3.org/2005/Atom}link')
            content_tag = entry.find('{http://www.w3.org/2005/Atom}content')
            author_tag = entry.find('{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name')
            
            if title_tag is not None and link_tag is not None:
                title = title_tag.text
                link = link_tag.get('href')
                author = author_tag.text if author_tag is not None else "N/A"
                
                # æ¸…ç†å†…å®¹
                content_html = content_tag.text if content_tag is not None else ""
                clean_content = re.sub(r'<.*?>', ' ', content_html)
                clean_content = re.sub(r'\[link\]|\[comments\]', '', clean_content).strip()
                
                # æå–Redditå¸–å­ID
                post_id = "reddit_unknown"
                if '/comments/' in link:
                    parts = link.split('/comments/')
                    if len(parts) > 1:
                        id_part = parts[1].split('/')[0]
                        post_id = f"reddit_{subreddit}_{id_part}"
                
                posts.append({
                    "id": post_id,
                    "title": title,
                    "link": link,
                    "author": author,
                    "content": clean_content[:500] + '...' if len(clean_content) > 500 else clean_content,
                    "subreddit": subreddit,
                    "score": 0,
                    "num_comments": 0
                })
        
        logger.info(f"    âœ“ r/{subreddit} è·å–åˆ° {len(posts)} ä¸ªå¸–å­")
        return posts
        
    except requests.exceptions.RequestException as e:
        logger.error(f"    âœ— è¯·æ±‚ r/{subreddit} RSSå¤±è´¥: {e}")
        return []
    except ET.ParseError as e:
        logger.error(f"    âœ— è§£æ r/{subreddit} XMLå¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"    âœ— å¤„ç† r/{subreddit} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def fetch_all_reddit_posts():
    """ä»æ‰€æœ‰é…ç½®çš„subredditè·å–å¸–å­"""
    logger.info("=== å¼€å§‹çˆ¬å–æ‰€æœ‰Subreddit ===")
    all_posts = []
    
    for subreddit in SUBREDDITS:
        posts = fetch_reddit_posts_from_subreddit(subreddit)
        all_posts.extend(posts)
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    logger.info(f"=== æ€»è®¡è·å– {len(all_posts)} ä¸ªå¸–å­ ===")
    return all_posts

# --- AIåˆ†æå‡½æ•° (ä¼˜åŒ–ä¸­æ–‡è¾“å‡º + è¯„è®ºé›†æˆ) ---
async def fetch_post_comments(post_id):
    """ä»æ•°æ®åº“è·å–å¸–å­çš„é«˜è´¨é‡è¯„è®º"""
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        # è·å–è¯„åˆ†æœ€é«˜çš„å‰5æ¡è¯„è®º
        comments = await conn.fetch("""
            SELECT author, body, score 
            FROM reddit_comments 
            WHERE post_id = $1 
            ORDER BY score DESC 
            LIMIT 5
        """, post_id)
        await conn.close()
        return [dict(c) for c in comments]
    except:
        return []

async def analyze_single_post_with_deepseek(post, retry_count=0, comments=None):
    """ä½¿ç”¨Geminiåˆ†æRedditå¸–å­å¹¶è¾“å‡ºå®Œæ•´ä¸­æ–‡ï¼ˆåŒ…å«è¯„è®ºç²¾åï¼‰"""
    excerpt = post.get('content', '')[:1000]
    if not excerpt.strip():
        excerpt = "ï¼ˆæ— è¯¦ç»†å†…å®¹ï¼‰"
    
    # æ„å»ºè¯„è®ºæ‘˜è¦
    comment_section = ""
    if comments and len(comments) > 0:
        comment_section = "\n\n**ç¤¾åŒºè®¨è®ºç²¾å**ï¼ˆé«˜èµè¯„è®ºï¼‰ï¼š\n"
        for i, comment in enumerate(comments[:3], 1):
            comment_body = comment['body'][:200]
            comment_section += f"{i}. [{comment['author']}] (ğŸ‘{comment['score']}): {comment_body}...\n"
        logger.info(f"  âœ“ åŒ…å« {len(comments[:3])} æ¡è¯„è®ºåˆ°åˆ†æ Prompt")
    else:
        num_comments = post.get('num_comments', 0)
        if num_comments > 0:
            comment_section = f"\n\n**æ³¨æ„**ï¼šè¯¥å¸–å­æœ‰ {num_comments} æ¡è¯„è®ºï¼Œä½†è¯„è®ºå†…å®¹æœªåŒ…å«åœ¨æœ¬æ¬¡åˆ†æä¸­ã€‚è¯·ä»…åŸºäºå¸–å­æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸è¦æ¨æµ‹è¯„è®ºåŒºå†…å®¹ã€‚"
            logger.info(f"  âš  å¸–å­æœ‰ {num_comments} æ¡è¯„è®ºä½†æœªè·å–åˆ°å†…å®¹")
        else:
            comment_section = ""
            logger.info(f"  â„¹ è¯¥å¸–å­æ— è¯„è®º")

    prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„RedditæŠ€æœ¯å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å¸–å­ï¼ˆå«ç¤¾åŒºè®¨è®ºï¼‰ï¼Œç”Ÿæˆä¸“ä¸šæŠ€æœ¯åˆ†ææŠ¥å‘Šã€‚

**åŸå§‹å¸–å­ä¿¡æ¯**ï¼š
- æ ‡é¢˜: {post['title']}
- æ¿å—: r/{post['subreddit']}
- å†…å®¹: {excerpt}{comment_section}

**è¯·ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦åŒ…å«```json```æ ‡è®°ï¼‰**ï¼š
{{
  "title_cn": "ä¸­æ–‡æ ‡é¢˜ç¿»è¯‘",
  "core_issue": "æ ¸å¿ƒè®®é¢˜ï¼ˆä¸€å¥è¯ï¼‰",
  "key_info": ["å…³é”®ä¿¡æ¯1", "å…³é”®ä¿¡æ¯2", "å…³é”®ä¿¡æ¯3"],
  "post_type": "ä»[æŠ€æœ¯è®¨è®º, æ–°é—»åˆ†äº«, é—®é¢˜æ±‚åŠ©, è§‚ç‚¹è®¨è®º, èµ„æºåˆ†äº«, æ•™ç¨‹æŒ‡å—, é¡¹ç›®å±•ç¤º, å…¶ä»–]é€‰ä¸€ä¸ª",
  "value_assessment": "ä»[é«˜, ä¸­, ä½]é€‰ä¸€ä¸ª",
  "detailed_analysis": "ç”Ÿæˆ600-1200å­—ä¸“ä¸šæŠ€æœ¯åˆ†æï¼Œmarkdownæ ¼å¼ï¼ŒåŒ…å«ï¼šæŠ€æœ¯èƒŒæ™¯ã€æ ¸å¿ƒæ–¹æ¡ˆã€å·¥ç¨‹å®è·µã€ç¤¾åŒºè®¨è®ºã€åº”ç”¨æŒ‡å—ã€æŠ€æœ¯è¶‹åŠ¿"
}}
"""
    
    # è°ƒè¯•ï¼šè¾“å‡º Prompt çš„å…³é”®éƒ¨åˆ†ï¼ˆé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
    logger.debug(f"  â†’ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦, è¯„è®ºåŒºé•¿åº¦: {len(comment_section)} å­—ç¬¦")
    
    try:
        # ä½¿ç”¨ DeepSeek API (REST)
        def call_deepseek():
            response = requests.post(
                DEEPSEEK_API_URL,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 2000,
                },
                timeout=60,
            )
            
            if not response.ok:
                raise Exception(f"DeepSeek API error: {response.status_code}")
            
            return response.json()
        
        data = await asyncio.to_thread(call_deepseek)
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        
        # æ¸…ç†JSONå†…å®¹
        if content.startswith('```json'):
            content = content[7:]
        if content.startswith('```'):
            content = content[3:]
        if content.endswith('```'):
            content = content[:-3]
        content = content.strip()
        
        try:
            analysis = json.loads(content)
            logger.info(f"  âœ“ å¸–å­åˆ†ææˆåŠŸ: {analysis.get('title_cn', 'N/A')[:30]}...")
            return analysis
        except json.JSONDecodeError:
            logger.error(f"  âœ— DeepSeekè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {content[:100]}...")
            return {
                "title_cn": post['title'][:50] + "...",
                "core_issue": "JSONè§£æå¤±è´¥",
                "key_info": ["è§£æå¤±è´¥"],
                "post_type": "å…¶ä»–",
                "value_assessment": "ä½"
            }
            
    except Exception as e:
        logger.error(f"  âœ— AIåˆ†æå¤±è´¥: {e}")
        if retry_count < 2:
            logger.info(f"  âŸ³ é‡è¯•AIåˆ†æ ({retry_count + 1}/2)...")
            await asyncio.sleep(2)
            return await analyze_single_post_with_deepseek(post, retry_count + 1, comments)
        else:
            return {
                "title_cn": post['title'][:50] + "...",
                "core_issue": "åˆ†æå¤±è´¥",
                "key_info": ["åˆ†æå¤±è´¥"],
                "post_type": "å…¶ä»–",
                "value_assessment": "ä½"
            }

# --- æ•°æ®åº“æ“ä½œ ---
async def create_posts_table():
    """åˆ›å»ºredditå¸–å­è¡¨"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
        return False

    conn = None
    try:
        # Neonéœ€è¦SSLè¿æ¥
        logger.info(f"å°è¯•è¿æ¥æ•°æ®åº“...")
        
        # æµ‹è¯•DNSè§£æ
        try:
            from urllib.parse import urlparse
            import socket
            parsed = urlparse(NEON_DB_URL)
            host = parsed.hostname
            logger.info(f"  æ­¥éª¤1: å°è¯•DNSè§£æä¸»æœº {host}...")
            ip = socket.gethostbyname(host)
            logger.info(f"  âœ“ DNSè§£ææˆåŠŸ: {host} -> {ip}")
        except socket.gaierror as e:
            logger.error(f"  âœ— DNSè§£æå¤±è´¥: {e}")
            logger.error(f"  å¯èƒ½åŸå› : 1)ä¸»æœºåé”™è¯¯ 2)ç½‘ç»œé—®é¢˜ 3)DNSæœåŠ¡å™¨é—®é¢˜")
            raise
        except Exception as e:
            logger.error(f"  âœ— DNSæµ‹è¯•å¼‚å¸¸: {e}")
            raise
        
        logger.info(f"  æ­¥éª¤2: é…ç½®SSLä¸Šä¸‹æ–‡...")
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        logger.info(f"  âœ“ SSLé…ç½®å®Œæˆ")
        
        logger.info(f"  æ­¥éª¤3: å°è¯•å»ºç«‹æ•°æ®åº“è¿æ¥...")
        conn = await asyncpg.connect(
            dsn=NEON_DB_URL,
            ssl=ssl_context,
            timeout=60
        )
        logger.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS reddit_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                title_cn TEXT,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                subreddit TEXT,
                score INTEGER,
                num_comments INTEGER,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("âœ“ æ•°æ®åº“è¡¨ 'reddit_posts' æ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âœ— åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
        return False

    conn = None
    try:
        # Neonéœ€è¦SSLè¿æ¥
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        conn = await asyncpg.connect(
            dsn=NEON_DB_URL,
            ssl=ssl_context,
            timeout=60
        )
        logger.info("å¼€å§‹æ’å…¥æ•°æ®åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                title_cn = post.get('analysis', {}).get('title_cn', title)
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []), ensure_ascii=False)
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                detailed_analysis = analysis.get('detailed_analysis')
                subreddit = post.get('subreddit')
                score = post.get('score', 0)
                num_comments = post.get('num_comments', 0)

                await conn.execute("""
                    INSERT INTO reddit_posts (id, title, title_cn, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, subreddit, score, num_comments)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                    ON CONFLICT (id) DO UPDATE SET
                        title = EXCLUDED.title,
                        title_cn = EXCLUDED.title_cn,
                        url = EXCLUDED.url,
                        core_issue = EXCLUDED.core_issue,
                        key_info = EXCLUDED.key_info,
                        post_type = EXCLUDED.post_type,
                        value_assessment = EXCLUDED.value_assessment,
                        detailed_analysis = EXCLUDED.detailed_analysis,
                        subreddit = EXCLUDED.subreddit,
                        score = EXCLUDED.score,
                        num_comments = EXCLUDED.num_comments,
                        timestamp = CURRENT_TIMESTAMP;
                """, post_id, title, title_cn, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, subreddit, score, num_comments)
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"  âœ— æ’å…¥å¸–å­ {post.get('id', 'N/A')} å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ“ æˆåŠŸæ’å…¥/æ›´æ–° {success_count}/{len(posts_data)} æ¡æ•°æ®")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âœ— æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# --- AIæ•´ä½“æ´å¯ŸæŠ¥å‘Š ---
async def generate_ai_summary_report(posts_data):
    """ç”Ÿæˆæ•´ä½“åˆ†ææŠ¥å‘Šï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œæ”¯æŒè¯„è®ºæŸ¥è¯¢ï¼Œä½¿ç”¨DeepSeekï¼‰"""
    processed_posts = []
    post_summaries = []

    logger.info("=== å¼€å§‹AIåˆ†æï¼ˆåŒ…å«è¯„è®ºï¼‰===")

    # é¢„å…ˆè·å–æ‰€æœ‰è¯„è®º
    all_comments = []
    for post in posts_data:
        logger.info(f"â†’ è·å–è¯„è®º: r/{post['subreddit']} - {post['title'][:40]}...")
        comments = await fetch_post_comments(post.get('id'))
        if comments:
            logger.info(f"  âœ“ è·å–åˆ° {len(comments)} æ¡é«˜è´¨é‡è¯„è®º")
        all_comments.append(comments)

    # å¹¶å‘åˆ†ææ‰€æœ‰å¸–å­
    logger.info(f"=== å¼€å§‹å¹¶å‘åˆ†æ {len(posts_data)} ä¸ªå¸–å­ ===")
    tasks = [
        analyze_single_post_with_deepseek(post, comments=comments) 
        for post, comments in zip(posts_data, all_comments)
    ]
    analyses = await asyncio.gather(*tasks)

    logger.info("=== AIåˆ†æå®Œæˆ ===")
    
    # å¤„ç†åˆ†æç»“æœ
    for i, analysis in enumerate(analyses):
        post = posts_data[i]
        if analysis and analysis.get("core_issue") not in ["åˆ†æå¤±è´¥", "JSONè§£æå¤±è´¥"]:
            processed_posts.append({
                "id": post.get('id'),
                "title": post.get('title'),
                "title_cn": analysis.get('title_cn', post.get('title')),
                "link": post.get('link'),
                "author": post.get('author'),
                "content": post.get('content'),
                "analysis": analysis,
                "subreddit": post.get('subreddit'),
                "score": post.get('score', 0),
                "num_comments": post.get('num_comments', 0)
            })
            
            post_summaries.append({
                "subreddit": post.get('subreddit'),
                "title_cn": analysis.get('title_cn', 'N/A'),
                "core_issue": analysis.get('core_issue', 'N/A'),
                "post_type": analysis.get('post_type', 'N/A'),
                "value_assessment": analysis.get('value_assessment')
            })
        else:
            logger.error(f"  âœ— å¸–å­åˆ†æå¤±è´¥: {post['title'][:30]}...")
    
    # ç”Ÿæˆæ•´ä½“æ´å¯Ÿï¼ˆå·²ç§»é™¤ - ä½¿ç”¨ DeepSeek ç»Ÿä¸€åˆ†æï¼‰
    logger.info("=== è·³è¿‡æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆ ===")
    summary_analysis = {
        "overview": f"ä»Šæ—¥å…±åˆ†æ {len(processed_posts)} ç¯‡æŠ€æœ¯/æ¸¸æˆå¼€å‘ç›¸å…³å¸–å­ã€‚",
        "highlights": {"tech_news": [], "dev_insights": [], "hot_topics": []},
        "conclusion": "è¯·æŸ¥çœ‹å„å¸–å­è¯¦ç»†åˆ†æã€‚"
    }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- æŠ¥å‘Šç”Ÿæˆ ---
def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Š"""
    try:
        os.makedirs('data', exist_ok=True)
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"data/reddit_multi_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"RedditæŠ€æœ¯+æ¸¸æˆå¼€å‘æ¯æ—¥æŠ¥å‘Š ({today_str})",
                "subreddits": SUBREDDITS,
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "subreddit": post.get('subreddit'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "title_cn": post.get('title_cn', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ JSONæŠ¥å‘Šç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âœ— JSONæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None

def generate_markdown_report(report_data, posts_count):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    try:
        os.makedirs('reports', exist_ok=True)
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"reports/Reddit_Multi_Daily_Report_{today_str}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# RedditæŠ€æœ¯+æ¸¸æˆå¼€å‘æ¯æ—¥æŠ¥å‘Š ({today_str})\n\n")
            f.write(f"> æ¥æºæ¿å—: {', '.join([f'r/{s}' for s in SUBREDDITS])}\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ¯ ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ\n\n")
            
            summary = report_data.get('summary_analysis', {})
            if summary:
                f.write(f"**æ•´ä½“æ¦‚å†µ**: {summary.get('overview', 'æš‚æ— ')}\n\n")
                
                highlights = summary.get('highlights', {})
                if highlights.get('tech_news'):
                    f.write("**ğŸ“° æŠ€æœ¯æ–°é—»**:\n")
                    for item in highlights['tech_news']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('dev_insights'):
                    f.write("**ğŸ® å¼€å‘è§è§£**:\n")
                    for item in highlights['dev_insights']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('hot_topics'):
                    f.write("**ğŸ”¥ çƒ­é—¨è¯é¢˜**:\n")
                    for item in highlights['hot_topics']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                f.write(f"**ğŸ“ å°ç»“**: {summary.get('conclusion', 'æš‚æ— ')}\n\n")
            
            f.write("---\n\n")
            f.write(f"## ğŸ“‹ å¸–å­åˆ—è¡¨ (å…± {posts_count} ç¯‡)\n\n")
            
            # æŒ‰subredditåˆ†ç»„
            posts = report_data.get('processed_posts', [])
            if posts:
                current_sub = None
                for i, post in enumerate(posts, 1):
                    subreddit = post.get('subreddit')
                    if subreddit != current_sub:
                        current_sub = subreddit
                        f.write(f"\n### r/{subreddit}\n\n")
                    
                    title_cn = post.get('title_cn', post.get('title', 'æ— æ ‡é¢˜'))
                    link = post.get('link', '#')
                    analysis = post.get('analysis', {})
                    
                    f.write(f"{i}. **{title_cn}**\n")
                    f.write(f"   - åŸæ ‡é¢˜: {post.get('title', 'N/A')}\n")
                    f.write(f"   - é“¾æ¥: {link}\n")
                    f.write(f"   - æ ¸å¿ƒè®®é¢˜: {analysis.get('core_issue', 'N/A')}\n")
                    f.write(f"   - ç±»å‹: {analysis.get('post_type', 'N/A')} | ä»·å€¼: {analysis.get('value_assessment', 'N/A')}\n\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­ã€‚\n")

        logger.info(f"âœ“ MarkdownæŠ¥å‘Šç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âœ— MarkdownæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None

# --- ä¸»å‡½æ•° ---
async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸš€ Redditå¤šæ¿å—çˆ¬è™«ä»»åŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“‹ ç›®æ ‡æ¿å—: {', '.join(SUBREDDITS)}")
    logger.info(f"ğŸŒ è¿è¡Œç¯å¢ƒ: {'GitHub Actions' if IS_GITHUB_ACTIONS else 'æœ¬åœ°ç¯å¢ƒ'}")
    logger.info("=" * 60)
    
    try:
        # éªŒè¯ç¯å¢ƒå˜é‡
        if not DEEPSEEK_API_KEY:
            logger.error("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return False
            
        if not NEON_DB_URL:
            logger.error("âŒ æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
            return False
        
        # éªŒè¯ DeepSeek API é…ç½®
        logger.info("âœ“ DeepSeek API é…ç½®å·²åŠ è½½")
        
        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            logger.error("âŒ æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        # è·å–æ‰€æœ‰å¸–å­
        posts_data = fetch_all_reddit_posts()
        
        if not posts_data:
            logger.error("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­")
            return False
        
        logger.info(f"âœ“ å…±è·å– {len(posts_data)} ä¸ªå¸–å­")
        
        # ç”ŸæˆAIæŠ¥å‘Šï¼ˆåŒ…å«è¯„è®ºåˆ†æï¼Œä½¿ç”¨DeepSeekï¼‰
        report_data = await generate_ai_summary_report(posts_data)
        
        # æ’å…¥æ•°æ®åº“
        if report_data.get('processed_posts'):
            await insert_posts_into_db(report_data['processed_posts'])
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        json_file = generate_json_report(report_data, len(posts_data))
        md_file = generate_markdown_report(report_data, len(posts_data))
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 60)
        logger.info("âœ… ä»»åŠ¡å®Œæˆ")
        logger.info(f"â±ï¸  ç”¨æ—¶: {duration:.2f} ç§’")
        logger.info(f"ğŸ“Š å¤„ç†: {len(posts_data)} ä¸ªå¸–å­")
        logger.info(f"ğŸ“„ æŠ¥å‘Š: {json_file}")
        logger.info(f"ğŸ“„ æŠ¥å‘Š: {md_file}")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)

