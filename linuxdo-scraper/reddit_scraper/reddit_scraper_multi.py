# reddit_scraper_multi.py - å¤šSubredditçˆ¬è™« + ä¸­æ–‡ç¿»è¯‘ä¼˜åŒ–
import asyncio
import os
import re
import json
import logging
from datetime import datetime
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg
import ssl

# --- é…ç½®æ—¥å¿— ---
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reddit_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- é…ç½® ---
load_dotenv()

# æ”¯æŒå¤šä¸ªsubreddit
SUBREDDITS = [
    "technology",    # ç§‘æŠ€
    "gamedev",       # ç‹¬ç«‹æ¸¸æˆå¼€å‘
    "godot",         # Godotå¼•æ“
    "Unity3D",       # Unityå¼•æ“
    "unrealengine"   # è™šå¹»å¼•æ“
]

POST_COUNT_PER_SUB = 5  # æ¯ä¸ªsubredditå–5ä¸ªå¸–å­
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
NEON_DB_URL = os.getenv("DATABASE_URL")

# asyncpgä¸æ”¯æŒURLä¸­çš„æŸ¥è¯¢å‚æ•°ï¼Œéœ€è¦æ¸…ç†
if NEON_DB_URL:
    original_url = NEON_DB_URL
    if '?' in NEON_DB_URL:
        NEON_DB_URL = NEON_DB_URL.split('?')[0]
        logger.info("å·²æ¸…ç†DATABASE_URLä¸­çš„æŸ¥è¯¢å‚æ•°")
    
    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºURLæ ¼å¼ï¼ˆéšè—å¯†ç ï¼‰
    import re
    safe_url = re.sub(r'://([^:]+):([^@]+)@', r'://\1:****@', NEON_DB_URL)
    logger.info(f"DATABASE_URLæ ¼å¼: {safe_url}")
    
    # è§£æURLå„éƒ¨åˆ†
    try:
        from urllib.parse import urlparse
        parsed = urlparse(NEON_DB_URL)
        logger.info(f"  - åè®®: {parsed.scheme}")
        logger.info(f"  - ä¸»æœº: {parsed.hostname}")
        logger.info(f"  - ç«¯å£: {parsed.port}")
        logger.info(f"  - æ•°æ®åº“: {parsed.path}")
        logger.info(f"  - ç”¨æˆ·å: {parsed.username}")
    except Exception as e:
        logger.error(f"è§£æDATABASE_URLå¤±è´¥: {e}")
else:
    logger.error("DATABASE_URL ä¸ºç©ºï¼")

# GitHub Actionsç¯å¢ƒæ£€æµ‹ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰
IS_GITHUB_ACTIONS = os.getenv("GITHUB_ACTIONS") == "true"
if not IS_GITHUB_ACTIONS and os.path.exists(".env"):
    # æœ¬åœ°å¼€å‘ç¯å¢ƒå¯ä»¥ä½¿ç”¨ä»£ç†
    proxy_for_all = os.getenv("PROXY_URL", "")
    if proxy_for_all:
        os.environ['HTTP_PROXY'] = proxy_for_all
        os.environ['HTTPS_PROXY'] = proxy_for_all
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_for_all}")

# --- Reddit çˆ¬è™«å‡½æ•° ---
def fetch_reddit_posts_from_subreddit(subreddit):
    """ä»å•ä¸ªSubreddit RSSè·å–å¸–å­æ•°æ®"""
    RSS_URL = f"https://www.reddit.com/r/{subreddit}/.rss?sort=hot"
    logger.info(f"  å¼€å§‹çˆ¬å– r/{subreddit}...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    proxies = {}
    if not IS_GITHUB_ACTIONS and os.getenv("PROXY_URL"):
        proxy_url = os.getenv("PROXY_URL")
        proxies = {"http": proxy_url, "https": proxy_url}
    
    try:
        response = requests.get(RSS_URL, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        posts = []
        
        for i, entry in enumerate(root.findall('{http://www.w3.org/2005/Atom}entry')):
            if i >= POST_COUNT_PER_SUB:
                break
                
            title_tag = entry.find('{http://www.w3.org/2005/Atom}title')
            link_tag = entry.find('{http://www.w3.org/2005/Atom}link')
            content_tag = entry.find('{http://www.w3.org/2005/Atom}content')
            author_tag = entry.find('{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name')
            
            if title_tag is not None and link_tag is not None:
                title = title_tag.text
                link = link_tag.get('href')
                author = author_tag.text if author_tag is not None else "N/A"
                
                # æ¸…ç†å†…å®¹
                content_html = content_tag.text if content_tag is not None else ""
                clean_content = re.sub(r'<.*?>', ' ', content_html)
                clean_content = re.sub(r'\[link\]|\[comments\]', '', clean_content).strip()
                
                # æå–Redditå¸–å­ID
                post_id = "reddit_unknown"
                if '/comments/' in link:
                    parts = link.split('/comments/')
                    if len(parts) > 1:
                        id_part = parts[1].split('/')[0]
                        post_id = f"reddit_{subreddit}_{id_part}"
                
                posts.append({
                    "id": post_id,
                    "title": title,
                    "link": link,
                    "author": author,
                    "content": clean_content[:500] + '...' if len(clean_content) > 500 else clean_content,
                    "subreddit": subreddit,
                    "score": 0,
                    "num_comments": 0
                })
        
        logger.info(f"    âœ“ r/{subreddit} è·å–åˆ° {len(posts)} ä¸ªå¸–å­")
        return posts
        
    except requests.exceptions.RequestException as e:
        logger.error(f"    âœ— è¯·æ±‚ r/{subreddit} RSSå¤±è´¥: {e}")
        return []
    except ET.ParseError as e:
        logger.error(f"    âœ— è§£æ r/{subreddit} XMLå¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"    âœ— å¤„ç† r/{subreddit} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def fetch_all_reddit_posts():
    """ä»æ‰€æœ‰é…ç½®çš„subredditè·å–å¸–å­"""
    logger.info("=== å¼€å§‹çˆ¬å–æ‰€æœ‰Subreddit ===")
    all_posts = []
    
    for subreddit in SUBREDDITS:
        posts = fetch_reddit_posts_from_subreddit(subreddit)
        all_posts.extend(posts)
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    logger.info(f"=== æ€»è®¡è·å– {len(all_posts)} ä¸ªå¸–å­ ===")
    return all_posts

# --- AIåˆ†æå‡½æ•° (ä¼˜åŒ–ä¸­æ–‡è¾“å‡º + è¯„è®ºé›†æˆ) ---
async def fetch_post_comments(post_id):
    """ä»æ•°æ®åº“è·å–å¸–å­çš„é«˜è´¨é‡è¯„è®º"""
    try:
        conn = await asyncpg.connect(NEON_DB_URL)
        # è·å–è¯„åˆ†æœ€é«˜çš„å‰5æ¡è¯„è®º
        comments = await conn.fetch("""
            SELECT author, body, score 
            FROM reddit_comments 
            WHERE post_id = $1 
            ORDER BY score DESC 
            LIMIT 5
        """, post_id)
        await conn.close()
        return [dict(c) for c in comments]
    except:
        return []

def analyze_single_post_with_deepseek(post, retry_count=0, comments=None):
    """ä½¿ç”¨DeepSeekåˆ†æRedditå¸–å­å¹¶è¾“å‡ºå®Œæ•´ä¸­æ–‡ï¼ˆåŒ…å«è¯„è®ºç²¾åï¼‰"""
    excerpt = post.get('content', '')[:1000]
    if not excerpt.strip():
        excerpt = "ï¼ˆæ— è¯¦ç»†å†…å®¹ï¼‰"
    
    # æ„å»ºè¯„è®ºæ‘˜è¦
    comment_section = ""
    if comments and len(comments) > 0:
        comment_section = "\n\n**ç¤¾åŒºè®¨è®ºç²¾å**ï¼ˆé«˜èµè¯„è®ºï¼‰ï¼š\n"
        for i, comment in enumerate(comments[:3], 1):
            comment_body = comment['body'][:200]
            comment_section += f"{i}. [{comment['author']}] (ğŸ‘{comment['score']}): {comment_body}...\n"
    else:
        num_comments = post.get('num_comments', 0)
        comment_section = f"\n- è¯„è®ºæ•°é‡: {num_comments}æ¡ï¼ˆæš‚æ— è¯„è®ºå†…å®¹ï¼‰" if num_comments > 0 else ""

    prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„RedditæŠ€æœ¯å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·æ·±åº¦åˆ†æä»¥ä¸‹å¸–å­ï¼ˆåŒ…å«ç¤¾åŒºè®¨è®ºï¼‰ï¼Œç”Ÿæˆä¸€ä»½**ä¸“ä¸šæŠ€æœ¯åˆ†ææŠ¥å‘Š**ã€‚

**åˆ†æè§’è‰²å®šä½**ï¼š
- æŠ€æœ¯æ·±åº¦ï¼šæ·±å…¥å‰–ææŠ€æœ¯åŸç†ã€æ¶æ„è®¾è®¡ã€å·¥ç¨‹å®è·µ
- ä¸“ä¸šè§†è§’ï¼šå…³æ³¨æœ€ä½³å®è·µã€æ€§èƒ½ä¼˜åŒ–ã€åˆ›æ–°æ–¹å‘
- å®æˆ˜ä»·å€¼ï¼šæä¾›å¯è½åœ°çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
- ç¤¾åŒºæ´å¯Ÿï¼šæ•´åˆè¯„è®ºåŒºçš„æŠ€æœ¯è®¨è®ºå’Œä¸åŒè§‚ç‚¹

**é‡è¦è¦æ±‚**ï¼š
1. æ ‡é¢˜ç¿»è¯‘æˆä¸“ä¸šã€å‡†ç¡®çš„ä¸­æ–‡
2. æ‰€æœ‰åˆ†æå†…å®¹å¿…é¡»æ˜¯ä¸­æ–‡
3. æŠ€æœ¯ç±»å¸–å­ï¼šæ·±å…¥åˆ†æåŸç†ã€æ¶æ„ã€ä»£ç è¦ç‚¹ã€æ€§èƒ½ä¼˜åŒ–
4. æ¸¸æˆå¼€å‘ç±»ï¼šå…³æ³¨å¼•æ“ã€å·¥å…·ã€ç®—æ³•ã€æ¸²æŸ“æŠ€æœ¯
5. è¯„è®ºæ•´åˆï¼šæç‚¼ç¤¾åŒºè®¨è®ºçš„æŠ€æœ¯è¦ç‚¹ã€äº‰è®®å’Œå…±è¯†
6. è¿”å›æ ¼å¼å¿…é¡»æ˜¯çº¯JSONï¼Œä¸è¦åŒ…å«```json```æ ‡è®°

**åŸå§‹å¸–å­ä¿¡æ¯**ï¼š
- æ ‡é¢˜ï¼ˆè‹±æ–‡ï¼‰: {post['title']}
- æ¥æºæ¿å—: r/{post['subreddit']}
- å†…å®¹æ‘˜è¦: {excerpt}{comment_section}

**è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡º**ï¼š
{{
  "title_cn": "å°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼ˆä¿æŒåŸæ„ï¼Œç®€æ´æ˜äº†ï¼‰",
  "core_issue": "ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªå¸–å­çš„æ ¸å¿ƒè®®é¢˜ï¼ˆä¸­æ–‡ï¼‰",
  "key_info": [
    "å…³é”®ä¿¡æ¯ç‚¹1ï¼ˆä¸­æ–‡ï¼‰",
    "å…³é”®ä¿¡æ¯ç‚¹2ï¼ˆä¸­æ–‡ï¼‰",
    "å…³é”®ä¿¡æ¯ç‚¹3ï¼ˆä¸­æ–‡ï¼‰"
  ],
  "post_type": "ä»[æŠ€æœ¯è®¨è®º, æ–°é—»åˆ†äº«, é—®é¢˜æ±‚åŠ©, è§‚ç‚¹è®¨è®º, èµ„æºåˆ†äº«, æ•™ç¨‹æŒ‡å—, é¡¹ç›®å±•ç¤º, å…¶ä»–]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "detailed_analysis": "ç”Ÿæˆ600-1200å­—çš„**ä¸“ä¸šæ·±åº¦æŠ€æœ¯åˆ†æ**ï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼ˆç”¨markdownæ ¼å¼ï¼‰ï¼š\\n\\n## ğŸ“‹ æŠ€æœ¯èƒŒæ™¯\\né˜è¿°æŠ€æœ¯èƒŒæ™¯ã€é—®é¢˜èµ·æºã€è¡Œä¸šç°çŠ¶ã€‚è¯´æ˜ä¸ºä»€ä¹ˆè¿™ä¸ªè¯é¢˜é‡è¦ã€è§£å†³äº†ä»€ä¹ˆç—›ç‚¹ã€‚\\n\\n## ğŸ¯ æ ¸å¿ƒæŠ€æœ¯æ–¹æ¡ˆ\\næ·±å…¥å‰–ææŠ€æœ¯åŸç†ã€æ¶æ„è®¾è®¡ã€å®ç°æ€è·¯ã€‚åŒ…æ‹¬ï¼š\\n- æŠ€æœ¯é€‰å‹å’Œç†ç”±\\n- ç³»ç»Ÿæ¶æ„æˆ–ç®—æ³•è®¾è®¡\\n- å…³é”®ä»£ç é€»è¾‘æˆ–APIä½¿ç”¨\\n- æ•°æ®æµå’ŒçŠ¶æ€ç®¡ç†\\n\\n## ğŸ’¡ å·¥ç¨‹å®è·µç»†èŠ‚\\n**æŠ€æœ¯è¦ç‚¹**ï¼š\\n- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼ˆæ—¶é—´/ç©ºé—´å¤æ‚åº¦ã€ç¼“å­˜ã€å¹¶å‘ï¼‰\\n- æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼\\n- å¸¸è§é™·é˜±å’Œè§£å†³æ–¹æ¡ˆ\\n- å…¼å®¹æ€§å’Œè¾¹ç•Œæƒ…å†µå¤„ç†\\n\\n**å·¥å…·é“¾**ï¼š\\n- æ¨èçš„æ¡†æ¶ã€åº“ã€å·¥å…·\\n- å¼€å‘ç¯å¢ƒé…ç½®\\n- æµ‹è¯•å’Œè°ƒè¯•æ–¹æ³•\\n\\n## ğŸ’¬ ç¤¾åŒºè®¨è®ºä¸äº‰è®®\\nåŸºäºè¯„è®ºåŒºè®¨è®ºï¼Œåˆ†æï¼š\\n- æŠ€æœ¯æ–¹æ¡ˆçš„ä¸åŒè§‚ç‚¹å’Œæ›¿ä»£æ–¹æ¡ˆ\\n- ç¤¾åŒºå…±è¯†å’Œäº‰è®®ç‚¹\\n- å®é™…åº”ç”¨ä¸­é‡åˆ°çš„é—®é¢˜\\n- ç»éªŒåˆ†äº«å’Œå»ºè®®\\n\\n## ğŸ”§ å®æˆ˜åº”ç”¨æŒ‡å—\\n**é€‚ç”¨åœºæ™¯**ï¼šä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨è¿™ä¸ªæ–¹æ¡ˆ\\n**å®æ–½æ­¥éª¤**ï¼šå…·ä½“çš„å®ç°è·¯å¾„å’Œæ³¨æ„äº‹é¡¹\\n**èµ„æºæ¨è**ï¼šå®˜æ–¹æ–‡æ¡£ã€æ•™ç¨‹ã€å¼€æºé¡¹ç›®é“¾æ¥\\n**é¿å‘æŒ‡å—**ï¼šå¸¸è§é”™è¯¯ã€æ€§èƒ½ç“¶é¢ˆã€å®‰å…¨éšæ‚£\\n\\n## ğŸš€ æŠ€æœ¯è¶‹åŠ¿ä¸å±•æœ›\\n- è¯¥æŠ€æœ¯åœ¨è¡Œä¸šä¸­çš„å‘å±•è¶‹åŠ¿\\n- ä¸å…¶ä»–æŠ€æœ¯çš„å¯¹æ¯”å’Œæ¼”è¿›æ–¹å‘\\n- æœªæ¥å¯èƒ½çš„åº”ç”¨åœºæ™¯\\n- å¯¹å¼€å‘è€…çš„å»ºè®®å’Œå­¦ä¹ è·¯å¾„"
}}
"""
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 2000,
        "temperature": 0.5
    }
    
    proxies = {}
    if not IS_GITHUB_ACTIONS and os.getenv("PROXY_URL"):
        proxy_url = os.getenv("PROXY_URL")
        proxies = {"http": proxy_url, "https": proxy_url}
    
    try:
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies=proxies,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            # æ¸…ç†JSONå†…å®¹
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            try:
                analysis = json.loads(content)
                logger.info(f"  âœ“ å¸–å­åˆ†ææˆåŠŸ: {analysis.get('title_cn', 'N/A')[:30]}...")
                return analysis
            except json.JSONDecodeError:
                logger.error(f"  âœ— AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {content[:100]}...")
                return {
                    "title_cn": post['title'][:50] + "...",
                    "core_issue": "JSONè§£æå¤±è´¥",
                    "key_info": ["è§£æå¤±è´¥"],
                    "post_type": "å…¶ä»–",
                    "value_assessment": "ä½"
                }
        else:
            logger.error(f"  âœ— DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return {
                "title_cn": post['title'][:50] + "...",
                "core_issue": "APIè°ƒç”¨å¤±è´¥",
                "key_info": ["APIå¤±è´¥"],
                "post_type": "å…¶ä»–",
                "value_assessment": "ä½"
            }
            
    except Exception as e:
        logger.error(f"  âœ— AIåˆ†æå¤±è´¥: {e}")
        if retry_count < 2:
            logger.info(f"  âŸ³ é‡è¯•AIåˆ†æ ({retry_count + 1}/2)...")
            time.sleep(2)
            return analyze_single_post_with_deepseek(post, retry_count + 1)
        else:
            return {
                "title_cn": post['title'][:50] + "...",
                "core_issue": "åˆ†æå¤±è´¥",
                "key_info": ["åˆ†æå¤±è´¥"],
                "post_type": "å…¶ä»–",
                "value_assessment": "ä½"
            }

# --- æ•°æ®åº“æ“ä½œ ---
async def create_posts_table():
    """åˆ›å»ºredditå¸–å­è¡¨"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
        return False

    conn = None
    try:
        # Neonéœ€è¦SSLè¿æ¥
        logger.info(f"å°è¯•è¿æ¥æ•°æ®åº“...")
        
        # æµ‹è¯•DNSè§£æ
        try:
            from urllib.parse import urlparse
            import socket
            parsed = urlparse(NEON_DB_URL)
            host = parsed.hostname
            logger.info(f"  æ­¥éª¤1: å°è¯•DNSè§£æä¸»æœº {host}...")
            ip = socket.gethostbyname(host)
            logger.info(f"  âœ“ DNSè§£ææˆåŠŸ: {host} -> {ip}")
        except socket.gaierror as e:
            logger.error(f"  âœ— DNSè§£æå¤±è´¥: {e}")
            logger.error(f"  å¯èƒ½åŸå› : 1)ä¸»æœºåé”™è¯¯ 2)ç½‘ç»œé—®é¢˜ 3)DNSæœåŠ¡å™¨é—®é¢˜")
            raise
        except Exception as e:
            logger.error(f"  âœ— DNSæµ‹è¯•å¼‚å¸¸: {e}")
            raise
        
        logger.info(f"  æ­¥éª¤2: é…ç½®SSLä¸Šä¸‹æ–‡...")
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        logger.info(f"  âœ“ SSLé…ç½®å®Œæˆ")
        
        logger.info(f"  æ­¥éª¤3: å°è¯•å»ºç«‹æ•°æ®åº“è¿æ¥...")
        conn = await asyncpg.connect(
            dsn=NEON_DB_URL,
            ssl=ssl_context,
            timeout=60
        )
        logger.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS reddit_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                title_cn TEXT,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                subreddit TEXT,
                score INTEGER,
                num_comments INTEGER,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("âœ“ æ•°æ®åº“è¡¨ 'reddit_posts' æ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âœ— åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
        return False

    conn = None
    try:
        # Neonéœ€è¦SSLè¿æ¥
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        conn = await asyncpg.connect(
            dsn=NEON_DB_URL,
            ssl=ssl_context,
            timeout=60
        )
        logger.info("å¼€å§‹æ’å…¥æ•°æ®åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                title_cn = post.get('analysis', {}).get('title_cn', title)
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []), ensure_ascii=False)
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                detailed_analysis = analysis.get('detailed_analysis')
                subreddit = post.get('subreddit')
                score = post.get('score', 0)
                num_comments = post.get('num_comments', 0)

                await conn.execute("""
                    INSERT INTO reddit_posts (id, title, title_cn, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, subreddit, score, num_comments)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                    ON CONFLICT (id) DO UPDATE SET
                        title = EXCLUDED.title,
                        title_cn = EXCLUDED.title_cn,
                        url = EXCLUDED.url,
                        core_issue = EXCLUDED.core_issue,
                        key_info = EXCLUDED.key_info,
                        post_type = EXCLUDED.post_type,
                        value_assessment = EXCLUDED.value_assessment,
                        detailed_analysis = EXCLUDED.detailed_analysis,
                        subreddit = EXCLUDED.subreddit,
                        score = EXCLUDED.score,
                        num_comments = EXCLUDED.num_comments,
                        timestamp = CURRENT_TIMESTAMP;
                """, post_id, title, title_cn, url, core_issue, key_info, post_type, value_assessment, detailed_analysis, subreddit, score, num_comments)
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"  âœ— æ’å…¥å¸–å­ {post.get('id', 'N/A')} å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ“ æˆåŠŸæ’å…¥/æ›´æ–° {success_count}/{len(posts_data)} æ¡æ•°æ®")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âœ— æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# --- AIæ•´ä½“æ´å¯ŸæŠ¥å‘Š ---
async def generate_ai_summary_report(posts_data):
    """ç”Ÿæˆæ•´ä½“åˆ†ææŠ¥å‘Šï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œæ”¯æŒè¯„è®ºæŸ¥è¯¢ï¼‰"""
    processed_posts = []
    post_summaries = []

    logger.info("=== å¼€å§‹AIåˆ†æï¼ˆåŒ…å«è¯„è®ºï¼‰===")

    for i, post in enumerate(posts_data):
        logger.info(f"[{i+1}/{len(posts_data)}] åˆ†æ: r/{post['subreddit']} - {post['title'][:40]}...")
        
        # å°è¯•è·å–è¯„è®º
        comments = await fetch_post_comments(post.get('id'))
        if comments:
            logger.info(f"  â†’ è·å–åˆ° {len(comments)} æ¡é«˜è´¨é‡è¯„è®º")
        
        analysis = analyze_single_post_with_deepseek(post, comments=comments)
        
        if analysis:
            processed_posts.append({
                "id": post.get('id'),
                "title": post.get('title'),
                "title_cn": analysis.get('title_cn', post.get('title')),
                "link": post.get('link'),
                "author": post.get('author'),
                "content": post.get('content'),
                "analysis": analysis,
                "subreddit": post.get('subreddit'),
                "score": post.get('score', 0),
                "num_comments": post.get('num_comments', 0)
            })
            
            post_summaries.append({
                "subreddit": post.get('subreddit'),
                "title_cn": analysis.get('title_cn', 'N/A'),
                "core_issue": analysis.get('core_issue', 'N/A'),
                "post_type": analysis.get('post_type', 'N/A'),
                "value_assessment": analysis.get('value_assessment')
            })
            
            time.sleep(3)  # APIé€Ÿç‡é™åˆ¶
        else:
            logger.error(f"  âœ— å¸–å­åˆ†æå¤±è´¥")

    logger.info("=== AIåˆ†æå®Œæˆ ===")
    
    # ç”Ÿæˆæ•´ä½“æ´å¯Ÿ
    logger.info("=== ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Š ===")
    
    try:
        all_summaries_text = json.dumps(post_summaries, ensure_ascii=False, indent=2)

        overall_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±çš„Redditå†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©ä»å¤šä¸ªæŠ€æœ¯/æ¸¸æˆå¼€å‘ç›¸å…³subreddité‡‡é›†çš„çƒ­é—¨å¸–å­æ‘˜è¦ã€‚

è¯·ç”Ÿæˆä¸€ä»½ç®€æ´çš„ä¸­æ–‡"ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ"æŠ¥å‘Šï¼Œä¸¥æ ¼æŒ‰JSONæ ¼å¼è¿”å›ï¼ˆä¸è¦åŒ…å«```json```æ ‡è®°ï¼‰ã€‚

**ä»Šæ—¥å¸–å­æ‘˜è¦**ï¼š
{all_summaries_text}

**è¾“å‡ºæ ¼å¼**ï¼š
{{
  "overview": "ç”¨1-2å¥è¯æ€»ç»“ä»Šå¤©è¿™äº›æ¿å—çš„æ•´ä½“è®¨è®ºæ°›å›´å’Œç„¦ç‚¹ï¼ˆä¸­æ–‡ï¼‰",
  "highlights": {{
    "tech_news": ["æç‚¼1-3æ¡æœ€é‡è¦çš„æŠ€æœ¯æ–°é—»æˆ–è¡Œä¸šåŠ¨æ€"],
    "dev_insights": ["æç‚¼1-3æ¡æ¸¸æˆå¼€å‘ç›¸å…³çš„æœ‰ä»·å€¼è§è§£æˆ–èµ„æº"],
    "hot_topics": ["æç‚¼1-3ä¸ªå¼•å‘å¹¿æ³›è®¨è®ºçš„çƒ­é—¨è¯é¢˜"]
  }},
  "conclusion": "ç”¨ä¸€å¥è¯åšä¸ªæ€»ç»“ï¼ˆå¯ä»¥å¹½é»˜æˆ–ä¸“ä¸šï¼‰"
}}
"""

        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": overall_prompt}],
            "max_tokens": 800,
            "temperature": 0.7
        }
        
        proxies = {}
        if not IS_GITHUB_ACTIONS and os.getenv("PROXY_URL"):
            proxy_url = os.getenv("PROXY_URL")
            proxies = {"http": proxy_url, "https": proxy_url}
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies=proxies,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            summary_analysis = json.loads(content)
            logger.info("âœ“ æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        else:
            logger.error(f"âœ— æ•´ä½“æ´å¯Ÿç”Ÿæˆå¤±è´¥: {response.status_code}")
            summary_analysis = {
                "overview": "ä»Šæ—¥æœªèƒ½ç”ŸæˆAIæ´å¯ŸæŠ¥å‘Šã€‚",
                "highlights": {"tech_news": [], "dev_insights": [], "hot_topics": []},
                "conclusion": "ç³»ç»Ÿç»´æŠ¤ä¸­ã€‚"
            }
            
    except Exception as e:
        logger.error(f"âœ— ç”Ÿæˆæ•´ä½“æ´å¯Ÿæ—¶å‡ºé”™: {e}")
        summary_analysis = {
            "overview": "ä»Šæ—¥å†…å®¹åˆ†æé‡åˆ°æŠ€æœ¯é—®é¢˜ã€‚",
            "highlights": {"tech_news": [], "dev_insights": [], "hot_topics": []},
            "conclusion": "ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œæ˜æ—¥æ¢å¤ã€‚"
        }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- æŠ¥å‘Šç”Ÿæˆ ---
def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Š"""
    try:
        os.makedirs('data', exist_ok=True)
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"data/reddit_multi_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"RedditæŠ€æœ¯+æ¸¸æˆå¼€å‘æ¯æ—¥æŠ¥å‘Š ({today_str})",
                "subreddits": SUBREDDITS,
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "subreddit": post.get('subreddit'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "title_cn": post.get('title_cn', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ JSONæŠ¥å‘Šç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âœ— JSONæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None

def generate_markdown_report(report_data, posts_count):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    try:
        os.makedirs('reports', exist_ok=True)
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"reports/Reddit_Multi_Daily_Report_{today_str}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# RedditæŠ€æœ¯+æ¸¸æˆå¼€å‘æ¯æ—¥æŠ¥å‘Š ({today_str})\n\n")
            f.write(f"> æ¥æºæ¿å—: {', '.join([f'r/{s}' for s in SUBREDDITS])}\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ¯ ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ\n\n")
            
            summary = report_data.get('summary_analysis', {})
            if summary:
                f.write(f"**æ•´ä½“æ¦‚å†µ**: {summary.get('overview', 'æš‚æ— ')}\n\n")
                
                highlights = summary.get('highlights', {})
                if highlights.get('tech_news'):
                    f.write("**ğŸ“° æŠ€æœ¯æ–°é—»**:\n")
                    for item in highlights['tech_news']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('dev_insights'):
                    f.write("**ğŸ® å¼€å‘è§è§£**:\n")
                    for item in highlights['dev_insights']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('hot_topics'):
                    f.write("**ğŸ”¥ çƒ­é—¨è¯é¢˜**:\n")
                    for item in highlights['hot_topics']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                f.write(f"**ğŸ“ å°ç»“**: {summary.get('conclusion', 'æš‚æ— ')}\n\n")
            
            f.write("---\n\n")
            f.write(f"## ğŸ“‹ å¸–å­åˆ—è¡¨ (å…± {posts_count} ç¯‡)\n\n")
            
            # æŒ‰subredditåˆ†ç»„
            posts = report_data.get('processed_posts', [])
            if posts:
                current_sub = None
                for i, post in enumerate(posts, 1):
                    subreddit = post.get('subreddit')
                    if subreddit != current_sub:
                        current_sub = subreddit
                        f.write(f"\n### r/{subreddit}\n\n")
                    
                    title_cn = post.get('title_cn', post.get('title', 'æ— æ ‡é¢˜'))
                    link = post.get('link', '#')
                    analysis = post.get('analysis', {})
                    
                    f.write(f"{i}. **{title_cn}**\n")
                    f.write(f"   - åŸæ ‡é¢˜: {post.get('title', 'N/A')}\n")
                    f.write(f"   - é“¾æ¥: {link}\n")
                    f.write(f"   - æ ¸å¿ƒè®®é¢˜: {analysis.get('core_issue', 'N/A')}\n")
                    f.write(f"   - ç±»å‹: {analysis.get('post_type', 'N/A')} | ä»·å€¼: {analysis.get('value_assessment', 'N/A')}\n\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­ã€‚\n")

        logger.info(f"âœ“ MarkdownæŠ¥å‘Šç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âœ— MarkdownæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None

# --- ä¸»å‡½æ•° ---
async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸš€ Redditå¤šæ¿å—çˆ¬è™«ä»»åŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“‹ ç›®æ ‡æ¿å—: {', '.join(SUBREDDITS)}")
    logger.info(f"ğŸŒ è¿è¡Œç¯å¢ƒ: {'GitHub Actions' if IS_GITHUB_ACTIONS else 'æœ¬åœ°ç¯å¢ƒ'}")
    logger.info("=" * 60)
    
    try:
        if not DEEPSEEK_API_KEY:
            logger.error("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return False
            
        if not NEON_DB_URL:
            logger.error("âŒ æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
            return False
        
        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            logger.error("âŒ æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        # è·å–æ‰€æœ‰å¸–å­
        posts_data = fetch_all_reddit_posts()
        
        if not posts_data:
            logger.error("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­")
            return False
        
        logger.info(f"âœ“ å…±è·å– {len(posts_data)} ä¸ªå¸–å­")
        
        # ç”ŸæˆAIæŠ¥å‘Šï¼ˆåŒ…å«è¯„è®ºåˆ†æï¼‰
        report_data = await generate_ai_summary_report(posts_data)
        
        # æ’å…¥æ•°æ®åº“
        if report_data.get('processed_posts'):
            await insert_posts_into_db(report_data['processed_posts'])
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        json_file = generate_json_report(report_data, len(posts_data))
        md_file = generate_markdown_report(report_data, len(posts_data))
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 60)
        logger.info("âœ… ä»»åŠ¡å®Œæˆ")
        logger.info(f"â±ï¸  ç”¨æ—¶: {duration:.2f} ç§’")
        logger.info(f"ğŸ“Š å¤„ç†: {len(posts_data)} ä¸ªå¸–å­")
        logger.info(f"ğŸ“„ æŠ¥å‘Š: {json_file}")
        logger.info(f"ğŸ“„ æŠ¥å‘Š: {md_file}")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)

