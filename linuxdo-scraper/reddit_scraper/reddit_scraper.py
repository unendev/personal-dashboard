# reddit_scraper.py - Reddit RSSçˆ¬è™«ï¼ŒåŸºäºlinuxdo-scraperæ¶æ„
import asyncio
import os
import re
import json
import logging
from datetime import datetime
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reddit_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- é…ç½® ---
load_dotenv()
SUBREDDIT = "technology"
RSS_URL = f"https://www.reddit.com/r/{SUBREDDIT}/.rss?sort=hot"
POST_COUNT_LIMIT = 10
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
NEON_DB_URL = os.getenv("DATABASE_URL")

proxy_for_all = "http://127.0.0.1:10809"
os.environ['HTTP_PROXY'] = proxy_for_all
os.environ['HTTPS_PROXY'] = proxy_for_all

# --- Reddit çˆ¬è™«å‡½æ•° ---
def fetch_reddit_posts():
    """ä»Reddit RSSè·å–å¸–å­æ•°æ®"""
    logger.info(f"å¼€å§‹ä» r/{SUBREDDIT} çš„ RSS æºçˆ¬å–çƒ­é—¨å¸–å­...")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        response = requests.get(RSS_URL, headers=headers, timeout=20, proxies={"http": proxy_for_all, "https": proxy_for_all})
        response.raise_for_status()
        root = ET.fromstring(response.content)
        all_posts = []
        
        for i, entry in enumerate(root.findall('{http://www.w3.org/2005/Atom}entry')):
            if i >= POST_COUNT_LIMIT:
                break
                
            title_tag = entry.find('{http://www.w3.org/2005/Atom}title')
            link_tag = entry.find('{http://www.w3.org/2005/Atom}link')
            content_tag = entry.find('{http://www.w3.org/2005/Atom}content')
            author_tag = entry.find('{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name')
            
            if title_tag is not None and link_tag is not None:
                title = title_tag.text
                link = link_tag.get('href')
                author = author_tag.text if author_tag is not None else "N/A"
                
                # æ¸…ç†å†…å®¹
                content_html = content_tag.text if content_tag is not None else ""
                clean_content = re.sub(r'<.*?>', ' ', content_html)
                clean_content = re.sub(r'\[link\]|\[comments\]', '', clean_content).strip()
                
                # æå–Redditå¸–å­ID (ä»URLä¸­æå–commentsåçš„éƒ¨åˆ†)
                post_id = "reddit_unknown"
                if '/comments/' in link:
                    # æ ¼å¼: /r/technology/comments/1nx583c/title/
                    parts = link.split('/comments/')
                    if len(parts) > 1:
                        id_part = parts[1].split('/')[0]
                        post_id = f"reddit_{id_part}"
                
                all_posts.append({
                    "id": post_id,
                    "title": title,
                    "link": link,
                    "author": author,
                    "content": clean_content[:500] + '...' if len(clean_content) > 500 else clean_content,
                    "subreddit": SUBREDDIT,
                    "score": 0,
                    "num_comments": 0
                })
                logger.info(f"  è§£æå¸–å­ {i+1}: {title[:60]}...")
        
        logger.info(f"æˆåŠŸè§£æåˆ° {len(all_posts)} ä¸ªå¸–å­ã€‚")
        return all_posts
        
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ Reddit RSS æºå¤±è´¥: {e}")
        return []
    except ET.ParseError as e:
        logger.error(f"XML è§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"å¤„ç†å¸–å­æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

# --- AIåˆ†æå‡½æ•° (å¤ç”¨ linuxdo-scraper çš„æç¤ºè¯ç»“æ„) ---
def analyze_single_post_with_deepseek(post, retry_count=0):
    """ä½¿ç”¨DeepSeekåˆ†æå•ä¸ªRedditå¸–å­"""
    excerpt = post.get('content', '')[:1000]
    if not excerpt.strip():
        excerpt = post.get('title', '')[:100]

    prompt = f"""
ä½ æ˜¯ä¸€åç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹Redditå¸–å­å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœã€‚
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

**å¸–å­æ ‡é¢˜**: {post['title']}
**å¸–å­å†…å®¹**: {excerpt}

**è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
{{
  "core_issue": "è¿™é‡Œç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­çš„æ ¸å¿ƒè®®é¢˜",
  "key_info": [
    "å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹1",
    "å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹2"
  ],
  "post_type": "ä»[æŠ€æœ¯è®¨è®º, æ–°é—»åˆ†äº«, é—®é¢˜æ±‚åŠ©, è§‚ç‚¹è®¨è®º, èµ„æºåˆ†äº«, å¨±ä¹å†…å®¹, å…¶ä»–]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª"
}}
"""
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 400,
        "temperature": 0.3
    }
    
    try:
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies={"http": proxy_for_all, "https": proxy_for_all},
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            # æ¸…ç†JSONå†…å®¹
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            try:
                analysis = json.loads(content)
                logger.info(f"å¸–å­ '{post['title'][:30]}...' AIåˆ†ææˆåŠŸ")
                return analysis
            except json.JSONDecodeError:
                logger.error(f"AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {content[:100]}...")
                return {"core_issue": "è§£æå¤±è´¥", "key_info": ["è§£æå¤±è´¥"], "post_type": "å…¶ä»–", "value_assessment": "ä½"}
        
        else:
            logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {"core_issue": "APIå¤±è´¥", "key_info": ["APIå¤±è´¥"], "post_type": "å…¶ä»–", "value_assessment": "ä½"}
            
    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥: {e}")
        if retry_count < 2:
            logger.info(f"é‡è¯•AIåˆ†æ ({retry_count + 1}/2)...")
            time.sleep(1)
            return analyze_single_post_with_deepseek(post, retry_count + 1)
        else:
            return {"core_issue": "åˆ†æå¤±è´¥", "key_info": ["åˆ†æå¤±è´¥"], "post_type": "å…¶ä»–", "value_assessment": "ä½"}

# --- æ•°æ®åº“æ“ä½œ ---
async def create_posts_table():
    """åˆ›å»ºredditå¸–å­è¡¨"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL, command_timeout=30)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS reddit_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                subreddit TEXT,
                score INTEGER,
                num_comments INTEGER,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("æ•°æ®åº“è¡¨ 'reddit_posts' æ£€æŸ¥æˆ–åˆ›å»ºæˆåŠŸã€‚")
        return True
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL, command_timeout=30)
        logger.info("å¼€å§‹å°† Reddit å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []))
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                subreddit = post.get('subreddit', SUBREDDIT)
                score = post.get('score', 0)
                num_comments = post.get('num_comments', 0)

                await conn.execute("""
                    INSERT INTO reddit_posts (id, title, url, core_issue, key_info, post_type, value_assessment, subreddit, score, num_comments)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ON CONFLICT (id) DO UPDATE SET
                        title = EXCLUDED.title,
                        url = EXCLUDED.url,
                        core_issue = EXCLUDED.core_issue,
                        key_info = EXCLUDED.key_info,
                        post_type = EXCLUDED.post_type,
                        value_assessment = EXCLUDED.value_assessment,
                        subreddit = EXCLUDED.subreddit,
                        score = EXCLUDED.score,
                        num_comments = EXCLUDED.num_comments,
                        timestamp = CURRENT_TIMESTAMP;
                """, post_id, title, url, core_issue, key_info, post_type, value_assessment, subreddit, score, num_comments)
                
                logger.info(f"  - Reddit å¸–å­ '{title[:30]}...' (ID: {post_id}) å·²æ’å…¥/æ›´æ–°ã€‚")
                success_count += 1
                
            except Exception as e:
                logger.error(f"æ’å…¥ Reddit å¸–å­ {post.get('id', 'N/A')} å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸæ’å…¥/æ›´æ–°äº† {success_count}/{len(posts_data)} æ¡ Reddit å¸–å­æ•°æ®åˆ°æ•°æ®åº“ã€‚")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"æ’å…¥ Reddit å¸–å­æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# --- AIæ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆ ---
def generate_ai_summary_report(posts_data):

    processed_posts = []
    post_summaries = []

    logger.info("--- å¼€å§‹ç”Ÿæˆé€å¸–ç»“æ„åŒ–åˆ†æ ---")

    for i, post in enumerate(posts_data):
        logger.info(f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(posts_data)} ç¯‡: {post['title'][:50]}...")
        
        analysis = analyze_single_post_with_deepseek(post)
        
        if analysis:
            processed_posts.append({
                "id": post.get('id'),
                "title": post.get('title'),
                "link": post.get('link'),
                "author": post.get('author'),
                "content": post.get('content'),
                "analysis": analysis,
                "subreddit": post.get('subreddit'),
                "score": post.get('score', 0),
                "num_comments": post.get('num_comments', 0)
            })
            
            post_summaries.append({
                "title": post.get('title', 'N/A'),
                "core_issue": analysis.get('core_issue', 'N/A'),
                "post_type": analysis.get('post_type', 'N/A'),
                "value_assessment": analysis.get('value_assessment')

            })
            
            logger.info(f"å¸–å­ '{post['title'][:30]}...' AIåˆ†ææˆåŠŸ")
            logger.info("    ...ç­‰å¾…3ç§’ä»¥éµå®ˆAPIé€Ÿç‡é™åˆ¶...")
            time.sleep(3)
        else:
            logger.error(f"å¸–å­ '{post['title'][:30]}...' AIåˆ†æå¤±è´¥")

    logger.info("--- é€å¸–åˆ†æå…¨éƒ¨å®Œæˆ ---")

    logger.info("--- å¼€å§‹ç”Ÿæˆä»Šæ—¥æ•´ä½“æ´å¯ŸæŠ¥å‘Š (JSONæ ¼å¼) ---")
    
    try:
        all_summaries_text = json.dumps(post_summaries, ensure_ascii=False, indent=2)

        overall_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±çš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šå¤© Reddit çƒ­é—¨å¸–å­çš„JSONæ ¼å¼æ‘˜è¦åˆ—è¡¨ã€‚
è¯·æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½é«˜åº¦æµ“ç¼©çš„ä¸­æ–‡"ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ"æŠ¥å‘Šï¼Œå¹¶ä¸¥æ ¼ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ã€‚
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

**ä»Šæ—¥å¸–å­æ‘˜è¦åˆé›† (JSONæ ¼å¼):**
{all_summaries_text}
---
**è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
{{
  "overview": "ç”¨ä¸€ä¸¤å¥è¯æ€»ç»“ä»Šå¤© Reddit ç¤¾åŒºçš„æ•´ä½“æ°›å›´å’Œè®¨è®ºç„¦ç‚¹ã€‚",
  "highlights": {
    "tech_savvy": ["æç‚¼1-3æ¡æœ€ç¡¬æ ¸çš„æŠ€æœ¯å¹²è´§æˆ–ç§‘æŠ€èµ„è®¯"],
    "resources_deals": ["æç‚¼1-3æ¡æœ€å€¼å¾—å…³æ³¨çš„èµ„æºåˆ†äº«æˆ–ä¼˜æƒ ä¿¡æ¯"],
    "hot_topics": ["æç‚¼1-3ä¸ªå¼•å‘æœ€å¹¿æ³›è®¨è®ºçš„çƒ­é—¨è¯é¢˜æˆ–äº‰è®®ç‚¹"]
  },
  "conclusion": "ç”¨ä¸€å¥è¯å¯¹ä»Šå¤©çš„å†…å®¹åšä¸ªé£è¶£æˆ–ä¸“ä¸šçš„æ€»ç»“ã€‚"
}}
"""

        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": overall_prompt}],
            "max_tokens": 800,
            "temperature": 0.7
        }
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            proxies={"http": proxy_for_all, "https": proxy_for_all},
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            summary_analysis = json.loads(content)
            logger.info("--- æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæ¯• ---")
        else:
            logger.error(f"æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {response.status_code}")
            summary_analysis = {
                "overview": "ä»Šæ—¥æœªèƒ½ç”ŸæˆAIæ´å¯ŸæŠ¥å‘Šã€‚",
                "highlights": {"tech_savvy": [], "resources_deals": [], "hot_topics": []},
                "conclusion": "æ•°æ®è§£æä¸­ï¼Œæ•¬è¯·æœŸå¾…æ˜æ—¥æ›´ç²¾å½©çš„å†…å®¹ã€‚"
            }
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        summary_analysis = {
            "overview": "ä»Šæ—¥å†…å®¹åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜ã€‚",
            "highlights": {"tech_savvy": [], "resources_deals": [], "hot_topics": []},
            "conclusion": "ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œæ˜æ—¥æ¢å¤æ­£å¸¸æœåŠ¡ã€‚"
        }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- JSONæŠ¥å‘Šç”Ÿæˆ ---
def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Šæ–‡ä»¶"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"data/reddit_technology_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"Reddit Technology æ¯æ—¥çƒ­ç‚¹æŠ¥å‘Š ({today_str})",
                "source": "Reddit",
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- MarkdownæŠ¥å‘Šç”Ÿæˆ ---
def generate_markdown_report(report_data, posts_count):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"reports/Reddit_Technology_Daily_Report_{today_str}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# Reddit Technology æ¯æ—¥çƒ­ç‚¹æŠ¥å‘Š ({today_str})\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ¯ ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ\n\n")
            
            summary = report_data.get('summary_analysis', {})
            if summary:
                f.write(f"**æ•´ä½“æ¦‚å†µ**: {summary.get('overview', 'æš‚æ— åˆ†æ')}\n\n")
                
                highlights = summary.get('highlights', {})
                if highlights.get('tech_savvy'):
                    f.write("**ğŸ”§ æŠ€æœ¯è¦é—»**:\n")
                    for item in highlights['tech_savvy']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('hot_topics'):
                    f.write("**ğŸ”¥ çƒ­é—¨è¯é¢˜**:\n")
                    for item in highlights['hot_topics']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                f.write(f"**ğŸ“ å°ç»“**: {summary.get('conclusion', 'æš‚æ— æ€»ç»“')}\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ“‹ åŸå§‹å¸–å­åˆ—è¡¨ (å…± {} ç¯‡)\n\n".format(posts_count))
            
            posts = report_data.get('processed_posts', [])
            if posts:
                for i, post in enumerate(posts, 1):
                    title = post.get('title', 'æ— æ ‡é¢˜')
                    link = post.get('link', '#')
                    analysis = post.get('analysis', {})
                    
                    f.write(f"{i}. [{title}]({link})\n")
                    f.write(f"   - **è®®é¢˜**: {analysis.get('core_issue', 'N/A')}\n")
                    f.write(f"   - **ç±»å‹**: {analysis.get('post_type', 'N/A')}\n")
                    f.write(f"   - **ä»·å€¼**: {analysis.get('value_assessment', 'N/A')}\n\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­ã€‚\n")

        logger.info(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- ä¸»å‡½æ•° ---
async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("=== å¼€å§‹æ‰§è¡Œ Reddit çˆ¬è™«ä»»åŠ¡ ===")
    
    try:
        if not DEEPSEEK_API_KEY:
            logger.error("æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            return False
            
        if not NEON_DB_URL:
            logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
            return False
        
        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            logger.error("æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        # è·å–å¸–å­æ•°æ®
        posts_data = fetch_reddit_posts()
        
        if not posts_data:
            logger.error("æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­æ•°æ®")
            empty_data = {
                "summary_analysis": {
                    "error": "æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–RSSæºå¼‚å¸¸",
                    "overview": "ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­æ•°æ®ã€‚"
                }, 
                "processed_posts": []
            }
            generate_json_report(empty_data, 0)
            generate_markdown_report(empty_data, 0)
            return False
        
        logger.info(f"æˆåŠŸè·å–åˆ° {len(posts_data)} æ¡å¸–å­æ•°æ®")
        
        # ç”ŸæˆAIæŠ¥å‘Š
        report_data = generate_ai_summary_report(posts_data)
        
        # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“
        if report_data.get('processed_posts'):
            db_success = await insert_posts_into_db(report_data['processed_posts'])
            if not db_success:
                logger.error("æ•°æ®åº“æ’å…¥å¤±è´¥")
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        json_file = generate_json_report(report_data, len(posts_data))
        md_file = generate_markdown_report(report_data, len(posts_data))
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info(f"=== ä»»åŠ¡å®Œæˆ ===")
        logger.info(f"å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        logger.info(f"å¤„ç†å¸–å­: {len(posts_data)} æ¡")
        logger.info(f"ç”Ÿæˆæ–‡ä»¶: {json_file}, {md_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    if success:
        print("\nğŸ‰ Reddit çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
    else:
        print("\nğŸ’¥ Reddit çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼")