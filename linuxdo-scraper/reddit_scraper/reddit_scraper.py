# reddit_scraper.py - Reddit RSSçˆ¬è™«ï¼ŒåŸºäºlinuxdo-scraperæ¶æ„
import asyncio
import os
import re
import json
import logging
from datetime import datetime
import requests
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import time
import asyncpg
import praw
import google.generativeai as genai

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/reddit_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- é…ç½® ---
load_dotenv()
SUBREDDIT = "technology"
RSS_URL = f"https://www.reddit.com/r/{SUBREDDIT}/.rss?sort=hot"
POST_COUNT_LIMIT = 10
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NEON_DB_URL = os.getenv("DATABASE_URL")
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT")

# --- çˆ¬è™«è¡Œä¸ºé…ç½® ---
COMMENT_COUNT_LIMIT = 5       # æ¯ä¸ªå¸–å­è·å–çš„é¡¶çº§è¯„è®ºæ•°
AI_RETRY_LIMIT = 2            # AIåˆ†æå¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
AI_RETRY_DELAY = 2            # AIåˆ†æé‡è¯•å‰çš„ç­‰å¾…ç§’æ•°
POST_ANALYSIS_DELAY = 3       # æ¯ä¸ªå¸–å­åˆ†æåçš„ç­‰å¾…ç§’æ•°ï¼ˆé˜²æ­¢APIé€Ÿç‡è¶…é™ï¼‰

proxy_for_all = "http://127.0.0.1:10809"
os.environ['HTTP_PROXY'] = proxy_for_all
os.environ['HTTPS_PROXY'] = proxy_for_all

# --- Reddit çˆ¬è™«å‡½æ•° ---
def fetch_reddit_posts(reddit_client):
    """ä»Reddit RSSè·å–å¸–å­æ•°æ®å¹¶æŠ“å–è¯„è®º"""
    logger.info(f"å¼€å§‹ä» r/{SUBREDDIT} çš„ RSS æºçˆ¬å–çƒ­é—¨å¸–å­...")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        response = requests.get(RSS_URL, headers=headers, timeout=20, proxies={"http": proxy_for_all, "https": proxy_for_all})
        response.raise_for_status()
        root = ET.fromstring(response.content)
        all_posts = []
        
        for i, entry in enumerate(root.findall('{http://www.w3.org/2005/Atom}entry')):
            if i >= POST_COUNT_LIMIT:
                break
                
            title_tag = entry.find('{http://www.w3.org/2005/Atom}title')
            link_tag = entry.find('{http://www.w3.org/2005/Atom}link')
            content_tag = entry.find('{http://www.w3.org/2005/Atom}content')
            author_tag = entry.find('{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name')
            
            if title_tag is not None and link_tag is not None:
                title = title_tag.text
                link = link_tag.get('href')
                author = author_tag.text if author_tag is not None else "N/A"
                
                content_html = content_tag.text if content_tag is not None else ""
                clean_content = re.sub(r'<.*?>', ' ', content_html)
                clean_content = re.sub(r'\[link\]|\[comments\]', '', clean_content).strip()
                
                post_id_match = re.search(r'/comments/([a-zA-Z0-9]+)/',
                                        link)
                post_id = f"reddit_{post_id_match.group(1)}" if post_id_match else None
                
                if not post_id:
                    logger.warning(f"æ— æ³•ä»URL {link} ä¸­æå– post IDï¼Œè·³è¿‡è¯„è®ºè·å–ã€‚")
                    comments_content = "æ²¡æœ‰è·å–åˆ°è¯„è®ºã€‚"
                else:
                    try:
                        logger.info(f"æ­£åœ¨ä¸ºå¸–å­ '{title[:30]}...' è·å–è¯„è®º...")
                        submission = reddit_client.submission(id=post_id.replace("reddit_", ""))
                        submission.comments.replace_more(limit=0) # å±•å¼€ 'more comments'
                        comments = []
                        for top_level_comment in submission.comments[:COMMENT_COUNT_LIMIT]: # è·å–æœ€å¤š5æ¡é¡¶çº§è¯„è®º
                            comments.append(f"- {top_level_comment.body}")
                        comments_content = "\n".join(comments)
                        logger.info(f"æˆåŠŸè·å– {len(comments)} æ¡è¯„è®ºã€‚")
                    except Exception as e:
                        logger.error(f"ä½¿ç”¨PRAWè·å–è¯„è®ºå¤±è´¥ (post ID: {post_id}): {e}")
                        comments_content = "è·å–è¯„è®ºæ—¶å‘ç”Ÿé”™è¯¯ã€‚"

                full_content = f"""**å¸–å­å†…å®¹:**\n{clean_content}\n\n**çƒ­é—¨è¯„è®º:**\n{comments_content}"""

                all_posts.append({
                    "id": post_id if post_id else f"reddit_unknown_{i}",
                    "title": title,
                    "link": link,
                    "author": author,
                    "content": clean_content[:500] + '...' if len(clean_content) > 500 else clean_content,
                    "full_content": full_content, # æ·»åŠ å®Œæ•´å†…å®¹ç”¨äºAIåˆ†æ
                    "subreddit": SUBREDDIT,
                    "score": 0,
                    "num_comments": 0
                })
                logger.info(f"  è§£æå¸–å­ {i+1}: {title[:60]}...")
        
        logger.info(f"æˆåŠŸè§£æåˆ° {len(all_posts)} ä¸ªå¸–å­ã€‚")
        return all_posts
        
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ Reddit RSS æºå¤±è´¥: {e}")
        return []
    except ET.ParseError as e:
        logger.error(f"XML è§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"å¤„ç†å¸–å­æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

# --- AIåˆ†æå‡½æ•° (ä½¿ç”¨ Gemini) ---
async def analyze_single_post_with_gemini(post, model, retry_count=0):
    """ä½¿ç”¨Geminiåˆ†æå•ä¸ªRedditå¸–å­åŠå…¶è¯„è®º (å¼‚æ­¥)"""
    content_to_analyze = post.get('full_content', '')
    if not content_to_analyze.strip():
        content_to_analyze = post.get('title', '')

    prompt = f"""
ä½ æ˜¯ä¸€åç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹Redditå¸–å­å†…å®¹åŠå…¶çƒ­é—¨è¯„è®ºï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœã€‚
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

**å¸–å­ä¸è¯„è®ºå†…å®¹**: 
{content_to_analyze}

**è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
{{
  "core_issue": "è¿™é‡Œç”¨ä¸€å¥è¯æ¦‚æ‹¬å¸–å­ä¸è¯„è®ºçš„æ ¸å¿ƒè®®é¢˜",
  "key_info": [
    "å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹1",
    "å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹2"
  ],
  "post_type": "ä»[æŠ€æœ¯è®¨è®º, æ–°é—»åˆ†äº«, é—®é¢˜æ±‚åŠ©, è§‚ç‚¹è®¨è®º, èµ„æºåˆ†äº«, å¨±ä¹å†…å®¹, å…¶ä»–]ä¸­é€‰æ‹©ä¸€ä¸ª",
  "value_assessment": "ä»[é«˜, ä¸­, ä½]ä¸­é€‰æ‹©ä¸€ä¸ª"
}}
"""
    
    try:
        # ä½¿ç”¨ asyncio.to_thread åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„SDKè°ƒç”¨
        response = await asyncio.to_thread(model.generate_content, prompt)
        content = response.text
        
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        content = content.strip()
        
        try:
            analysis = json.loads(content)
            logger.info(f"å¸–å­ '{post['title'][:30]}...' Geminiåˆ†ææˆåŠŸ")
            return analysis
        except json.JSONDecodeError:
            logger.error(f"Geminiè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {content[:100]}...")
            return {"core_issue": "è§£æå¤±è´¥", "key_info": ["è§£æå¤±è´¥"], "post_type": "å…¶ä»–", "value_assessment": "ä½"}
            
    except Exception as e:
        logger.error(f"âœ— AIåˆ†æå¤±è´¥: {e}")
        if retry_count < AI_RETRY_LIMIT:
            logger.info(f"âŸ³ é‡è¯•AIåˆ†æ ({retry_count + 1}/{AI_RETRY_LIMIT})...")
            await asyncio.sleep(AI_RETRY_DELAY)
            return await analyze_single_post_with_gemini(post, model, retry_count + 1)
        else:
            return {"core_issue": "åˆ†æå¤±è´¥", "key_info": ["åˆ†æå¤±è´¥"], "post_type": "å…¶ä»–", "value_assessment": "ä½"}

# --- æ•°æ®åº“æ“ä½œ ---
async def create_posts_table():
    """åˆ›å»ºredditå¸–å­è¡¨"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL, command_timeout=30)
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS reddit_posts (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                core_issue TEXT,
                key_info JSONB,
                post_type TEXT,
                value_assessment TEXT,
                subreddit TEXT,
                score INTEGER,
                num_comments INTEGER,
                timestamp TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info("æ•°æ®åº“è¡¨ 'reddit_posts' æ£€æŸ¥æˆ–åˆ›å»ºæˆåŠŸã€‚")
        return True
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

async def insert_posts_into_db(posts_data):
    """å°†å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“"""
    if not NEON_DB_URL:
        logger.error("æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return False

    conn = None
    try:
        conn = await asyncpg.connect(NEON_DB_URL, command_timeout=30)
        logger.info("å¼€å§‹å°† Reddit å¸–å­æ•°æ®æ’å…¥åˆ°æ•°æ®åº“...")
        
        success_count = 0
        for post in posts_data:
            try:
                post_id = post.get('id')
                title = post.get('title')
                url = post.get('link')
                analysis = post.get('analysis', {})
                core_issue = analysis.get('core_issue')
                key_info = json.dumps(analysis.get('key_info', []))
                post_type = analysis.get('post_type')
                value_assessment = analysis.get('value_assessment')
                subreddit = post.get('subreddit', SUBREDDIT)
                score = post.get('score', 0)
                num_comments = post.get('num_comments', 0)

                await conn.execute("""
                    INSERT INTO reddit_posts (id, title, url, core_issue, key_info, post_type, value_assessment, subreddit, score, num_comments)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ON CONFLICT (id) DO UPDATE SET
                        title = EXCLUDED.title,
                        url = EXCLUDED.url,
                        core_issue = EXCLUDED.core_issue,
                        key_info = EXCLUDED.key_info,
                        post_type = EXCLUDED.post_type,
                        value_assessment = EXCLUDED.value_assessment,
                        subreddit = EXCLUDED.subreddit,
                        score = EXCLUDED.score,
                        num_comments = EXCLUDED.num_comments,
                        timestamp = CURRENT_TIMESTAMP;
                """, post_id, title, url, core_issue, key_info, post_type, value_assessment, subreddit, score, num_comments)
                
                logger.info(f"  - Reddit å¸–å­ '{title[:30]}...' (ID: {post_id}) å·²æ’å…¥/æ›´æ–°ã€‚")
                success_count += 1
                
            except Exception as e:
                logger.error(f"æ’å…¥ Reddit å¸–å­ {post.get('id', 'N/A')} å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸæ’å…¥/æ›´æ–°äº† {success_count}/{len(posts_data)} æ¡ Reddit å¸–å­æ•°æ®åˆ°æ•°æ®åº“ã€‚")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"æ’å…¥ Reddit å¸–å­æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            await conn.close()

# --- AIæ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆ ---
async def generate_ai_summary_report(posts_data, model):
    """ç”ŸæˆAIæ´å¯ŸæŠ¥å‘Š (å¼‚æ­¥)"""
    processed_posts = []
    post_summaries = []

    logger.info(f"--- å¼€å§‹ä¸º {len(posts_data)} ä¸ªå¸–å­å¹¶å‘ç”Ÿæˆç»“æ„åŒ–åˆ†æ ---")

    # åˆ›å»ºå¹¶å‘ä»»åŠ¡åˆ—è¡¨
    tasks = [analyze_single_post_with_gemini(post, model) for post in posts_data]
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
    analyses = await asyncio.gather(*tasks)

    logger.info("--- é€å¸–åˆ†æå…¨éƒ¨å®Œæˆ ---")

    for i, analysis in enumerate(analyses):
        post = posts_data[i]
        if analysis and analysis.get("core_issue") not in ["åˆ†æå¤±è´¥", "è§£æå¤±è´¥", "APIå¤±è´¥"]:
            processed_posts.append({
                "id": post.get('id'),
                "title": post.get('title'),
                "link": post.get('link'),
                "author": post.get('author'),
                "content": post.get('content'),
                "analysis": analysis,
                "subreddit": post.get('subreddit'),
                "score": post.get('score', 0),
                "num_comments": post.get('num_comments', 0)
            })
            post_summaries.append({
                "title": post.get('title', 'N/A'),
                "core_issue": analysis.get('core_issue', 'N/A'),
                "post_type": analysis.get('post_type', 'N/A'),
                "value_assessment": analysis.get('value_assessment')
            })
        else:
            logger.error(f"å¸–å­ '{post['title'][:30]}...' AIåˆ†ææœ€ç»ˆå¤±è´¥")

    logger.info("--- å¼€å§‹ç”Ÿæˆä»Šæ—¥æ•´ä½“æ´å¯ŸæŠ¥å‘Š (JSONæ ¼å¼) ---")
    
    try:
        all_summaries_text = json.dumps(post_summaries, ensure_ascii=False, indent=2)

        overall_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±çš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯ä»Šå¤© Reddit çƒ­é—¨å¸–å­çš„JSONæ ¼å¼æ‘˜è¦åˆ—è¡¨ã€‚
è¯·æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½é«˜åº¦æµ“ç¼©çš„ä¸­æ–‡"ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ"æŠ¥å‘Šï¼Œå¹¶ä¸¥æ ¼ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ã€‚
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–Markdownçš„```json ```æ ‡è®°ã€‚

**ä»Šæ—¥å¸–å­æ‘˜è¦åˆé›† (JSONæ ¼å¼):**
{all_summaries_text}
---
**è¯·è¾“å‡ºä»¥ä¸‹ç»“æ„çš„JSON**:
{{
  "overview": "ç”¨ä¸€ä¸¤å¥è¯æ€»ç»“ä»Šå¤© Reddit ç¤¾åŒºçš„æ•´ä½“æ°›å›´å’Œè®¨è®ºç„¦ç‚¹ã€‚",
  "highlights": {
    "tech_savvy": ["æç‚¼1-3æ¡æœ€ç¡¬æ ¸çš„æŠ€æœ¯å¹²è´§æˆ–ç§‘æŠ€èµ„è®¯"],
    "resources_deals": ["æç‚¼1-3æ¡æœ€å€¼å¾—å…³æ³¨çš„èµ„æºåˆ†äº«æˆ–ä¼˜æƒ ä¿¡æ¯"],
    "hot_topics": ["æç‚¼1-3ä¸ªå¼•å‘æœ€å¹¿æ³›è®¨è®ºçš„çƒ­é—¨è¯é¢˜æˆ–äº‰è®®ç‚¹"]
  },
  "conclusion": "ç”¨ä¸€å¥è¯å¯¹ä»Šå¤©çš„å†…å®¹åšä¸ªé£è¶£æˆ–ä¸“ä¸šçš„æ€»ç»“ã€‚"
}}
"""

        response = await asyncio.to_thread(model.generate_content, overall_prompt)
        content = response.text
        
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        content = content.strip()
        
        summary_analysis = json.loads(content)
        logger.info("--- æ•´ä½“æ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæ¯• ---")
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ•´ä½“æ´å¯ŸæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        summary_analysis = {
            "overview": "ä»Šæ—¥å†…å®¹åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜ã€‚",
            "highlights": {"tech_savvy": [], "resources_deals": [], "hot_topics": []},
            "conclusion": "ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œæ˜æ—¥æ¢å¤æ­£å¸¸æœåŠ¡ã€‚"
        }

    return {
        "summary_analysis": summary_analysis,
        "processed_posts": processed_posts
    }

# --- JSONæŠ¥å‘Šç”Ÿæˆ ---
def generate_json_report(report_data, posts_count):
    """ç”ŸæˆJSONæŠ¥å‘Šæ–‡ä»¶"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"data/reddit_technology_report_{today_str}.json"

        final_json = {
            "meta": {
                "report_date": today_str,
                "title": f"Reddit Technology æ¯æ—¥çƒ­ç‚¹æŠ¥å‘Š ({today_str})",
                "source": "Reddit",
                "post_count": posts_count,
                "generation_time": datetime.now().isoformat(),
                "status": "success" if posts_count > 0 else "no_data"
            },
            "summary": report_data.get('summary_analysis', {}),
            "posts": []
        }

        for post in report_data.get('processed_posts', []):
            final_json["posts"].append({
                "id": post.get('id', 'N/A'),
                "title": post.get('title', 'æ— æ ‡é¢˜'),
                "url": post.get('link', '#'),
                "analysis": post.get('analysis', {})
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆJSONæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- MarkdownæŠ¥å‘Šç”Ÿæˆ ---
def generate_markdown_report(report_data, posts_count):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"reports/Reddit_Technology_Daily_Report_{today_str}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# Reddit Technology æ¯æ—¥çƒ­ç‚¹æŠ¥å‘Š ({today_str})\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ¯ ä»Šæ—¥çƒ­ç‚¹æ´å¯Ÿ\n\n")
            
            summary = report_data.get('summary_analysis', {})
            if summary:
                f.write(f"**æ•´ä½“æ¦‚å†µ**: {summary.get('overview', 'æš‚æ— åˆ†æ')}\n\n")
                
                highlights = summary.get('highlights', {})
                if highlights.get('tech_savvy'):
                    f.write("**ğŸ”§ æŠ€æœ¯è¦é—»**:\n")
                    for item in highlights['tech_savvy']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                if highlights.get('hot_topics'):
                    f.write("**ğŸ”¥ çƒ­é—¨è¯é¢˜**:\n")
                    for item in highlights['hot_topics']:
                        f.write(f"- {item}\n")
                    f.write("\n")
                
                f.write(f"**ğŸ“ å°ç»“**: {summary.get('conclusion', 'æš‚æ— æ€»ç»“')}\n\n")
            
            f.write("---\n\n")
            f.write("## ğŸ“‹ åŸå§‹å¸–å­åˆ—è¡¨ (å…± {} ç¯‡)\n\n".format(posts_count))
            
            posts = report_data.get('processed_posts', [])
            if posts:
                for i, post in enumerate(posts, 1):
                    title = post.get('title', 'æ— æ ‡é¢˜')
                    link = post.get('link', '#')
                    analysis = post.get('analysis', {})
                    
                    f.write(f"{i}. [{title}]({link})\n")
                    f.write(f"   - **è®®é¢˜**: {analysis.get('core_issue', 'N/A')}\n")
                    f.write(f"   - **ç±»å‹**: {analysis.get('post_type', 'N/A')}\n")
                    f.write(f"   - **ä»·å€¼**: {analysis.get('value_assessment', 'N/A')}\n\n")
            else:
                f.write("ä»Šæ—¥æœªèƒ½æŠ“å–åˆ°ä»»ä½•å¸–å­ã€‚\n")

        logger.info(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
        return None

# --- ä¸»å‡½æ•° ---
async def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    logger.info("=== å¼€å§‹æ‰§è¡Œ Reddit çˆ¬è™«ä»»åŠ¡ ===")
    
    try:
        # éªŒè¯ç¯å¢ƒå˜é‡
        if not GEMINI_API_KEY:
            logger.error("âœ— æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")
            return False
        if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT]):
            logger.error("âœ— ç¼ºå°‘ Reddit API å‡­æ® (CLIENT_ID, CLIENT_SECRET, USER_AGENT)")
            return False
        if not NEON_DB_URL:
            logger.error("âœ— æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡")
            return False

        # åˆå§‹åŒ– APIå®¢æˆ·ç«¯
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-pro')
        logger.info("âœ“ Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")

        reddit_client = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT,
        )
        logger.info("âœ“ Redditå®¢æˆ·ç«¯ (PRAW) åˆå§‹åŒ–æˆåŠŸ")

        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not await create_posts_table():
            logger.error("âœ— æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        # è·å–å¸–å­å’Œè¯„è®ºæ•°æ®
        posts_data = fetch_reddit_posts(reddit_client)
        
        if not posts_data:
            logger.warning("æœªèƒ½è·å–åˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œä»»åŠ¡æå‰ç»“æŸã€‚")
            return True # å¯èƒ½æ˜¯å½“å¤©æ²¡æœ‰å¸–å­ï¼Œä¸ç®—å¤±è´¥
        
        logger.info(f"âœ“ æˆåŠŸè·å–åˆ° {len(posts_data)} æ¡å¸–å­æ•°æ®")
        
        # ç”ŸæˆAIæŠ¥å‘Š
        report_data = await generate_ai_summary_report(posts_data, gemini_model)
        
        # æ’å…¥æ•°æ®åˆ°æ•°æ®åº“
        if report_data.get('processed_posts'):
            db_success = await insert_posts_into_db(report_data['processed_posts'])
            if not db_success:
                logger.error("âœ— æ•°æ®åº“æ’å…¥å¤±è´¥")
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        json_file = generate_json_report(report_data, len(posts_data))
        md_file = generate_markdown_report(report_data, len(posts_data))
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=== ä»»åŠ¡å®Œæˆ ===")
        logger.info(f"å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        logger.info(f"å¤„ç†å¸–å­: {len(posts_data)} æ¡")
        logger.info(f"ç”Ÿæˆæ–‡ä»¶: {json_file}, {md_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # è¿è¡Œä¸»å¼‚æ­¥å‡½æ•°
    if asyncio.run(main()):
        logger.info("\nğŸ‰ Reddit çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
    else:
        logger.error("\nğŸ’¥ Reddit çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼")